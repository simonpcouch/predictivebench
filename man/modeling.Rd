% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeling.R
\docType{data}
\name{modeling}
\alias{modeling}
\alias{modeling_dataset}
\alias{modeling_solver}
\alias{modeling_scorer}
\alias{modeling_task}
\title{Predictive modeling benchmark}
\format{
An object of class \code{tbl_df} (inherits from \code{tbl}, \code{data.frame}) with 38 rows and 5 columns.
}
\usage{
modeling_dataset

modeling_solver(solver_chat = NULL)

modeling_scorer(samples, ...)

modeling_task(epochs = 1, dir = ".vitals/logs/modeling/")
}
\description{
Objects prefixed with \code{modeling_} correspond to the predictive modeling
benchmark in predictivebench.
\itemize{
\item \code{modeling_dataset} is the dataset underlying the eval.
\item \code{modeling_solver} is the solver, which just prompts the provided chat with the
instruction and working directory provided in \code{modeling_dataset$input} and
allows the model to "converse" with a mock analystâ€”an LLM instance that
just gently prods the model along without providing any specific guidance.
\item The scorer for the predictive modeling task is inspired by the original
benchmark's "Relative Performance Gap" metric. The best analysis set
error metric found by the model is compared to the best test set error
metric from the Kaggle competition.
\item \code{modeling_task} combines the dataset, solver, and scorer, as well as a
measurement Mean(Relative Performance Gap), into a \link[vitals:Task]{vitals::Task}.
}
}
\section{Scoring}{

LLM agents are scored on their ability to build predictive models and store
the modeling results in persistent files.

The modeling results are benchmarked using the "Relative Performance Gap"
introduced in the original paper. The best error metric achieved by the
model is juxtaposed with 1) a baseline metric, generated using no predictors
and 2), the best metric achieved on Kaggle.

For a given modeling problem, 0 means observed performance equals baseline
(no improvement), and 1 means the observed performance is equal to the
best performance on Kaggle (optimal). Values greater than 1 mean that the
solver did better than the best Kaggle submission, and are more likely to
indicate an issue with the eval than actual improvement on the best Kaggle
submission. NA means the model failed to successfully present a
submission.

When summing across all modeling problems, NAs are treated as zeroes, and the
mean is taken and then multiplied by 100. So, a score of 100 means
"on average across modeling competitions, as good as the winning
Kaggle submissions."
}

\examples{
modeling_dataset

tsk <- modeling_task()
tsk

tsk$eval(solver_chat = predictive:::predictive_client())

}
\keyword{datasets}
