{
  "version": 2,
  "status": "success",
  "eval": {
    "run_id": "obeq1fC9DYeBdn5oluX1fL",
    "created": "2025-08-20T16:27:41-05:00",
    "task": "predictivebench",
    "task_id": "9dae87f00a04e5e7e1cbca",
    "task_version": 0,
    "task_file": "/Users/simoncouch/Documents/rrr/predictivebench/inst/run_eval.R",
    "task_attribs": {},
    "task_args": {},
    "dataset": {
      "samples": 1,
      "sample_ids": [
        1
      ],
      "shuffled": false
    },
    "model": "modeling_solver() (gpt-4.1)",
    "model_args": {},
    "config": {},
    "revision": {
      "type": "git",
      "origin": "https://github.com/UKGovernmentBEIS/inspect_ai.git",
      "commit": "9140d8a2"
    },
    "packages": {
      "inspect_ai": "0.3.63"
    },
    "scorers": [
      {
        "name": "modeling_scorer",
        "options": {},
        "metrics": [
          {
            "name": "mean",
            "options": {}
          }
        ],
        "metadata": {}
      }
    ]
  },
  "plan": {
    "name": "plan",
    "steps": [
      {
        "solver": "system_message",
        "params": {
          "template": "/Users/simoncouch/Documents/rrr/predictivebench/inst/prompt_sans_run_experiment.md\nAt some point in the conversation, once you're satisfied with the model you've built, save the predictions on the test set as 'submission.csv' in the current working directory. At that point, you should notify the user that you have written a submission with the keyword '[SUBMITTED]'. Make sure to include the keyword in all caps, surrounded in square brackets, as written.\n\nThe training data is called train.csv and the testing data test.csv. Resample the training data with vfold_cv.\n\n## Submission format\n\nThe return value is a submission.csv file with predictions with the **same number of rows as the data being predicted** and in the original order from `test.csv`. Do not reorder rows. For observations with missing data such that a prediction cannot be generated, return `NA`. \n\nIf you're using tidymodels, the submission can be `write.csv(collect_predictions(last_fit(<some_workflow>)), file = \"submission.csv\")` (or `augment(<some_workflow>)` for a fitted workflow). If you use either of those functions, provide their output exactly as-is.\n  \nIf you're not using tidymodels, here's more information on the return format:\n\n* For univariate, numeric point estimates, the column should be named `.pred`. For multivariate numeric predictions (excluding probabilities), the columns should be named `.pred_{outcome name}`.\n* For classification tasks, provide both hard class predictions and probabilities:\n    * Class predictions should be factors with the same levels as the original outcome and named `.pred_class`. \n    * For class probability predictions, the columns should be named the same as the factor levels, e.g., `.pred_{level}`, and there should be as many columns as factor levels. \n\nIn the classification case, your submission must have columns for _both_ hard class predictions (i.e. `.pred_class`) and probabilities (i.e. `.pred_{level_1}`, `.pred_{level_2}`, etc)."
        }
      },
      {
        "solver": "modeling_solver() (gpt-4.1)",
        "params": {
          "1": "self$get_samples()$input",
          "solver_chat": "solver_run_r_code_and_prompt(gpt_4_1)"
        }
      }
    ],
    "config": {}
  },
  "results": {
    "total_samples": 1,
    "completed_samples": 1,
    "scores": [
      {
        "name": "modeling_scorer",
        "scorer": "modeling_scorer",
        "params": {},
        "metrics": {
          "relative_performance": {
            "name": "relative_performance",
            "value": 93.2642,
            "params": {
              "1": "self$get_samples()$score"
            }
          }
        }
      }
    ]
  },
  "stats": {
    "started_at": "2025-08-20T16:26:22-05:00",
    "completed_at": "2025-08-20T16:27:41-05:00",
    "model_usage": {
      "gpt-4.1": {
        "input_tokens": 14838,
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0,
        "output_tokens": 1857,
        "total_tokens": 16695
      }
    }
  },
  "samples": [
    {
      "id": "dont-overfit-ii",
      "epoch": 1,
      "input": "question: I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?\n\nknowledge: You are predicting a binary target variable associated with each row in the dataset. The training data consists of 250 rows with 300 continuous variables (numbered 0-299) plus an id column and the binary target variable. The test set contains 19,750 rows with the same structure but without the target variable. The key challenge is avoiding overfitting given the small training sample size relative to the number of features (20,000x200 matrix mentioned refers to the combined train/test data dimensions). Submissions should predict probabilities for the target variable and are evaluated using AUC ROC between predicted and actual target values. The submission format requires id and target columns, where target represents the predicted probability for each test sample. A sample submission file is provided showing the correct format with ids starting from 300.\n\ndir: DSBench/data_modeling/data/data_resplit/dont-overfit-ii",
      "target": "0.886",
      "messages": [
        {
          "id": "aCcGa3sd1kIUmM6Zg50snS",
          "content": "/Users/simoncouch/Documents/rrr/predictivebench/inst/prompt_sans_run_experiment.md\nAt some point in the conversation, once you're satisfied with the model you've built, save the predictions on the test set as 'submission.csv' in the current working directory. At that point, you should notify the user that you have written a submission with the keyword '[SUBMITTED]'. Make sure to include the keyword in all caps, surrounded in square brackets, as written.\n\nThe training data is called train.csv and the testing data test.csv. Resample the training data with vfold_cv.\n\n## Submission format\n\nThe return value is a submission.csv file with predictions with the **same number of rows as the data being predicted** and in the original order from `test.csv`. Do not reorder rows. For observations with missing data such that a prediction cannot be generated, return `NA`. \n\nIf you're using tidymodels, the submission can be `write.csv(collect_predictions(last_fit(<some_workflow>)), file = \"submission.csv\")` (or `augment(<some_workflow>)` for a fitted workflow). If you use either of those functions, provide their output exactly as-is.\n  \nIf you're not using tidymodels, here's more information on the return format:\n\n* For univariate, numeric point estimates, the column should be named `.pred`. For multivariate numeric predictions (excluding probabilities), the columns should be named `.pred_{outcome name}`.\n* For classification tasks, provide both hard class predictions and probabilities:\n    * Class predictions should be factors with the same levels as the original outcome and named `.pred_class`. \n    * For class probability predictions, the columns should be named the same as the factor levels, e.g., `.pred_{level}`, and there should be as many columns as factor levels. \n\nIn the classification case, your submission must have columns for _both_ hard class predictions (i.e. `.pred_class`) and probabilities (i.e. `.pred_{level_1}`, `.pred_{level_2}`, etc).",
          "role": "system"
        },
        {
          "id": "Pa6Kc56j6eqg3p4fKcN03r",
          "content": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?",
          "source": "input",
          "role": "user"
        },
        {
          "id": "cVDUMKP2L6VfGgjRhyO9yM",
          "content": [
            {
              "type": "text",
              "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
            }
          ],
          "source": "generate",
          "role": "assistant"
        },
        {
          "id": "LFoTIbSrPjGyACi1p7iC4Q",
          "content": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection.",
          "source": "input",
          "role": "user"
        },
        {
          "id": "BCoIan4MklACyDabte8NAm",
          "content": [
            {
              "type": "text",
              "text": ""
            }
          ],
          "source": "generate",
          "tool_calls": [
            {
              "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
              "function": "run_r_code",
              "arguments": {
                "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
              }
            }
          ],
          "model": "gpt-4.1",
          "role": "assistant"
        },
        {
          "id": "bXruEqbMFHN0lZVDCpJEdt",
          "content": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
          "tool_call_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
          "function": "run_r_code"
        },
        {
          "id": "wvXR2orEvGYOHPtFs72uvD",
          "content": [
            {
              "type": "text",
              "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
            }
          ],
          "source": "generate",
          "role": "assistant"
        },
        {
          "id": "yKtjLqxbTeWsPCfVsKVSFb",
          "content": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset.",
          "source": "input",
          "role": "user"
        },
        {
          "id": "mYVPmmoIXBMqAf8yE2s6JZ",
          "content": [
            {
              "type": "text",
              "text": ""
            }
          ],
          "source": "generate",
          "tool_calls": [
            {
              "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
              "function": "run_r_code",
              "arguments": {
                "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
              }
            }
          ],
          "model": "gpt-4.1",
          "role": "assistant"
        },
        {
          "id": "xjfTIEra0CWOvUiMahxuL5",
          "content": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]",
          "tool_call_id": "call_QQIBObfU48S1e8BoXwQdM4jG",
          "function": "run_r_code"
        },
        {
          "id": "zkGK0rMB1Zi2rgahHnSnpY",
          "content": [
            {
              "type": "text",
              "text": "Here are the top results from cross-validated hyperparameter tuning of regularized logistic regression (glmnet) with your 10-fold stratified CV:\n\n| Penalty  | Mixture | Mean ROC AUC | Std Err |\n|----------|---------|--------------|---------|\n| 0.0599   | 0.6667  | 0.791        | 0.021   |\n| 0.0599   | 0.5556  | 0.783        | 0.027   |\n| 0.2154   | 0.1111  | 0.776        | 0.032   |\n| 0.2154   | 0.2222  | 0.774        | 0.022   |\n| 0.0599   | 0.7778  | 0.768        | 0.023   |\n\n- The best ROC AUC (≈ 0.791) is achieved with a penalty of 0.0599 and mixture of 0.6667 (mixture controls balance between L1 and L2 regularization—here it's favoring more L1, i.e., feature selection).\n- The standard errors indicate stability across folds.\n\nWould you like to:\n1. Use these best-tuned parameters to fit the final model and generate test set predictions for submission?\n2. Explore the selected features or coefficients?\n3. Try an alternate approach (e.g., with feature filtering or PCA)?\n\nLet me know your next step!"
            }
          ],
          "source": "generate",
          "role": "assistant"
        },
        {
          "id": "zhOb2ZV9taPCPLz2f4ePgn",
          "content": "Let's go with option 1 - use the best-tuned parameters to fit the final model and generate test set predictions for submission.",
          "source": "input",
          "role": "user"
        },
        {
          "id": "Q1RhPHuUnfWDdLM1ink0tO",
          "content": [
            {
              "type": "text",
              "text": ""
            }
          ],
          "source": "generate",
          "tool_calls": [
            {
              "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
              "function": "run_r_code",
              "arguments": {
                "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
              }
            }
          ],
          "model": "gpt-4.1",
          "role": "assistant"
        },
        {
          "id": "kFtC021bLutUK77QhMctr6",
          "content": "[1]  50 304\n\n[{\".pred_class\":\"0\",\".pred_0\":0.6901,\".pred_1\":0.3099,\"id\":142,\"X0\":0.081,\"X1\":0.312,\"X2\":0.015,\"X3\":0.688,\"X4\":-0.145,\"X5\":0.894,\"X6\":-0.153,\"X7\":-1.351,\"X8\":-0.183,\"X9\":0.627,\"X10\":0.262,\"X11\":0.496,\"X12\":0.018,\"X13\":-1.367,\"X14\":0.65,\"X15\":-0.943,\"X16\":0.312,\"X17\":0.406,\"X18\":3.117,\"X19\":0.659,\"X20\":0.788,\"X21\":0.355,\"X22\":-0.735,\"X23\":-0.037,\"X24\":2.214,\"X25\":1.731,\"X26\":-1.157,\"X27\":0.398,\"X28\":1.195,\"X29\":1.141,\"X30\":-1.266,\"X31\":0.349,\"X32\":0.93,\"X33\":1.282,\"X34\":-0.271,\"X35\":-0.947,\"X36\":0.259,\"X37\":2.251,\"X38\":0.242,\"X39\":0.28,\"X40\":-0.011,\"X41\":1.209,\"X42\":0.278,\"X43\":1.048,\"X44\":0.079,\"X45\":-1.25,\"X46\":0.113,\"X47\":0.984,\"X48\":0.51,\"X49\":0.526,\"X50\":-0.956,\"X51\":0.04,\"X52\":-0.632,\"X53\":0.421,\"X54\":0.612,\"X55\":-1.21,\"X56\":1.458,\"X57\":-0.483,\"X58\":1.008,\"X59\":0.986,\"X60\":-0.998,\"X61\":0.664,\"X62\":-0.764,\"X63\":-1.109,\"X64\":0.715,\"X65\":0.161,\"X66\":0.077,\"X67\":-0.869,\"X68\":0.807,\"X69\":1.097,\"X70\":0.996,\"X71\":-0.673,\"X72\":-1.651,\"X73\":-0.33,\"X74\":-0.088,\"X75\":0.397,\"X76\":-0.643,\"X77\":-0.462,\"X78\":0.866,\"X79\":-1.627,\"X80\":0.795,\"X81\":-0.706,\"X82\":0.158,\"X83\":-1.204,\"X84\":0.908,\"X85\":0.255,\"X86\":-0.031,\"X87\":-0.137,\"X88\":0.197,\"X89\":-0.125,\"X90\":-0.969,\"X91\":-0.133,\"X92\":-0.333,\"X93\":-1,\"X94\":0.391,\"X95\":-0.957,\"X96\":0.13,\"X97\":0.164,\"X98\":-0.403,\"X99\":1.591,\"X100\":0.323,\"X101\":-0.845,\"X102\":1.018,\"X103\":2.033,\"X104\":1.161,\"X105\":2.87,\"X106\":-0.426,\"X107\":1.361,\"X108\":-0.204,\"X109\":0.524,\"X110\":0.736,\"X111\":0.274,\"X112\":-0.76,\"X113\":-1.589,\"X114\":-0.053,\"X115\":-0.992,\"X116\":-0.431,\"X117\":-0.413,\"X118\":0.788,\"X119\":-0.017,\"X120\":0.068,\"X121\":-0.074,\"X122\":0.95,\"X123\":-0.788,\"X124\":-0.567,\"X125\":-1.259,\"X126\":0.914,\"X127\":-1.595,\"X128\":0.202,\"X129\":0.128,\"X130\":0.883,\"X131\":-0.147,\"X132\":-1.395,\"X133\":-0.831,\"X134\":-2.199,\"X135\":-0.967,\"X136\":0.592,\"X137\":0.363,\"X138\":0.941,\"X139\":0.548,\"X140\":-0.109,\"X141\":0.641,\"X142\":0.687,\"X143\":0.271,\"X144\":-1.243,\"X145\":1.207,\"X146\":-0.122,\"X147\":-2.6,\"X148\":0.234,\"X149\":0.218,\"X150\":0.739,\"X151\":-0.118,\"X152\":-0.69,\"X153\":-0.476,\"X154\":-0.311,\"X155\":0.593,\"X156\":-0.789,\"X157\":-0.333,\"X158\":0.757,\"X159\":-0.024,\"X160\":-0.047,\"X161\":-1.422,\"X162\":1.395,\"X163\":2.555,\"X164\":1.277,\"X165\":0.499,\"X166\":-0.457,\"X167\":0.076,\"X168\":-0.459,\"X169\":1.094,\"X170\":-1.078,\"X171\":0.435,\"X172\":-0.664,\"X173\":-0.829,\"X174\":-0.509,\"X175\":-0.106,\"X176\":-0.803,\"X177\":-0.495,\"X178\":-0.545,\"X179\":0.293,\"X180\":-0.492,\"X181\":0.404,\"X182\":1.435,\"X183\":-1.011,\"X184\":1.116,\"X185\":0.406,\"X186\":0.164,\"X187\":0.18,\"X188\":-0.286,\"X189\":0.437,\"X190\":0.282,\"X191\":-0.353,\"X192\":0.578,\"X193\":-1.131,\"X194\":0.499,\"X195\":-1.052,\"X196\":0.487,\"X197\":0.114,\"X198\":-0.304,\"X199\":-1.416,\"X200\":0.362,\"X201\":-0.624,\"X202\":-0.066,\"X203\":1.391,\"X204\":-0.896,\"X205\":-0.401,\"X206\":0.633,\"X207\":1.132,\"X208\":-1.979,\"X209\":0.208,\"X210\":-1.051,\"X211\":1.235,\"X212\":-0.821,\"X213\":-0.78,\"X214\":0.142,\"X215\":2.014,\"X216\":-1.135,\"X217\":-0.261,\"X218\":-0.323,\"X219\":-0.965,\"X220\":1.067,\"X221\":-0.371,\"X222\":0.496,\"X223\":1.009,\"X224\":1.725,\"X225\":0.719,\"X226\":0.106,\"X227\":-0.01,\"X228\":0.501,\"X229\":-0.263,\"X230\":-0.605,\"X231\":-0.94,\"X232\":-1.897,\"X233\":1.156,\"X234\":-0.299,\"X235\":-0.655,\"X236\":0.429,\"X237\":-0.557,\"X238\":0.766,\"X239\":0.008,\"X240\":-0.925,\"X241\":-2.201,\"X242\":-0.206,\"X243\":-0.224,\"X244\":1.103,\"X245\":1.027,\"X246\":1.689,\"X247\":-0.66,\"X248\":-0.019,\"X249\":1.429,\"X250\":-1.661,\"X251\":-0.328,\"X252\":-0.29,\"X253\":1.005,\"X254\":-1.328,\"X255\":0.29,\"X256\":1.387,\"X257\":-1.331,\"X258\":0.615,\"X259\":-0.987,\"X260\":0.869,\"X261\":-0.84,\"X262\":0.405,\"X263\":0.625,\"X264\":-1.491,\"X265\":-0.896,\"X266\":-0.056,\"X267\":0.526,\"X268\":-1.403,\"X269\":0.281,\"X270\":-0.886,\"X271\":2.277,\"X272\":-0.882,\"X273\":-0.155,\"X274\":0.005,\"X275\":-0.203,\"X276\":-1.682,\"X277\":0.182,\"X278\":-0.932,\"X279\":0.916,\"X280\":0.11,\"X281\":-0.224,\"X282\":0.632,\"X283\":0.617,\"X284\":0.098,\"X285\":-1.351,\"X286\":0.922,\"X287\":1.724,\"X288\":0.507,\"X289\":0.182,\"X290\":0.307,\"X291\":-0.62,\"X292\":1.04,\"X293\":0.224,\"X294\":-0.678,\"X295\":-0.596,\"X296\":0.203,\"X297\":1.138,\"X298\":1.896,\"X299\":0.952},{\".pred_class\":\"1\",\".pred_0\":0.4394,\".pred_1\":0.5606,\"id\":6,\"X0\":1.257,\"X1\":-1.562,\"X2\":0.832,\"X3\":0.432,\"X4\":1.667,\"X5\":0.843,\"X6\":-0.037,\"X7\":-0.96,\"X8\":-0.311,\"X9\":0.173,\"X10\":-0.059,\"X11\":1.744,\"X12\":-0.92,\"X13\":-0.077,\"X14\":-0.818,\"X15\":-2.053,\"X16\":-2.042,\"X17\":0.219,\"X18\":0.508,\"X19\":-0.659,\"X20\":0.063,\"X21\":-0.443,\"X22\":1.086,\"X23\":-2.087,\"X24\":0.227,\"X25\":1.854,\"X26\":0.211,\"X27\":0.172,\"X28\":0.815,\"X29\":-0.013,\"X30\":-0.408,\"X31\":-1.385,\"X32\":1.2,\"X33\":-0.02,\"X34\":0.554,\"X35\":-0.076,\"X36\":0.452,\"X37\":-0.147,\"X38\":-0.61,\"X39\":-0.907,\"X40\":0.283,\"X41\":0.518,\"X42\":-3.191,\"X43\":-0.975,\"X44\":-1.255,\"X45\":-0.663,\"X46\":2.072,\"X47\":0.087,\"X48\":0.418,\"X49\":-0.067,\"X50\":-0.028,\"X51\":0.266,\"X52\":-0.56,\"X53\":0.389,\"X54\":0.777,\"X55\":-0.858,\"X56\":1.434,\"X57\":1.907,\"X58\":-2.262,\"X59\":-0.727,\"X60\":0.709,\"X61\":0.575,\"X62\":-0.504,\"X63\":-0.113,\"X64\":0.643,\"X65\":-0.22,\"X66\":-0.828,\"X67\":0.402,\"X68\":0.806,\"X69\":0.154,\"X70\":0.052,\"X71\":-0.5,\"X72\":0.832,\"X73\":-0.488,\"X74\":-0.215,\"X75\":-0.88,\"X76\":0.373,\"X77\":0.766,\"X78\":0.518,\"X79\":0.508,\"X80\":-0.609,\"X81\":-2.461,\"X82\":0.302,\"X83\":-0.252,\"X84\":1.067,\"X85\":1.455,\"X86\":0.424,\"X87\":0.959,\"X88\":-0.472,\"X89\":-1.019,\"X90\":-1.926,\"X91\":-0.351,\"X92\":-0.755,\"X93\":-1.043,\"X94\":-0.676,\"X95\":-0.688,\"X96\":1.477,\"X97\":-1.266,\"X98\":0.062,\"X99\":0.967,\"X100\":-1.343,\"X101\":1.48,\"X102\":-0.591,\"X103\":0.449,\"X104\":1.438,\"X105\":0.822,\"X106\":-0.792,\"X107\":-0.007,\"X108\":-0.409,\"X109\":-0.945,\"X110\":-1.055,\"X111\":-0.867,\"X112\":-0.559,\"X113\":-0.279,\"X114\":-0.866,\"X115\":-0.741,\"X116\":0.051,\"X117\":0.921,\"X118\":-1.204,\"X119\":0.62,\"X120\":-1.104,\"X121\":-0.565,\"X122\":1.373,\"X123\":0.581,\"X124\":-0.666,\"X125\":1.256,\"X126\":0.923,\"X127\":2.11,\"X128\":-0.56,\"X129\":-1.786,\"X130\":-1.588,\"X131\":1.139,\"X132\":-0.287,\"X133\":-1.48,\"X134\":2.327,\"X135\":-0.73,\"X136\":1.293,\"X137\":0.639,\"X138\":-1.781,\"X139\":0.249,\"X140\":-0.57,\"X141\":1.124,\"X142\":-0.442,\"X143\":-0.806,\"X144\":-0.5,\"X145\":0.664,\"X146\":-0.425,\"X147\":-0.007,\"X148\":-0.775,\"X149\":0.51,\"X150\":0.408,\"X151\":1.717,\"X152\":0.095,\"X153\":0.2,\"X154\":0.714,\"X155\":-1.086,\"X156\":-0.322,\"X157\":0.58,\"X158\":0.745,\"X159\":-1.32,\"X160\":0.587,\"X161\":-1.219,\"X162\":0.843,\"X163\":1.561,\"X164\":-0.092,\"X165\":-0.815,\"X166\":0.699,\"X167\":-1.377,\"X168\":1.503,\"X169\":0.496,\"X170\":1.064,\"X171\":-1.636,\"X172\":0.508,\"X173\":-0.002,\"X174\":-0.503,\"X175\":-0.703,\"X176\":-1.114,\"X177\":0.991,\"X178\":-0.664,\"X179\":-0.927,\"X180\":-0.715,\"X181\":0.594,\"X182\":1.306,\"X183\":0.162,\"X184\":0.095,\"X185\":0.833,\"X186\":-0.708,\"X187\":1.401,\"X188\":-1.953,\"X189\":-1.42,\"X190\":-0.638,\"X191\":-0.163,\"X192\":-1.514,\"X193\":1.096,\"X194\":-0.172,\"X195\":0.357,\"X196\":-1.562,\"X197\":0.154,\"X198\":0.726,\"X199\":-0.054,\"X200\":0.938,\"X201\":-0.326,\"X202\":-0.314,\"X203\":-0.234,\"X204\":-0.781,\"X205\":-0.914,\"X206\":-2.33,\"X207\":1.627,\"X208\":-2.215,\"X209\":-0.724,\"X210\":-1.439,\"X211\":-0.68,\"X212\":0.747,\"X213\":-0.852,\"X214\":-0.187,\"X215\":0.34,\"X216\":-0.417,\"X217\":0.846,\"X218\":-0.065,\"X219\":-1.632,\"X220\":-0.16,\"X221\":0.577,\"X222\":-0.832,\"X223\":-0.773,\"X224\":1.068,\"X225\":-0.442,\"X226\":-0.362,\"X227\":-1.214,\"X228\":0.852,\"X229\":0.239,\"X230\":-0.101,\"X231\":0.131,\"X232\":0.78,\"X233\":1.369,\"X234\":-2.154,\"X235\":-0.668,\"X236\":1.497,\"X237\":-0.209,\"X238\":-1.053,\"X239\":0.336,\"X240\":0.862,\"X241\":-1.926,\"X242\":-1.013,\"X243\":0.508,\"X244\":-0.862,\"X245\":0.749,\"X246\":0.539,\"X247\":-0.303,\"X248\":-1.239,\"X249\":-1.961,\"X250\":-0.612,\"X251\":0.687,\"X252\":-0.49,\"X253\":-0.648,\"X254\":-0.537,\"X255\":0.074,\"X256\":0.101,\"X257\":0.158,\"X258\":0.594,\"X259\":0.72,\"X260\":-0.604,\"X261\":0.566,\"X262\":1.239,\"X263\":0.27,\"X264\":0.588,\"X265\":-0.207,\"X266\":-1.395,\"X267\":-1.579,\"X268\":-0.141,\"X269\":-0.236,\"X270\":-0.538,\"X271\":0.061,\"X272\":0.642,\"X273\":0.118,\"X274\":-0.15,\"X275\":-1.399,\"X276\":-0.423,\"X277\":0.553,\"X278\":-0.857,\"X279\":0.393,\"X280\":0.958,\"X281\":-0.124,\"X282\":1.379,\"X283\":-0.748,\"X284\":0.218,\"X285\":-1.407,\"X286\":-0.739,\"X287\":0.59,\"X288\":0.044,\"X289\":-0.268,\"X290\":0.024,\"X291\":0.391,\"X292\":0.547,\"X293\":-0.416,\"X294\":-0.333,\"X295\":0.857,\"X296\":0.422,\"X297\":-0.833,\"X298\":-1.055,\"X299\":0.439},{\".pred_class\":\"0\",\".pred_0\":0.8511,\".pred_1\":0.1489,\"id\":97,\"X0\":0.66,\"X1\":0.126,\"X2\":-1.829,\"X3\":-0.73,\"X4\":-0.683,\"X5\":-0.943,\"X6\":0.892,\"X7\":0.121,\"X8\":-1.003,\"X9\":0.398,\"X10\":1.396,\"X11\":1.681,\"X12\":0.089,\"X13\":0.008,\"X14\":0.479,\"X15\":-0.354,\"X16\":0.253,\"X17\":-0.918,\"X18\":-0.948,\"X19\":2.087,\"X20\":0.003,\"X21\":-0.526,\"X22\":0.961,\"X23\":1.105,\"X24\":-2.217,\"X25\":-0.206,\"X26\":-0.709,\"X27\":-1.434,\"X28\":-0.181,\"X29\":-1.473,\"X30\":0.725,\"X31\":0.658,\"X32\":0.137,\"X33\":0.68,\"X34\":-0.512,\"X35\":1.037,\"X36\":-0.293,\"X37\":0.445,\"X38\":0.271,\"X39\":2.01,\"X40\":-0.07,\"X41\":0.754,\"X42\":-0.961,\"X43\":0.341,\"X44\":0.965,\"X45\":0.328,\"X46\":0.167,\"X47\":-0.319,\"X48\":-0.456,\"X49\":1.108,\"X50\":0.535,\"X51\":-1.703,\"X52\":0.505,\"X53\":0.072,\"X54\":1.713,\"X55\":0.562,\"X56\":-0.745,\"X57\":0.055,\"X58\":-0.82,\"X59\":0.937,\"X60\":0.426,\"X61\":0.441,\"X62\":-0.919,\"X63\":0.676,\"X64\":0.184,\"X65\":0.591,\"X66\":-0.006,\"X67\":1.754,\"X68\":-0.337,\"X69\":-0.198,\"X70\":-0.796,\"X71\":-1.298,\"X72\":-0.737,\"X73\":1.285,\"X74\":-1.637,\"X75\":-0.529,\"X76\":0.377,\"X77\":0.212,\"X78\":-0.727,\"X79\":-0.905,\"X80\":-0.701,\"X81\":-0.312,\"X82\":-0.463,\"X83\":-1.797,\"X84\":0.664,\"X85\":-0.171,\"X86\":0.559,\"X87\":0.636,\"X88\":-0.209,\"X89\":0.064,\"X90\":0.144,\"X91\":-0.818,\"X92\":0.7,\"X93\":-0.364,\"X94\":-0.333,\"X95\":-0.191,\"X96\":-1.579,\"X97\":2.081,\"X98\":-1.575,\"X99\":-0.685,\"X100\":-0.385,\"X101\":0.716,\"X102\":-0.767,\"X103\":-0.756,\"X104\":-2.331,\"X105\":-0.586,\"X106\":0.764,\"X107\":-0.622,\"X108\":0.651,\"X109\":0.071,\"X110\":-0.091,\"X111\":-0.129,\"X112\":-1.848,\"X113\":-0.354,\"X114\":0.428,\"X115\":0.928,\"X116\":0.557,\"X117\":-0.181,\"X118\":-0.258,\"X119\":-0.872,\"X120\":-0.864,\"X121\":-0.257,\"X122\":0.795,\"X123\":0.882,\"X124\":-0.519,\"X125\":-0.673,\"X126\":2.144,\"X127\":-0.595,\"X128\":0.624,\"X129\":1.164,\"X130\":0.136,\"X131\":0.52,\"X132\":0.423,\"X133\":0.457,\"X134\":-0.374,\"X135\":-1.384,\"X136\":2.57,\"X137\":0.223,\"X138\":0.217,\"X139\":0.364,\"X140\":-0.132,\"X141\":-1.192,\"X142\":0.471,\"X143\":-0.172,\"X144\":-1.586,\"X145\":-0.091,\"X146\":0.58,\"X147\":-0.43,\"X148\":0.073,\"X149\":-0.315,\"X150\":-0.122,\"X151\":-0.988,\"X152\":-2.52,\"X153\":-0.831,\"X154\":1.146,\"X155\":-0.883,\"X156\":0.723,\"X157\":0.452,\"X158\":-0.237,\"X159\":-0.789,\"X160\":0.422,\"X161\":-1.003,\"X162\":-1.352,\"X163\":1.384,\"X164\":-1.245,\"X165\":-1.602,\"X166\":1.604,\"X167\":-0.947,\"X168\":-0.573,\"X169\":0.903,\"X170\":-0.446,\"X171\":-1.406,\"X172\":-0.847,\"X173\":0.586,\"X174\":-0.16,\"X175\":-2.209,\"X176\":0.685,\"X177\":-0.956,\"X178\":1.433,\"X179\":-1.486,\"X180\":-0.167,\"X181\":0.788,\"X182\":-0.92,\"X183\":-1.12,\"X184\":-0.957,\"X185\":1.929,\"X186\":-1.875,\"X187\":1.304,\"X188\":0.113,\"X189\":1.808,\"X190\":0.525,\"X191\":1.976,\"X192\":0.233,\"X193\":-0.247,\"X194\":0.525,\"X195\":0.253,\"X196\":-0.546,\"X197\":-2.025,\"X198\":-0.252,\"X199\":0.894,\"X200\":-1.113,\"X201\":-0.049,\"X202\":1.231,\"X203\":1.905,\"X204\":-1.276,\"X205\":0.054,\"X206\":-0.264,\"X207\":-0.173,\"X208\":0.239,\"X209\":-1.871,\"X210\":0.605,\"X211\":0.095,\"X212\":0.483,\"X213\":1.36,\"X214\":0.074,\"X215\":0.534,\"X216\":-1.597,\"X217\":-0.818,\"X218\":-0.413,\"X219\":0.21,\"X220\":-0.009,\"X221\":-1.321,\"X222\":1.381,\"X223\":-0.73,\"X224\":-0.465,\"X225\":1.11,\"X226\":0.656,\"X227\":-0.399,\"X228\":-1.875,\"X229\":-0.567,\"X230\":0.552,\"X231\":-0.007,\"X232\":1.404,\"X233\":-0.936,\"X234\":-1.252,\"X235\":0.122,\"X236\":1.556,\"X237\":1.066,\"X238\":0.018,\"X239\":0.56,\"X240\":0.54,\"X241\":-1.882,\"X242\":1.559,\"X243\":1.769,\"X244\":-1.09,\"X245\":0.528,\"X246\":-0.695,\"X247\":1.028,\"X248\":0.413,\"X249\":-1.269,\"X250\":-0.887,\"X251\":0.516,\"X252\":-0.938,\"X253\":-0.33,\"X254\":0.9,\"X255\":-0.383,\"X256\":-1.226,\"X257\":0.247,\"X258\":0.399,\"X259\":-0.931,\"X260\":-0.873,\"X261\":-1.91,\"X262\":0.211,\"X263\":-0.413,\"X264\":0.116,\"X265\":-2.179,\"X266\":-0.908,\"X267\":1.206,\"X268\":0.374,\"X269\":-1.129,\"X270\":-0.577,\"X271\":0.48,\"X272\":-0.584,\"X273\":0.195,\"X274\":0.526,\"X275\":-0.163,\"X276\":-0.77,\"X277\":-0.239,\"X278\":-0.966,\"X279\":1.684,\"X280\":-1.159,\"X281\":-1.329,\"X282\":-0.724,\"X283\":0.221,\"X284\":-1.771,\"X285\":0.25,\"X286\":-1.131,\"X287\":0.609,\"X288\":-1.171,\"X289\":-0.191,\"X290\":1.675,\"X291\":-0.182,\"X292\":0.143,\"X293\":0.302,\"X294\":-1.068,\"X295\":0.445,\"X296\":-0.223,\"X297\":1.092,\"X298\":0.816,\"X299\":0.794},{\".pred_class\":\"0\",\".pred_0\":0.7855,\".pred_1\":0.2145,\"id\":60,\"X0\":-0.071,\"X1\":2.146,\"X2\":-0.512,\"X3\":1.304,\"X4\":-1.778,\"X5\":1.146,\"X6\":0.335,\"X7\":0.13,\"X8\":0.386,\"X9\":1.755,\"X10\":-1.175,\"X11\":0.851,\"X12\":0.31,\"X13\":0.969,\"X14\":-0.229,\"X15\":0.234,\"X16\":-0.049,\"X17\":0.286,\"X18\":0.204,\"X19\":-0.434,\"X20\":-0.726,\"X21\":-1.615,\"X22\":0.804,\"X23\":0.181,\"X24\":1.754,\"X25\":-0.669,\"X26\":-0.564,\"X27\":1.306,\"X28\":-0.11,\"X29\":0.954,\"X30\":-1.445,\"X31\":-1.2,\"X32\":0.553,\"X33\":2.311,\"X34\":-0.637,\"X35\":0.314,\"X36\":-0.289,\"X37\":0.029,\"X38\":0.128,\"X39\":-0.183,\"X40\":-1.007,\"X41\":1.086,\"X42\":-0.1,\"X43\":-1.01,\"X44\":0.306,\"X45\":-2.516,\"X46\":1.955,\"X47\":-0.879,\"X48\":1.855,\"X49\":0.111,\"X50\":-0.721,\"X51\":0.026,\"X52\":0.366,\"X53\":0.463,\"X54\":1.041,\"X55\":-1.566,\"X56\":0.172,\"X57\":-0.219,\"X58\":-1.044,\"X59\":0.475,\"X60\":0.281,\"X61\":-0.105,\"X62\":-0.397,\"X63\":0.155,\"X64\":2.039,\"X65\":-0.86,\"X66\":-0.129,\"X67\":-0.4,\"X68\":-0.076,\"X69\":0.698,\"X70\":0.971,\"X71\":-0.61,\"X72\":-1.25,\"X73\":-0.711,\"X74\":-0.96,\"X75\":0.295,\"X76\":0.097,\"X77\":-0.198,\"X78\":0.275,\"X79\":-0.379,\"X80\":-1.792,\"X81\":-0.38,\"X82\":-0.521,\"X83\":-0.41,\"X84\":-1.741,\"X85\":-0.202,\"X86\":0.538,\"X87\":-1.158,\"X88\":0.714,\"X89\":1.3,\"X90\":0.905,\"X91\":-1.355,\"X92\":-1.109,\"X93\":-0,\"X94\":-1.058,\"X95\":0.603,\"X96\":0.404,\"X97\":0.307,\"X98\":-0.113,\"X99\":0.446,\"X100\":-1.063,\"X101\":0.132,\"X102\":1.218,\"X103\":-1.725,\"X104\":0.358,\"X105\":-0.004,\"X106\":-0.556,\"X107\":-0.77,\"X108\":2.218,\"X109\":-0.513,\"X110\":0.808,\"X111\":-0.594,\"X112\":0.812,\"X113\":-1.549,\"X114\":0.898,\"X115\":0.437,\"X116\":-0.686,\"X117\":0.703,\"X118\":0.07,\"X119\":-0.489,\"X120\":-1.282,\"X121\":-0.653,\"X122\":-0.475,\"X123\":0.428,\"X124\":-0.579,\"X125\":0.547,\"X126\":0.361,\"X127\":-2.15,\"X128\":0.145,\"X129\":-0.7,\"X130\":-0.777,\"X131\":0.313,\"X132\":-0.418,\"X133\":0.387,\"X134\":0.797,\"X135\":1.424,\"X136\":-2.685,\"X137\":-0.982,\"X138\":-0.313,\"X139\":0.635,\"X140\":-0.318,\"X141\":-0.388,\"X142\":0.809,\"X143\":1.091,\"X144\":-0.885,\"X145\":0.149,\"X146\":-0.512,\"X147\":-1.851,\"X148\":0.236,\"X149\":-1.27,\"X150\":-0.141,\"X151\":-1.346,\"X152\":0.313,\"X153\":0.685,\"X154\":-0.042,\"X155\":-0.484,\"X156\":-1.437,\"X157\":-0.778,\"X158\":0.25,\"X159\":-0.738,\"X160\":0.937,\"X161\":-1.204,\"X162\":-0.567,\"X163\":0.583,\"X164\":0.174,\"X165\":-1.473,\"X166\":0.7,\"X167\":0.05,\"X168\":0.992,\"X169\":1.782,\"X170\":-0.59,\"X171\":0.429,\"X172\":0.843,\"X173\":1.086,\"X174\":0.298,\"X175\":-0.161,\"X176\":-0.526,\"X177\":-0.447,\"X178\":-1.108,\"X179\":0.149,\"X180\":0.655,\"X181\":-0.943,\"X182\":1.006,\"X183\":1.047,\"X184\":-1.297,\"X185\":0.186,\"X186\":0.187,\"X187\":-1.078,\"X188\":-0.307,\"X189\":-1.081,\"X190\":-1.492,\"X191\":-0.232,\"X192\":-1.353,\"X193\":-1.49,\"X194\":-0.081,\"X195\":-0.158,\"X196\":0.727,\"X197\":-1.28,\"X198\":0.622,\"X199\":1.45,\"X200\":0.041,\"X201\":2.502,\"X202\":-0.37,\"X203\":1.711,\"X204\":-0.713,\"X205\":1.455,\"X206\":0.545,\"X207\":-0.15,\"X208\":0.047,\"X209\":0.503,\"X210\":-0.438,\"X211\":-0.44,\"X212\":-0.209,\"X213\":1.295,\"X214\":-0.375,\"X215\":0.399,\"X216\":0.889,\"X217\":-0.034,\"X218\":0.682,\"X219\":1.688,\"X220\":0.441,\"X221\":-1.276,\"X222\":1.687,\"X223\":0.027,\"X224\":-0.89,\"X225\":-0.178,\"X226\":0.602,\"X227\":1.752,\"X228\":0.08,\"X229\":0.056,\"X230\":-1.158,\"X231\":-0.211,\"X232\":0.084,\"X233\":-1.743,\"X234\":0.623,\"X235\":0.834,\"X236\":0.59,\"X237\":0.9,\"X238\":0.932,\"X239\":-0.543,\"X240\":0.163,\"X241\":0.079,\"X242\":0.352,\"X243\":-0.731,\"X244\":0.293,\"X245\":0.862,\"X246\":1.428,\"X247\":-0.478,\"X248\":1.253,\"X249\":0.414,\"X250\":0.816,\"X251\":0.125,\"X252\":1.251,\"X253\":1.374,\"X254\":0.282,\"X255\":-0.087,\"X256\":-0.016,\"X257\":0.162,\"X258\":-0.978,\"X259\":1.223,\"X260\":-1.505,\"X261\":-0.25,\"X262\":0.682,\"X263\":0.007,\"X264\":-0.277,\"X265\":1.278,\"X266\":0.097,\"X267\":-0.571,\"X268\":-0.597,\"X269\":0.403,\"X270\":-0.201,\"X271\":-2.726,\"X272\":1.972,\"X273\":0.36,\"X274\":-0.99,\"X275\":-0.264,\"X276\":0.024,\"X277\":1.257,\"X278\":-0.106,\"X279\":0.764,\"X280\":0.101,\"X281\":-1.368,\"X282\":-0.714,\"X283\":-2.981,\"X284\":-0.873,\"X285\":-0.952,\"X286\":1.04,\"X287\":0.248,\"X288\":0.025,\"X289\":-0.413,\"X290\":0.126,\"X291\":0.722,\"X292\":-0.617,\"X293\":-0.758,\"X294\":0.454,\"X295\":0.233,\"X296\":1.097,\"X297\":-0.647,\"X298\":0.467,\"X299\":-1.155},{\".pred_class\":\"0\",\".pred_0\":0.822,\".pred_1\":0.178,\"id\":112,\"X0\":-1.158,\"X1\":-0.183,\"X2\":-0.719,\"X3\":-0.099,\"X4\":0.419,\"X5\":1.298,\"X6\":-1.224,\"X7\":0.171,\"X8\":-0.395,\"X9\":-0.003,\"X10\":-0.162,\"X11\":0.657,\"X12\":-0.903,\"X13\":-0.491,\"X14\":0.685,\"X15\":1.289,\"X16\":1.36,\"X17\":0.223,\"X18\":-1.566,\"X19\":-0.147,\"X20\":-0.488,\"X21\":1.168,\"X22\":-1.355,\"X23\":-0.384,\"X24\":-1.612,\"X25\":0.383,\"X26\":0.223,\"X27\":0.929,\"X28\":-0.542,\"X29\":0.339,\"X30\":0.92,\"X31\":0.577,\"X32\":-0.335,\"X33\":-0.693,\"X34\":-0.205,\"X35\":0.145,\"X36\":1.11,\"X37\":-1.154,\"X38\":0.157,\"X39\":1.352,\"X40\":0.619,\"X41\":-1.112,\"X42\":-0.135,\"X43\":0.051,\"X44\":-0.048,\"X45\":-0.276,\"X46\":0.181,\"X47\":-1.416,\"X48\":-0.336,\"X49\":1.106,\"X50\":-0.474,\"X51\":0.684,\"X52\":-0.233,\"X53\":0.591,\"X54\":-2.135,\"X55\":-0.21,\"X56\":1.15,\"X57\":1.12,\"X58\":-0.644,\"X59\":0.02,\"X60\":0.344,\"X61\":-0.092,\"X62\":0.207,\"X63\":-1.083,\"X64\":0.973,\"X65\":0.074,\"X66\":-2.224,\"X67\":-1.071,\"X68\":0.1,\"X69\":0.275,\"X70\":-1.283,\"X71\":0.511,\"X72\":-0.841,\"X73\":2.441,\"X74\":-0.413,\"X75\":-0.333,\"X76\":0.582,\"X77\":-0.598,\"X78\":-2.841,\"X79\":-1.705,\"X80\":-0.548,\"X81\":-0.002,\"X82\":0.541,\"X83\":-0.386,\"X84\":0.181,\"X85\":0.65,\"X86\":-0.357,\"X87\":-1.619,\"X88\":0.785,\"X89\":0.036,\"X90\":-0.517,\"X91\":0.889,\"X92\":0.476,\"X93\":0.06,\"X94\":1.889,\"X95\":-0.403,\"X96\":0.33,\"X97\":-0.965,\"X98\":-0.693,\"X99\":-0.847,\"X100\":-0.303,\"X101\":1.452,\"X102\":0.04,\"X103\":1.22,\"X104\":-1.494,\"X105\":0.92,\"X106\":0.305,\"X107\":0.066,\"X108\":1.088,\"X109\":2.123,\"X110\":0.204,\"X111\":-0.793,\"X112\":0.169,\"X113\":-0.437,\"X114\":-0.565,\"X115\":-1.62,\"X116\":0.855,\"X117\":0.316,\"X118\":1.421,\"X119\":-1.217,\"X120\":-2.034,\"X121\":0.494,\"X122\":0.277,\"X123\":1.948,\"X124\":-0.8,\"X125\":-0.812,\"X126\":0.884,\"X127\":-0.298,\"X128\":-0.072,\"X129\":2.459,\"X130\":0.036,\"X131\":-2.704,\"X132\":0.573,\"X133\":0.07,\"X134\":-0.061,\"X135\":-0.049,\"X136\":-0.126,\"X137\":-0.503,\"X138\":0.657,\"X139\":0.244,\"X140\":0.471,\"X141\":-1.8,\"X142\":-0.744,\"X143\":-1.445,\"X144\":-2.253,\"X145\":1.167,\"X146\":0.217,\"X147\":-0.896,\"X148\":0.596,\"X149\":0.364,\"X150\":-0.402,\"X151\":-0.447,\"X152\":-1.424,\"X153\":1.844,\"X154\":-0.501,\"X155\":-1.044,\"X156\":0.762,\"X157\":0.93,\"X158\":0.398,\"X159\":-1.02,\"X160\":-1.578,\"X161\":-0.045,\"X162\":2.261,\"X163\":-0.012,\"X164\":0.731,\"X165\":0.857,\"X166\":-1.499,\"X167\":-0.955,\"X168\":-1.609,\"X169\":-0.358,\"X170\":0.887,\"X171\":0.101,\"X172\":-0.647,\"X173\":-0.963,\"X174\":-0.976,\"X175\":0.084,\"X176\":0.313,\"X177\":-0.801,\"X178\":-1.322,\"X179\":0.988,\"X180\":-2.314,\"X181\":-0.201,\"X182\":-0.688,\"X183\":0.995,\"X184\":0.511,\"X185\":-0.094,\"X186\":1.303,\"X187\":0.483,\"X188\":-0.216,\"X189\":-0.679,\"X190\":0.671,\"X191\":-0.583,\"X192\":-0.901,\"X193\":-1.116,\"X194\":1.084,\"X195\":0.129,\"X196\":-0.516,\"X197\":-0.565,\"X198\":-0.825,\"X199\":-0.155,\"X200\":-1.842,\"X201\":-0.588,\"X202\":-2.215,\"X203\":-1.181,\"X204\":-0.521,\"X205\":-1.474,\"X206\":1.665,\"X207\":-0.731,\"X208\":0.895,\"X209\":0.532,\"X210\":0.157,\"X211\":-0.063,\"X212\":-0.906,\"X213\":2.487,\"X214\":-1.9,\"X215\":0.014,\"X216\":-2.328,\"X217\":-0.924,\"X218\":0.553,\"X219\":-0.055,\"X220\":-0.078,\"X221\":-0.173,\"X222\":-0.16,\"X223\":2.247,\"X224\":-0.762,\"X225\":0.891,\"X226\":0.848,\"X227\":0.496,\"X228\":-0.905,\"X229\":0.058,\"X230\":-0.284,\"X231\":1.227,\"X232\":0.322,\"X233\":0.991,\"X234\":0.503,\"X235\":-1.511,\"X236\":0.526,\"X237\":0.109,\"X238\":-0.248,\"X239\":0.914,\"X240\":-0.142,\"X241\":-0.208,\"X242\":-0.437,\"X243\":-0.428,\"X244\":-1.019,\"X245\":1.319,\"X246\":0.212,\"X247\":-0.656,\"X248\":-0.231,\"X249\":-1.089,\"X250\":1.063,\"X251\":-2.201,\"X252\":0.068,\"X253\":0.505,\"X254\":1.71,\"X255\":-0.985,\"X256\":-1.462,\"X257\":0.063,\"X258\":-0.179,\"X259\":0.747,\"X260\":-0.356,\"X261\":0.37,\"X262\":-0.027,\"X263\":0.217,\"X264\":0.355,\"X265\":-0.316,\"X266\":1.686,\"X267\":-0.37,\"X268\":-1.1,\"X269\":-0.878,\"X270\":0.361,\"X271\":0.447,\"X272\":0.9,\"X273\":0.331,\"X274\":-0.46,\"X275\":1.334,\"X276\":0.324,\"X277\":0.655,\"X278\":-1.535,\"X279\":0.179,\"X280\":2.001,\"X281\":2.003,\"X282\":-0.631,\"X283\":-0.719,\"X284\":-2.091,\"X285\":0.166,\"X286\":0.341,\"X287\":0.208,\"X288\":0.706,\"X289\":0.182,\"X290\":-0.414,\"X291\":-0.954,\"X292\":-0.362,\"X293\":4.28,\"X294\":0.158,\"X295\":-0.006,\"X296\":0.413,\"X297\":1.058,\"X298\":-0.296,\"X299\":-0.16},{\".pred_class\":\"0\",\".pred_0\":0.6422,\".pred_1\":0.3578,\"id\":181,\"X0\":0.2,\"X1\":-0.198,\"X2\":-0.884,\"X3\":-0.065,\"X4\":0.168,\"X5\":-0.908,\"X6\":-0.856,\"X7\":0.986,\"X8\":-0.563,\"X9\":2.602,\"X10\":-0.684,\"X11\":1.296,\"X12\":-0.177,\"X13\":0.473,\"X14\":0.521,\"X15\":-0.239,\"X16\":-0.525,\"X17\":-0.605,\"X18\":2.114,\"X19\":-1.064,\"X20\":-1.211,\"X21\":0.051,\"X22\":0.454,\"X23\":1.006,\"X24\":1.654,\"X25\":-1.19,\"X26\":-0.628,\"X27\":1.239,\"X28\":-0.348,\"X29\":-0.401,\"X30\":-0.566,\"X31\":0.615,\"X32\":0.17,\"X33\":1.402,\"X34\":0.076,\"X35\":-0.54,\"X36\":-1.896,\"X37\":-1.321,\"X38\":1.431,\"X39\":0.676,\"X40\":-1.385,\"X41\":0.976,\"X42\":1.286,\"X43\":-0.024,\"X44\":0.437,\"X45\":2.017,\"X46\":0.379,\"X47\":0.997,\"X48\":0.761,\"X49\":-1.315,\"X50\":1.06,\"X51\":0.775,\"X52\":-0.779,\"X53\":0.102,\"X54\":1.136,\"X55\":0.974,\"X56\":-0.357,\"X57\":1.622,\"X58\":-0.572,\"X59\":-0.839,\"X60\":-0.043,\"X61\":0.667,\"X62\":0.78,\"X63\":-0.747,\"X64\":0.726,\"X65\":1.001,\"X66\":-1.908,\"X67\":-0.388,\"X68\":1.161,\"X69\":0.236,\"X70\":0.359,\"X71\":-1.386,\"X72\":2.109,\"X73\":-0.027,\"X74\":-1.054,\"X75\":0.423,\"X76\":-0.733,\"X77\":-0.132,\"X78\":0.59,\"X79\":1.577,\"X80\":-2.12,\"X81\":0.761,\"X82\":-0.794,\"X83\":-0.162,\"X84\":-1.221,\"X85\":-0.141,\"X86\":-0.65,\"X87\":-0.385,\"X88\":-0.206,\"X89\":0.375,\"X90\":0.316,\"X91\":0.613,\"X92\":2.061,\"X93\":0.416,\"X94\":-1.059,\"X95\":-1.129,\"X96\":1.153,\"X97\":-1.923,\"X98\":-1.694,\"X99\":-1.491,\"X100\":-0.82,\"X101\":1.444,\"X102\":0.375,\"X103\":-1.374,\"X104\":-0.786,\"X105\":-0.015,\"X106\":-0.001,\"X107\":-1.405,\"X108\":0.416,\"X109\":-1.137,\"X110\":-0.859,\"X111\":-0.919,\"X112\":-0.431,\"X113\":0.579,\"X114\":-0.401,\"X115\":0.095,\"X116\":0.957,\"X117\":-0.794,\"X118\":0.479,\"X119\":-0.144,\"X120\":-0.974,\"X121\":-0.838,\"X122\":0.149,\"X123\":-1.291,\"X124\":0.308,\"X125\":0,\"X126\":0.55,\"X127\":-0.934,\"X128\":0.924,\"X129\":0.358,\"X130\":1.482,\"X131\":0.616,\"X132\":2.092,\"X133\":1.237,\"X134\":1.245,\"X135\":-0.975,\"X136\":0.951,\"X137\":1.326,\"X138\":-0.263,\"X139\":0.747,\"X140\":-1.329,\"X141\":-1.092,\"X142\":-1.009,\"X143\":2.399,\"X144\":0.093,\"X145\":-0.015,\"X146\":0.339,\"X147\":-0.19,\"X148\":-0.732,\"X149\":-0.176,\"X150\":-0.461,\"X151\":0.821,\"X152\":0.37,\"X153\":-0.818,\"X154\":-0.91,\"X155\":0.883,\"X156\":0.077,\"X157\":0.676,\"X158\":3.724,\"X159\":-1.067,\"X160\":0.426,\"X161\":-0.959,\"X162\":-0.188,\"X163\":0.707,\"X164\":-0.519,\"X165\":1.789,\"X166\":1.427,\"X167\":1.968,\"X168\":0.55,\"X169\":-1.083,\"X170\":0.121,\"X171\":-1.903,\"X172\":-0.507,\"X173\":0.527,\"X174\":-0.588,\"X175\":1.685,\"X176\":-0.141,\"X177\":0.566,\"X178\":0.711,\"X179\":0.999,\"X180\":-0.113,\"X181\":1.92,\"X182\":-0.484,\"X183\":-0.698,\"X184\":0.416,\"X185\":1.682,\"X186\":0.165,\"X187\":1.764,\"X188\":0.166,\"X189\":1.184,\"X190\":-0.471,\"X191\":-1.962,\"X192\":-2.337,\"X193\":1.835,\"X194\":-0.449,\"X195\":-0.416,\"X196\":-0.769,\"X197\":-0.687,\"X198\":-0.852,\"X199\":0.194,\"X200\":1.126,\"X201\":-0.685,\"X202\":2.113,\"X203\":-0.447,\"X204\":0.205,\"X205\":-1.133,\"X206\":0.089,\"X207\":0.1,\"X208\":0.818,\"X209\":1.458,\"X210\":-1.025,\"X211\":-0.074,\"X212\":2.166,\"X213\":-0.31,\"X214\":1.144,\"X215\":-0.172,\"X216\":-0.454,\"X217\":-0.263,\"X218\":1.02,\"X219\":-0.995,\"X220\":-1.203,\"X221\":-0.837,\"X222\":0.64,\"X223\":0.831,\"X224\":1.362,\"X225\":-0.123,\"X226\":0.172,\"X227\":-0.412,\"X228\":-0.015,\"X229\":-0.181,\"X230\":-0.298,\"X231\":0.507,\"X232\":0.852,\"X233\":0.587,\"X234\":-0.359,\"X235\":-1.842,\"X236\":-1.102,\"X237\":-0.465,\"X238\":1.009,\"X239\":0.257,\"X240\":-1.184,\"X241\":1.028,\"X242\":1.287,\"X243\":-0.298,\"X244\":0.248,\"X245\":2.495,\"X246\":0.267,\"X247\":-1.3,\"X248\":-0.347,\"X249\":-0.331,\"X250\":0.816,\"X251\":-0.006,\"X252\":0.023,\"X253\":1.05,\"X254\":-1.08,\"X255\":-0.515,\"X256\":-0.85,\"X257\":-1.525,\"X258\":0.319,\"X259\":1.14,\"X260\":-0.297,\"X261\":1.587,\"X262\":-1.149,\"X263\":-1.117,\"X264\":1.471,\"X265\":0.53,\"X266\":-0.381,\"X267\":1.138,\"X268\":1.998,\"X269\":-1.17,\"X270\":1.348,\"X271\":0.073,\"X272\":0.014,\"X273\":0.949,\"X274\":-0.469,\"X275\":-0.326,\"X276\":0.837,\"X277\":1.323,\"X278\":0.37,\"X279\":-2.357,\"X280\":2.439,\"X281\":0.48,\"X282\":-1.339,\"X283\":0.351,\"X284\":0.366,\"X285\":0.949,\"X286\":0.799,\"X287\":1.216,\"X288\":0.441,\"X289\":-1.834,\"X290\":-2.824,\"X291\":0.729,\"X292\":-0.247,\"X293\":-0.031,\"X294\":-1.241,\"X295\":1.522,\"X296\":-0.475,\"X297\":-1.053,\"X298\":-1.152,\"X299\":1.461}]",
          "tool_call_id": "call_OhWYEFBss59jAUNjhgjoJAa7",
          "function": "run_r_code"
        },
        {
          "id": "pDeJXba8kS5Fx8EoelwhlH",
          "content": [
            {
              "type": "text",
              "text": "The final regularized logistic regression model has been trained on all your training data using the best-tuned parameters (penalty = 0.0599, mixture = 0.6667) and predictions for your test set have been generated and saved as submission.csv in your current working directory.\n\nYour test set predictions include both:\n- .pred_class: the predicted class (as a factor, matching your original target levels)\n- .pred_0 and .pred_1: the class probabilities for each class\n\n[SUBMITTED]\n\nIf you need a sample of the output file, further analysis, or model interpretation (such as feature coefficients), just let me know!"
            }
          ],
          "source": "generate",
          "role": "assistant"
        }
      ],
      "output": {
        "model": "gpt-4.1",
        "choices": [
          {
            "message": {
              "id": "slYCnxoY7zUi2JAk4Fitlr",
              "content": [
                {
                  "type": "text",
                  "text": "The final regularized logistic regression model has been trained on all your training data using the best-tuned parameters (penalty = 0.0599, mixture = 0.6667) and predictions for your test set have been generated and saved as submission.csv in your current working directory.\n\nYour test set predictions include both:\n- .pred_class: the predicted class (as a factor, matching your original target levels)\n- .pred_0 and .pred_1: the class probabilities for each class\n\n[SUBMITTED]\n\nIf you need a sample of the output file, further analysis, or model interpretation (such as feature coefficients), just let me know!"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            "stop_reason": "stop"
          }
        ],
        "usage": {
          "input_tokens": 14838,
          "input_tokens_cache_write": 0,
          "input_tokens_cache_read": 0,
          "output_tokens": 1857,
          "total_tokens": 16695
        },
        "time": 1
      },
      "scores": {
        "modeling_scorer": {
          "value": 0.9326,
          "answer": "Submission successful.",
          "explanation": "modeling_scorer",
          "metadata": {
            "scorer_metadata": [
              {
                "metric": 0.86,
                "caught": ""
              }
            ]
          }
        }
      },
      "metadata": {
        "solver_metadata": [
          {
            ".pred_class": 0,
            ".pred_0": 0.6901,
            ".pred_1": 0.3099,
            "id": 142,
            "X0": 0.081,
            "X1": 0.312,
            "X2": 0.015,
            "X3": 0.688,
            "X4": -0.145,
            "X5": 0.894,
            "X6": -0.153,
            "X7": -1.351,
            "X8": -0.183,
            "X9": 0.627,
            "X10": 0.262,
            "X11": 0.496,
            "X12": 0.018,
            "X13": -1.367,
            "X14": 0.65,
            "X15": -0.943,
            "X16": 0.312,
            "X17": 0.406,
            "X18": 3.117,
            "X19": 0.659,
            "X20": 0.788,
            "X21": 0.355,
            "X22": -0.735,
            "X23": -0.037,
            "X24": 2.214,
            "X25": 1.731,
            "X26": -1.157,
            "X27": 0.398,
            "X28": 1.195,
            "X29": 1.141,
            "X30": -1.266,
            "X31": 0.349,
            "X32": 0.93,
            "X33": 1.282,
            "X34": -0.271,
            "X35": -0.947,
            "X36": 0.259,
            "X37": 2.251,
            "X38": 0.242,
            "X39": 0.28,
            "X40": -0.011,
            "X41": 1.209,
            "X42": 0.278,
            "X43": 1.048,
            "X44": 0.079,
            "X45": -1.25,
            "X46": 0.113,
            "X47": 0.984,
            "X48": 0.51,
            "X49": 0.526,
            "X50": -0.956,
            "X51": 0.04,
            "X52": -0.632,
            "X53": 0.421,
            "X54": 0.612,
            "X55": -1.21,
            "X56": 1.458,
            "X57": -0.483,
            "X58": 1.008,
            "X59": 0.986,
            "X60": -0.998,
            "X61": 0.664,
            "X62": -0.764,
            "X63": -1.109,
            "X64": 0.715,
            "X65": 0.161,
            "X66": 0.077,
            "X67": -0.869,
            "X68": 0.807,
            "X69": 1.097,
            "X70": 0.996,
            "X71": -0.673,
            "X72": -1.651,
            "X73": -0.33,
            "X74": -0.088,
            "X75": 0.397,
            "X76": -0.643,
            "X77": -0.462,
            "X78": 0.866,
            "X79": -1.627,
            "X80": 0.795,
            "X81": -0.706,
            "X82": 0.158,
            "X83": -1.204,
            "X84": 0.908,
            "X85": 0.255,
            "X86": -0.031,
            "X87": -0.137,
            "X88": 0.197,
            "X89": -0.125,
            "X90": -0.969,
            "X91": -0.133,
            "X92": -0.333,
            "X93": -1,
            "X94": 0.391,
            "X95": -0.957,
            "X96": 0.13,
            "X97": 0.164,
            "X98": -0.403,
            "X99": 1.591,
            "X100": 0.323,
            "X101": -0.845,
            "X102": 1.018,
            "X103": 2.033,
            "X104": 1.161,
            "X105": 2.87,
            "X106": -0.426,
            "X107": 1.361,
            "X108": -0.204,
            "X109": 0.524,
            "X110": 0.736,
            "X111": 0.274,
            "X112": -0.76,
            "X113": -1.589,
            "X114": -0.053,
            "X115": -0.992,
            "X116": -0.431,
            "X117": -0.413,
            "X118": 0.788,
            "X119": -0.017,
            "X120": 0.068,
            "X121": -0.074,
            "X122": 0.95,
            "X123": -0.788,
            "X124": -0.567,
            "X125": -1.259,
            "X126": 0.914,
            "X127": -1.595,
            "X128": 0.202,
            "X129": 0.128,
            "X130": 0.883,
            "X131": -0.147,
            "X132": -1.395,
            "X133": -0.831,
            "X134": -2.199,
            "X135": -0.967,
            "X136": 0.592,
            "X137": 0.363,
            "X138": 0.941,
            "X139": 0.548,
            "X140": -0.109,
            "X141": 0.641,
            "X142": 0.687,
            "X143": 0.271,
            "X144": -1.243,
            "X145": 1.207,
            "X146": -0.122,
            "X147": -2.6,
            "X148": 0.234,
            "X149": 0.218,
            "X150": 0.739,
            "X151": -0.118,
            "X152": -0.69,
            "X153": -0.476,
            "X154": -0.311,
            "X155": 0.593,
            "X156": -0.789,
            "X157": -0.333,
            "X158": 0.757,
            "X159": -0.024,
            "X160": -0.047,
            "X161": -1.422,
            "X162": 1.395,
            "X163": 2.555,
            "X164": 1.277,
            "X165": 0.499,
            "X166": -0.457,
            "X167": 0.076,
            "X168": -0.459,
            "X169": 1.094,
            "X170": -1.078,
            "X171": 0.435,
            "X172": -0.664,
            "X173": -0.829,
            "X174": -0.509,
            "X175": -0.106,
            "X176": -0.803,
            "X177": -0.495,
            "X178": -0.545,
            "X179": 0.293,
            "X180": -0.492,
            "X181": 0.404,
            "X182": 1.435,
            "X183": -1.011,
            "X184": 1.116,
            "X185": 0.406,
            "X186": 0.164,
            "X187": 0.18,
            "X188": -0.286,
            "X189": 0.437,
            "X190": 0.282,
            "X191": -0.353,
            "X192": 0.578,
            "X193": -1.131,
            "X194": 0.499,
            "X195": -1.052,
            "X196": 0.487,
            "X197": 0.114,
            "X198": -0.304,
            "X199": -1.416,
            "X200": 0.362,
            "X201": -0.624,
            "X202": -0.066,
            "X203": 1.391,
            "X204": -0.896,
            "X205": -0.401,
            "X206": 0.633,
            "X207": 1.132,
            "X208": -1.979,
            "X209": 0.208,
            "X210": -1.051,
            "X211": 1.235,
            "X212": -0.821,
            "X213": -0.78,
            "X214": 0.142,
            "X215": 2.014,
            "X216": -1.135,
            "X217": -0.261,
            "X218": -0.323,
            "X219": -0.965,
            "X220": 1.067,
            "X221": -0.371,
            "X222": 0.496,
            "X223": 1.009,
            "X224": 1.725,
            "X225": 0.719,
            "X226": 0.106,
            "X227": -0.01,
            "X228": 0.501,
            "X229": -0.263,
            "X230": -0.605,
            "X231": -0.94,
            "X232": -1.897,
            "X233": 1.156,
            "X234": -0.299,
            "X235": -0.655,
            "X236": 0.429,
            "X237": -0.557,
            "X238": 0.766,
            "X239": 0.008,
            "X240": -0.925,
            "X241": -2.201,
            "X242": -0.206,
            "X243": -0.224,
            "X244": 1.103,
            "X245": 1.027,
            "X246": 1.689,
            "X247": -0.66,
            "X248": -0.019,
            "X249": 1.429,
            "X250": -1.661,
            "X251": -0.328,
            "X252": -0.29,
            "X253": 1.005,
            "X254": -1.328,
            "X255": 0.29,
            "X256": 1.387,
            "X257": -1.331,
            "X258": 0.615,
            "X259": -0.987,
            "X260": 0.869,
            "X261": -0.84,
            "X262": 0.405,
            "X263": 0.625,
            "X264": -1.491,
            "X265": -0.896,
            "X266": -0.056,
            "X267": 0.526,
            "X268": -1.403,
            "X269": 0.281,
            "X270": -0.886,
            "X271": 2.277,
            "X272": -0.882,
            "X273": -0.155,
            "X274": 0.005,
            "X275": -0.203,
            "X276": -1.682,
            "X277": 0.182,
            "X278": -0.932,
            "X279": 0.916,
            "X280": 0.11,
            "X281": -0.224,
            "X282": 0.632,
            "X283": 0.617,
            "X284": 0.098,
            "X285": -1.351,
            "X286": 0.922,
            "X287": 1.724,
            "X288": 0.507,
            "X289": 0.182,
            "X290": 0.307,
            "X291": -0.62,
            "X292": 1.04,
            "X293": 0.224,
            "X294": -0.678,
            "X295": -0.596,
            "X296": 0.203,
            "X297": 1.138,
            "X298": 1.896,
            "X299": 0.952
          },
          {
            ".pred_class": 1,
            ".pred_0": 0.4394,
            ".pred_1": 0.5606,
            "id": 6,
            "X0": 1.257,
            "X1": -1.562,
            "X2": 0.832,
            "X3": 0.432,
            "X4": 1.667,
            "X5": 0.843,
            "X6": -0.037,
            "X7": -0.96,
            "X8": -0.311,
            "X9": 0.173,
            "X10": -0.059,
            "X11": 1.744,
            "X12": -0.92,
            "X13": -0.077,
            "X14": -0.818,
            "X15": -2.053,
            "X16": -2.042,
            "X17": 0.219,
            "X18": 0.508,
            "X19": -0.659,
            "X20": 0.063,
            "X21": -0.443,
            "X22": 1.086,
            "X23": -2.087,
            "X24": 0.227,
            "X25": 1.854,
            "X26": 0.211,
            "X27": 0.172,
            "X28": 0.815,
            "X29": -0.013,
            "X30": -0.408,
            "X31": -1.385,
            "X32": 1.2,
            "X33": -0.02,
            "X34": 0.554,
            "X35": -0.076,
            "X36": 0.452,
            "X37": -0.147,
            "X38": -0.61,
            "X39": -0.907,
            "X40": 0.283,
            "X41": 0.518,
            "X42": -3.191,
            "X43": -0.975,
            "X44": -1.255,
            "X45": -0.663,
            "X46": 2.072,
            "X47": 0.087,
            "X48": 0.418,
            "X49": -0.067,
            "X50": -0.028,
            "X51": 0.266,
            "X52": -0.56,
            "X53": 0.389,
            "X54": 0.777,
            "X55": -0.858,
            "X56": 1.434,
            "X57": 1.907,
            "X58": -2.262,
            "X59": -0.727,
            "X60": 0.709,
            "X61": 0.575,
            "X62": -0.504,
            "X63": -0.113,
            "X64": 0.643,
            "X65": -0.22,
            "X66": -0.828,
            "X67": 0.402,
            "X68": 0.806,
            "X69": 0.154,
            "X70": 0.052,
            "X71": -0.5,
            "X72": 0.832,
            "X73": -0.488,
            "X74": -0.215,
            "X75": -0.88,
            "X76": 0.373,
            "X77": 0.766,
            "X78": 0.518,
            "X79": 0.508,
            "X80": -0.609,
            "X81": -2.461,
            "X82": 0.302,
            "X83": -0.252,
            "X84": 1.067,
            "X85": 1.455,
            "X86": 0.424,
            "X87": 0.959,
            "X88": -0.472,
            "X89": -1.019,
            "X90": -1.926,
            "X91": -0.351,
            "X92": -0.755,
            "X93": -1.043,
            "X94": -0.676,
            "X95": -0.688,
            "X96": 1.477,
            "X97": -1.266,
            "X98": 0.062,
            "X99": 0.967,
            "X100": -1.343,
            "X101": 1.48,
            "X102": -0.591,
            "X103": 0.449,
            "X104": 1.438,
            "X105": 0.822,
            "X106": -0.792,
            "X107": -0.007,
            "X108": -0.409,
            "X109": -0.945,
            "X110": -1.055,
            "X111": -0.867,
            "X112": -0.559,
            "X113": -0.279,
            "X114": -0.866,
            "X115": -0.741,
            "X116": 0.051,
            "X117": 0.921,
            "X118": -1.204,
            "X119": 0.62,
            "X120": -1.104,
            "X121": -0.565,
            "X122": 1.373,
            "X123": 0.581,
            "X124": -0.666,
            "X125": 1.256,
            "X126": 0.923,
            "X127": 2.11,
            "X128": -0.56,
            "X129": -1.786,
            "X130": -1.588,
            "X131": 1.139,
            "X132": -0.287,
            "X133": -1.48,
            "X134": 2.327,
            "X135": -0.73,
            "X136": 1.293,
            "X137": 0.639,
            "X138": -1.781,
            "X139": 0.249,
            "X140": -0.57,
            "X141": 1.124,
            "X142": -0.442,
            "X143": -0.806,
            "X144": -0.5,
            "X145": 0.664,
            "X146": -0.425,
            "X147": -0.007,
            "X148": -0.775,
            "X149": 0.51,
            "X150": 0.408,
            "X151": 1.717,
            "X152": 0.095,
            "X153": 0.2,
            "X154": 0.714,
            "X155": -1.086,
            "X156": -0.322,
            "X157": 0.58,
            "X158": 0.745,
            "X159": -1.32,
            "X160": 0.587,
            "X161": -1.219,
            "X162": 0.843,
            "X163": 1.561,
            "X164": -0.092,
            "X165": -0.815,
            "X166": 0.699,
            "X167": -1.377,
            "X168": 1.503,
            "X169": 0.496,
            "X170": 1.064,
            "X171": -1.636,
            "X172": 0.508,
            "X173": -0.002,
            "X174": -0.503,
            "X175": -0.703,
            "X176": -1.114,
            "X177": 0.991,
            "X178": -0.664,
            "X179": -0.927,
            "X180": -0.715,
            "X181": 0.594,
            "X182": 1.306,
            "X183": 0.162,
            "X184": 0.095,
            "X185": 0.833,
            "X186": -0.708,
            "X187": 1.401,
            "X188": -1.953,
            "X189": -1.42,
            "X190": -0.638,
            "X191": -0.163,
            "X192": -1.514,
            "X193": 1.096,
            "X194": -0.172,
            "X195": 0.357,
            "X196": -1.562,
            "X197": 0.154,
            "X198": 0.726,
            "X199": -0.054,
            "X200": 0.938,
            "X201": -0.326,
            "X202": -0.314,
            "X203": -0.234,
            "X204": -0.781,
            "X205": -0.914,
            "X206": -2.33,
            "X207": 1.627,
            "X208": -2.215,
            "X209": -0.724,
            "X210": -1.439,
            "X211": -0.68,
            "X212": 0.747,
            "X213": -0.852,
            "X214": -0.187,
            "X215": 0.34,
            "X216": -0.417,
            "X217": 0.846,
            "X218": -0.065,
            "X219": -1.632,
            "X220": -0.16,
            "X221": 0.577,
            "X222": -0.832,
            "X223": -0.773,
            "X224": 1.068,
            "X225": -0.442,
            "X226": -0.362,
            "X227": -1.214,
            "X228": 0.852,
            "X229": 0.239,
            "X230": -0.101,
            "X231": 0.131,
            "X232": 0.78,
            "X233": 1.369,
            "X234": -2.154,
            "X235": -0.668,
            "X236": 1.497,
            "X237": -0.209,
            "X238": -1.053,
            "X239": 0.336,
            "X240": 0.862,
            "X241": -1.926,
            "X242": -1.013,
            "X243": 0.508,
            "X244": -0.862,
            "X245": 0.749,
            "X246": 0.539,
            "X247": -0.303,
            "X248": -1.239,
            "X249": -1.961,
            "X250": -0.612,
            "X251": 0.687,
            "X252": -0.49,
            "X253": -0.648,
            "X254": -0.537,
            "X255": 0.074,
            "X256": 0.101,
            "X257": 0.158,
            "X258": 0.594,
            "X259": 0.72,
            "X260": -0.604,
            "X261": 0.566,
            "X262": 1.239,
            "X263": 0.27,
            "X264": 0.588,
            "X265": -0.207,
            "X266": -1.395,
            "X267": -1.579,
            "X268": -0.141,
            "X269": -0.236,
            "X270": -0.538,
            "X271": 0.061,
            "X272": 0.642,
            "X273": 0.118,
            "X274": -0.15,
            "X275": -1.399,
            "X276": -0.423,
            "X277": 0.553,
            "X278": -0.857,
            "X279": 0.393,
            "X280": 0.958,
            "X281": -0.124,
            "X282": 1.379,
            "X283": -0.748,
            "X284": 0.218,
            "X285": -1.407,
            "X286": -0.739,
            "X287": 0.59,
            "X288": 0.044,
            "X289": -0.268,
            "X290": 0.024,
            "X291": 0.391,
            "X292": 0.547,
            "X293": -0.416,
            "X294": -0.333,
            "X295": 0.857,
            "X296": 0.422,
            "X297": -0.833,
            "X298": -1.055,
            "X299": 0.439
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8511,
            ".pred_1": 0.1489,
            "id": 97,
            "X0": 0.66,
            "X1": 0.126,
            "X2": -1.829,
            "X3": -0.73,
            "X4": -0.683,
            "X5": -0.943,
            "X6": 0.892,
            "X7": 0.121,
            "X8": -1.003,
            "X9": 0.398,
            "X10": 1.396,
            "X11": 1.681,
            "X12": 0.089,
            "X13": 0.008,
            "X14": 0.479,
            "X15": -0.354,
            "X16": 0.253,
            "X17": -0.918,
            "X18": -0.948,
            "X19": 2.087,
            "X20": 0.003,
            "X21": -0.526,
            "X22": 0.961,
            "X23": 1.105,
            "X24": -2.217,
            "X25": -0.206,
            "X26": -0.709,
            "X27": -1.434,
            "X28": -0.181,
            "X29": -1.473,
            "X30": 0.725,
            "X31": 0.658,
            "X32": 0.137,
            "X33": 0.68,
            "X34": -0.512,
            "X35": 1.037,
            "X36": -0.293,
            "X37": 0.445,
            "X38": 0.271,
            "X39": 2.01,
            "X40": -0.07,
            "X41": 0.754,
            "X42": -0.961,
            "X43": 0.341,
            "X44": 0.965,
            "X45": 0.328,
            "X46": 0.167,
            "X47": -0.319,
            "X48": -0.456,
            "X49": 1.108,
            "X50": 0.535,
            "X51": -1.703,
            "X52": 0.505,
            "X53": 0.072,
            "X54": 1.713,
            "X55": 0.562,
            "X56": -0.745,
            "X57": 0.055,
            "X58": -0.82,
            "X59": 0.937,
            "X60": 0.426,
            "X61": 0.441,
            "X62": -0.919,
            "X63": 0.676,
            "X64": 0.184,
            "X65": 0.591,
            "X66": -0.006,
            "X67": 1.754,
            "X68": -0.337,
            "X69": -0.198,
            "X70": -0.796,
            "X71": -1.298,
            "X72": -0.737,
            "X73": 1.285,
            "X74": -1.637,
            "X75": -0.529,
            "X76": 0.377,
            "X77": 0.212,
            "X78": -0.727,
            "X79": -0.905,
            "X80": -0.701,
            "X81": -0.312,
            "X82": -0.463,
            "X83": -1.797,
            "X84": 0.664,
            "X85": -0.171,
            "X86": 0.559,
            "X87": 0.636,
            "X88": -0.209,
            "X89": 0.064,
            "X90": 0.144,
            "X91": -0.818,
            "X92": 0.7,
            "X93": -0.364,
            "X94": -0.333,
            "X95": -0.191,
            "X96": -1.579,
            "X97": 2.081,
            "X98": -1.575,
            "X99": -0.685,
            "X100": -0.385,
            "X101": 0.716,
            "X102": -0.767,
            "X103": -0.756,
            "X104": -2.331,
            "X105": -0.586,
            "X106": 0.764,
            "X107": -0.622,
            "X108": 0.651,
            "X109": 0.071,
            "X110": -0.091,
            "X111": -0.129,
            "X112": -1.848,
            "X113": -0.354,
            "X114": 0.428,
            "X115": 0.928,
            "X116": 0.557,
            "X117": -0.181,
            "X118": -0.258,
            "X119": -0.872,
            "X120": -0.864,
            "X121": -0.257,
            "X122": 0.795,
            "X123": 0.882,
            "X124": -0.519,
            "X125": -0.673,
            "X126": 2.144,
            "X127": -0.595,
            "X128": 0.624,
            "X129": 1.164,
            "X130": 0.136,
            "X131": 0.52,
            "X132": 0.423,
            "X133": 0.457,
            "X134": -0.374,
            "X135": -1.384,
            "X136": 2.57,
            "X137": 0.223,
            "X138": 0.217,
            "X139": 0.364,
            "X140": -0.132,
            "X141": -1.192,
            "X142": 0.471,
            "X143": -0.172,
            "X144": -1.586,
            "X145": -0.091,
            "X146": 0.58,
            "X147": -0.43,
            "X148": 0.073,
            "X149": -0.315,
            "X150": -0.122,
            "X151": -0.988,
            "X152": -2.52,
            "X153": -0.831,
            "X154": 1.146,
            "X155": -0.883,
            "X156": 0.723,
            "X157": 0.452,
            "X158": -0.237,
            "X159": -0.789,
            "X160": 0.422,
            "X161": -1.003,
            "X162": -1.352,
            "X163": 1.384,
            "X164": -1.245,
            "X165": -1.602,
            "X166": 1.604,
            "X167": -0.947,
            "X168": -0.573,
            "X169": 0.903,
            "X170": -0.446,
            "X171": -1.406,
            "X172": -0.847,
            "X173": 0.586,
            "X174": -0.16,
            "X175": -2.209,
            "X176": 0.685,
            "X177": -0.956,
            "X178": 1.433,
            "X179": -1.486,
            "X180": -0.167,
            "X181": 0.788,
            "X182": -0.92,
            "X183": -1.12,
            "X184": -0.957,
            "X185": 1.929,
            "X186": -1.875,
            "X187": 1.304,
            "X188": 0.113,
            "X189": 1.808,
            "X190": 0.525,
            "X191": 1.976,
            "X192": 0.233,
            "X193": -0.247,
            "X194": 0.525,
            "X195": 0.253,
            "X196": -0.546,
            "X197": -2.025,
            "X198": -0.252,
            "X199": 0.894,
            "X200": -1.113,
            "X201": -0.049,
            "X202": 1.231,
            "X203": 1.905,
            "X204": -1.276,
            "X205": 0.054,
            "X206": -0.264,
            "X207": -0.173,
            "X208": 0.239,
            "X209": -1.871,
            "X210": 0.605,
            "X211": 0.095,
            "X212": 0.483,
            "X213": 1.36,
            "X214": 0.074,
            "X215": 0.534,
            "X216": -1.597,
            "X217": -0.818,
            "X218": -0.413,
            "X219": 0.21,
            "X220": -0.009,
            "X221": -1.321,
            "X222": 1.381,
            "X223": -0.73,
            "X224": -0.465,
            "X225": 1.11,
            "X226": 0.656,
            "X227": -0.399,
            "X228": -1.875,
            "X229": -0.567,
            "X230": 0.552,
            "X231": -0.007,
            "X232": 1.404,
            "X233": -0.936,
            "X234": -1.252,
            "X235": 0.122,
            "X236": 1.556,
            "X237": 1.066,
            "X238": 0.018,
            "X239": 0.56,
            "X240": 0.54,
            "X241": -1.882,
            "X242": 1.559,
            "X243": 1.769,
            "X244": -1.09,
            "X245": 0.528,
            "X246": -0.695,
            "X247": 1.028,
            "X248": 0.413,
            "X249": -1.269,
            "X250": -0.887,
            "X251": 0.516,
            "X252": -0.938,
            "X253": -0.33,
            "X254": 0.9,
            "X255": -0.383,
            "X256": -1.226,
            "X257": 0.247,
            "X258": 0.399,
            "X259": -0.931,
            "X260": -0.873,
            "X261": -1.91,
            "X262": 0.211,
            "X263": -0.413,
            "X264": 0.116,
            "X265": -2.179,
            "X266": -0.908,
            "X267": 1.206,
            "X268": 0.374,
            "X269": -1.129,
            "X270": -0.577,
            "X271": 0.48,
            "X272": -0.584,
            "X273": 0.195,
            "X274": 0.526,
            "X275": -0.163,
            "X276": -0.77,
            "X277": -0.239,
            "X278": -0.966,
            "X279": 1.684,
            "X280": -1.159,
            "X281": -1.329,
            "X282": -0.724,
            "X283": 0.221,
            "X284": -1.771,
            "X285": 0.25,
            "X286": -1.131,
            "X287": 0.609,
            "X288": -1.171,
            "X289": -0.191,
            "X290": 1.675,
            "X291": -0.182,
            "X292": 0.143,
            "X293": 0.302,
            "X294": -1.068,
            "X295": 0.445,
            "X296": -0.223,
            "X297": 1.092,
            "X298": 0.816,
            "X299": 0.794
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7855,
            ".pred_1": 0.2145,
            "id": 60,
            "X0": -0.071,
            "X1": 2.146,
            "X2": -0.512,
            "X3": 1.304,
            "X4": -1.778,
            "X5": 1.146,
            "X6": 0.335,
            "X7": 0.13,
            "X8": 0.386,
            "X9": 1.755,
            "X10": -1.175,
            "X11": 0.851,
            "X12": 0.31,
            "X13": 0.969,
            "X14": -0.229,
            "X15": 0.234,
            "X16": -0.049,
            "X17": 0.286,
            "X18": 0.204,
            "X19": -0.434,
            "X20": -0.726,
            "X21": -1.615,
            "X22": 0.804,
            "X23": 0.181,
            "X24": 1.754,
            "X25": -0.669,
            "X26": -0.564,
            "X27": 1.306,
            "X28": -0.11,
            "X29": 0.954,
            "X30": -1.445,
            "X31": -1.2,
            "X32": 0.553,
            "X33": 2.311,
            "X34": -0.637,
            "X35": 0.314,
            "X36": -0.289,
            "X37": 0.029,
            "X38": 0.128,
            "X39": -0.183,
            "X40": -1.007,
            "X41": 1.086,
            "X42": -0.1,
            "X43": -1.01,
            "X44": 0.306,
            "X45": -2.516,
            "X46": 1.955,
            "X47": -0.879,
            "X48": 1.855,
            "X49": 0.111,
            "X50": -0.721,
            "X51": 0.026,
            "X52": 0.366,
            "X53": 0.463,
            "X54": 1.041,
            "X55": -1.566,
            "X56": 0.172,
            "X57": -0.219,
            "X58": -1.044,
            "X59": 0.475,
            "X60": 0.281,
            "X61": -0.105,
            "X62": -0.397,
            "X63": 0.155,
            "X64": 2.039,
            "X65": -0.86,
            "X66": -0.129,
            "X67": -0.4,
            "X68": -0.076,
            "X69": 0.698,
            "X70": 0.971,
            "X71": -0.61,
            "X72": -1.25,
            "X73": -0.711,
            "X74": -0.96,
            "X75": 0.295,
            "X76": 0.097,
            "X77": -0.198,
            "X78": 0.275,
            "X79": -0.379,
            "X80": -1.792,
            "X81": -0.38,
            "X82": -0.521,
            "X83": -0.41,
            "X84": -1.741,
            "X85": -0.202,
            "X86": 0.538,
            "X87": -1.158,
            "X88": 0.714,
            "X89": 1.3,
            "X90": 0.905,
            "X91": -1.355,
            "X92": -1.109,
            "X93": 0,
            "X94": -1.058,
            "X95": 0.603,
            "X96": 0.404,
            "X97": 0.307,
            "X98": -0.113,
            "X99": 0.446,
            "X100": -1.063,
            "X101": 0.132,
            "X102": 1.218,
            "X103": -1.725,
            "X104": 0.358,
            "X105": -0.004,
            "X106": -0.556,
            "X107": -0.77,
            "X108": 2.218,
            "X109": -0.513,
            "X110": 0.808,
            "X111": -0.594,
            "X112": 0.812,
            "X113": -1.549,
            "X114": 0.898,
            "X115": 0.437,
            "X116": -0.686,
            "X117": 0.703,
            "X118": 0.07,
            "X119": -0.489,
            "X120": -1.282,
            "X121": -0.653,
            "X122": -0.475,
            "X123": 0.428,
            "X124": -0.579,
            "X125": 0.547,
            "X126": 0.361,
            "X127": -2.15,
            "X128": 0.145,
            "X129": -0.7,
            "X130": -0.777,
            "X131": 0.313,
            "X132": -0.418,
            "X133": 0.387,
            "X134": 0.797,
            "X135": 1.424,
            "X136": -2.685,
            "X137": -0.982,
            "X138": -0.313,
            "X139": 0.635,
            "X140": -0.318,
            "X141": -0.388,
            "X142": 0.809,
            "X143": 1.091,
            "X144": -0.885,
            "X145": 0.149,
            "X146": -0.512,
            "X147": -1.851,
            "X148": 0.236,
            "X149": -1.27,
            "X150": -0.141,
            "X151": -1.346,
            "X152": 0.313,
            "X153": 0.685,
            "X154": -0.042,
            "X155": -0.484,
            "X156": -1.437,
            "X157": -0.778,
            "X158": 0.25,
            "X159": -0.738,
            "X160": 0.937,
            "X161": -1.204,
            "X162": -0.567,
            "X163": 0.583,
            "X164": 0.174,
            "X165": -1.473,
            "X166": 0.7,
            "X167": 0.05,
            "X168": 0.992,
            "X169": 1.782,
            "X170": -0.59,
            "X171": 0.429,
            "X172": 0.843,
            "X173": 1.086,
            "X174": 0.298,
            "X175": -0.161,
            "X176": -0.526,
            "X177": -0.447,
            "X178": -1.108,
            "X179": 0.149,
            "X180": 0.655,
            "X181": -0.943,
            "X182": 1.006,
            "X183": 1.047,
            "X184": -1.297,
            "X185": 0.186,
            "X186": 0.187,
            "X187": -1.078,
            "X188": -0.307,
            "X189": -1.081,
            "X190": -1.492,
            "X191": -0.232,
            "X192": -1.353,
            "X193": -1.49,
            "X194": -0.081,
            "X195": -0.158,
            "X196": 0.727,
            "X197": -1.28,
            "X198": 0.622,
            "X199": 1.45,
            "X200": 0.041,
            "X201": 2.502,
            "X202": -0.37,
            "X203": 1.711,
            "X204": -0.713,
            "X205": 1.455,
            "X206": 0.545,
            "X207": -0.15,
            "X208": 0.047,
            "X209": 0.503,
            "X210": -0.438,
            "X211": -0.44,
            "X212": -0.209,
            "X213": 1.295,
            "X214": -0.375,
            "X215": 0.399,
            "X216": 0.889,
            "X217": -0.034,
            "X218": 0.682,
            "X219": 1.688,
            "X220": 0.441,
            "X221": -1.276,
            "X222": 1.687,
            "X223": 0.027,
            "X224": -0.89,
            "X225": -0.178,
            "X226": 0.602,
            "X227": 1.752,
            "X228": 0.08,
            "X229": 0.056,
            "X230": -1.158,
            "X231": -0.211,
            "X232": 0.084,
            "X233": -1.743,
            "X234": 0.623,
            "X235": 0.834,
            "X236": 0.59,
            "X237": 0.9,
            "X238": 0.932,
            "X239": -0.543,
            "X240": 0.163,
            "X241": 0.079,
            "X242": 0.352,
            "X243": -0.731,
            "X244": 0.293,
            "X245": 0.862,
            "X246": 1.428,
            "X247": -0.478,
            "X248": 1.253,
            "X249": 0.414,
            "X250": 0.816,
            "X251": 0.125,
            "X252": 1.251,
            "X253": 1.374,
            "X254": 0.282,
            "X255": -0.087,
            "X256": -0.016,
            "X257": 0.162,
            "X258": -0.978,
            "X259": 1.223,
            "X260": -1.505,
            "X261": -0.25,
            "X262": 0.682,
            "X263": 0.007,
            "X264": -0.277,
            "X265": 1.278,
            "X266": 0.097,
            "X267": -0.571,
            "X268": -0.597,
            "X269": 0.403,
            "X270": -0.201,
            "X271": -2.726,
            "X272": 1.972,
            "X273": 0.36,
            "X274": -0.99,
            "X275": -0.264,
            "X276": 0.024,
            "X277": 1.257,
            "X278": -0.106,
            "X279": 0.764,
            "X280": 0.101,
            "X281": -1.368,
            "X282": -0.714,
            "X283": -2.981,
            "X284": -0.873,
            "X285": -0.952,
            "X286": 1.04,
            "X287": 0.248,
            "X288": 0.025,
            "X289": -0.413,
            "X290": 0.126,
            "X291": 0.722,
            "X292": -0.617,
            "X293": -0.758,
            "X294": 0.454,
            "X295": 0.233,
            "X296": 1.097,
            "X297": -0.647,
            "X298": 0.467,
            "X299": -1.155
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.822,
            ".pred_1": 0.178,
            "id": 112,
            "X0": -1.158,
            "X1": -0.183,
            "X2": -0.719,
            "X3": -0.099,
            "X4": 0.419,
            "X5": 1.298,
            "X6": -1.224,
            "X7": 0.171,
            "X8": -0.395,
            "X9": -0.003,
            "X10": -0.162,
            "X11": 0.657,
            "X12": -0.903,
            "X13": -0.491,
            "X14": 0.685,
            "X15": 1.289,
            "X16": 1.36,
            "X17": 0.223,
            "X18": -1.566,
            "X19": -0.147,
            "X20": -0.488,
            "X21": 1.168,
            "X22": -1.355,
            "X23": -0.384,
            "X24": -1.612,
            "X25": 0.383,
            "X26": 0.223,
            "X27": 0.929,
            "X28": -0.542,
            "X29": 0.339,
            "X30": 0.92,
            "X31": 0.577,
            "X32": -0.335,
            "X33": -0.693,
            "X34": -0.205,
            "X35": 0.145,
            "X36": 1.11,
            "X37": -1.154,
            "X38": 0.157,
            "X39": 1.352,
            "X40": 0.619,
            "X41": -1.112,
            "X42": -0.135,
            "X43": 0.051,
            "X44": -0.048,
            "X45": -0.276,
            "X46": 0.181,
            "X47": -1.416,
            "X48": -0.336,
            "X49": 1.106,
            "X50": -0.474,
            "X51": 0.684,
            "X52": -0.233,
            "X53": 0.591,
            "X54": -2.135,
            "X55": -0.21,
            "X56": 1.15,
            "X57": 1.12,
            "X58": -0.644,
            "X59": 0.02,
            "X60": 0.344,
            "X61": -0.092,
            "X62": 0.207,
            "X63": -1.083,
            "X64": 0.973,
            "X65": 0.074,
            "X66": -2.224,
            "X67": -1.071,
            "X68": 0.1,
            "X69": 0.275,
            "X70": -1.283,
            "X71": 0.511,
            "X72": -0.841,
            "X73": 2.441,
            "X74": -0.413,
            "X75": -0.333,
            "X76": 0.582,
            "X77": -0.598,
            "X78": -2.841,
            "X79": -1.705,
            "X80": -0.548,
            "X81": -0.002,
            "X82": 0.541,
            "X83": -0.386,
            "X84": 0.181,
            "X85": 0.65,
            "X86": -0.357,
            "X87": -1.619,
            "X88": 0.785,
            "X89": 0.036,
            "X90": -0.517,
            "X91": 0.889,
            "X92": 0.476,
            "X93": 0.06,
            "X94": 1.889,
            "X95": -0.403,
            "X96": 0.33,
            "X97": -0.965,
            "X98": -0.693,
            "X99": -0.847,
            "X100": -0.303,
            "X101": 1.452,
            "X102": 0.04,
            "X103": 1.22,
            "X104": -1.494,
            "X105": 0.92,
            "X106": 0.305,
            "X107": 0.066,
            "X108": 1.088,
            "X109": 2.123,
            "X110": 0.204,
            "X111": -0.793,
            "X112": 0.169,
            "X113": -0.437,
            "X114": -0.565,
            "X115": -1.62,
            "X116": 0.855,
            "X117": 0.316,
            "X118": 1.421,
            "X119": -1.217,
            "X120": -2.034,
            "X121": 0.494,
            "X122": 0.277,
            "X123": 1.948,
            "X124": -0.8,
            "X125": -0.812,
            "X126": 0.884,
            "X127": -0.298,
            "X128": -0.072,
            "X129": 2.459,
            "X130": 0.036,
            "X131": -2.704,
            "X132": 0.573,
            "X133": 0.07,
            "X134": -0.061,
            "X135": -0.049,
            "X136": -0.126,
            "X137": -0.503,
            "X138": 0.657,
            "X139": 0.244,
            "X140": 0.471,
            "X141": -1.8,
            "X142": -0.744,
            "X143": -1.445,
            "X144": -2.253,
            "X145": 1.167,
            "X146": 0.217,
            "X147": -0.896,
            "X148": 0.596,
            "X149": 0.364,
            "X150": -0.402,
            "X151": -0.447,
            "X152": -1.424,
            "X153": 1.844,
            "X154": -0.501,
            "X155": -1.044,
            "X156": 0.762,
            "X157": 0.93,
            "X158": 0.398,
            "X159": -1.02,
            "X160": -1.578,
            "X161": -0.045,
            "X162": 2.261,
            "X163": -0.012,
            "X164": 0.731,
            "X165": 0.857,
            "X166": -1.499,
            "X167": -0.955,
            "X168": -1.609,
            "X169": -0.358,
            "X170": 0.887,
            "X171": 0.101,
            "X172": -0.647,
            "X173": -0.963,
            "X174": -0.976,
            "X175": 0.084,
            "X176": 0.313,
            "X177": -0.801,
            "X178": -1.322,
            "X179": 0.988,
            "X180": -2.314,
            "X181": -0.201,
            "X182": -0.688,
            "X183": 0.995,
            "X184": 0.511,
            "X185": -0.094,
            "X186": 1.303,
            "X187": 0.483,
            "X188": -0.216,
            "X189": -0.679,
            "X190": 0.671,
            "X191": -0.583,
            "X192": -0.901,
            "X193": -1.116,
            "X194": 1.084,
            "X195": 0.129,
            "X196": -0.516,
            "X197": -0.565,
            "X198": -0.825,
            "X199": -0.155,
            "X200": -1.842,
            "X201": -0.588,
            "X202": -2.215,
            "X203": -1.181,
            "X204": -0.521,
            "X205": -1.474,
            "X206": 1.665,
            "X207": -0.731,
            "X208": 0.895,
            "X209": 0.532,
            "X210": 0.157,
            "X211": -0.063,
            "X212": -0.906,
            "X213": 2.487,
            "X214": -1.9,
            "X215": 0.014,
            "X216": -2.328,
            "X217": -0.924,
            "X218": 0.553,
            "X219": -0.055,
            "X220": -0.078,
            "X221": -0.173,
            "X222": -0.16,
            "X223": 2.247,
            "X224": -0.762,
            "X225": 0.891,
            "X226": 0.848,
            "X227": 0.496,
            "X228": -0.905,
            "X229": 0.058,
            "X230": -0.284,
            "X231": 1.227,
            "X232": 0.322,
            "X233": 0.991,
            "X234": 0.503,
            "X235": -1.511,
            "X236": 0.526,
            "X237": 0.109,
            "X238": -0.248,
            "X239": 0.914,
            "X240": -0.142,
            "X241": -0.208,
            "X242": -0.437,
            "X243": -0.428,
            "X244": -1.019,
            "X245": 1.319,
            "X246": 0.212,
            "X247": -0.656,
            "X248": -0.231,
            "X249": -1.089,
            "X250": 1.063,
            "X251": -2.201,
            "X252": 0.068,
            "X253": 0.505,
            "X254": 1.71,
            "X255": -0.985,
            "X256": -1.462,
            "X257": 0.063,
            "X258": -0.179,
            "X259": 0.747,
            "X260": -0.356,
            "X261": 0.37,
            "X262": -0.027,
            "X263": 0.217,
            "X264": 0.355,
            "X265": -0.316,
            "X266": 1.686,
            "X267": -0.37,
            "X268": -1.1,
            "X269": -0.878,
            "X270": 0.361,
            "X271": 0.447,
            "X272": 0.9,
            "X273": 0.331,
            "X274": -0.46,
            "X275": 1.334,
            "X276": 0.324,
            "X277": 0.655,
            "X278": -1.535,
            "X279": 0.179,
            "X280": 2.001,
            "X281": 2.003,
            "X282": -0.631,
            "X283": -0.719,
            "X284": -2.091,
            "X285": 0.166,
            "X286": 0.341,
            "X287": 0.208,
            "X288": 0.706,
            "X289": 0.182,
            "X290": -0.414,
            "X291": -0.954,
            "X292": -0.362,
            "X293": 4.28,
            "X294": 0.158,
            "X295": -0.006,
            "X296": 0.413,
            "X297": 1.058,
            "X298": -0.296,
            "X299": -0.16
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6422,
            ".pred_1": 0.3578,
            "id": 181,
            "X0": 0.2,
            "X1": -0.198,
            "X2": -0.884,
            "X3": -0.065,
            "X4": 0.168,
            "X5": -0.908,
            "X6": -0.856,
            "X7": 0.986,
            "X8": -0.563,
            "X9": 2.602,
            "X10": -0.684,
            "X11": 1.296,
            "X12": -0.177,
            "X13": 0.473,
            "X14": 0.521,
            "X15": -0.239,
            "X16": -0.525,
            "X17": -0.605,
            "X18": 2.114,
            "X19": -1.064,
            "X20": -1.211,
            "X21": 0.051,
            "X22": 0.454,
            "X23": 1.006,
            "X24": 1.654,
            "X25": -1.19,
            "X26": -0.628,
            "X27": 1.239,
            "X28": -0.348,
            "X29": -0.401,
            "X30": -0.566,
            "X31": 0.615,
            "X32": 0.17,
            "X33": 1.402,
            "X34": 0.076,
            "X35": -0.54,
            "X36": -1.896,
            "X37": -1.321,
            "X38": 1.431,
            "X39": 0.676,
            "X40": -1.385,
            "X41": 0.976,
            "X42": 1.286,
            "X43": -0.024,
            "X44": 0.437,
            "X45": 2.017,
            "X46": 0.379,
            "X47": 0.997,
            "X48": 0.761,
            "X49": -1.315,
            "X50": 1.06,
            "X51": 0.775,
            "X52": -0.779,
            "X53": 0.102,
            "X54": 1.136,
            "X55": 0.974,
            "X56": -0.357,
            "X57": 1.622,
            "X58": -0.572,
            "X59": -0.839,
            "X60": -0.043,
            "X61": 0.667,
            "X62": 0.78,
            "X63": -0.747,
            "X64": 0.726,
            "X65": 1.001,
            "X66": -1.908,
            "X67": -0.388,
            "X68": 1.161,
            "X69": 0.236,
            "X70": 0.359,
            "X71": -1.386,
            "X72": 2.109,
            "X73": -0.027,
            "X74": -1.054,
            "X75": 0.423,
            "X76": -0.733,
            "X77": -0.132,
            "X78": 0.59,
            "X79": 1.577,
            "X80": -2.12,
            "X81": 0.761,
            "X82": -0.794,
            "X83": -0.162,
            "X84": -1.221,
            "X85": -0.141,
            "X86": -0.65,
            "X87": -0.385,
            "X88": -0.206,
            "X89": 0.375,
            "X90": 0.316,
            "X91": 0.613,
            "X92": 2.061,
            "X93": 0.416,
            "X94": -1.059,
            "X95": -1.129,
            "X96": 1.153,
            "X97": -1.923,
            "X98": -1.694,
            "X99": -1.491,
            "X100": -0.82,
            "X101": 1.444,
            "X102": 0.375,
            "X103": -1.374,
            "X104": -0.786,
            "X105": -0.015,
            "X106": -0.001,
            "X107": -1.405,
            "X108": 0.416,
            "X109": -1.137,
            "X110": -0.859,
            "X111": -0.919,
            "X112": -0.431,
            "X113": 0.579,
            "X114": -0.401,
            "X115": 0.095,
            "X116": 0.957,
            "X117": -0.794,
            "X118": 0.479,
            "X119": -0.144,
            "X120": -0.974,
            "X121": -0.838,
            "X122": 0.149,
            "X123": -1.291,
            "X124": 0.308,
            "X125": 0,
            "X126": 0.55,
            "X127": -0.934,
            "X128": 0.924,
            "X129": 0.358,
            "X130": 1.482,
            "X131": 0.616,
            "X132": 2.092,
            "X133": 1.237,
            "X134": 1.245,
            "X135": -0.975,
            "X136": 0.951,
            "X137": 1.326,
            "X138": -0.263,
            "X139": 0.747,
            "X140": -1.329,
            "X141": -1.092,
            "X142": -1.009,
            "X143": 2.399,
            "X144": 0.093,
            "X145": -0.015,
            "X146": 0.339,
            "X147": -0.19,
            "X148": -0.732,
            "X149": -0.176,
            "X150": -0.461,
            "X151": 0.821,
            "X152": 0.37,
            "X153": -0.818,
            "X154": -0.91,
            "X155": 0.883,
            "X156": 0.077,
            "X157": 0.676,
            "X158": 3.724,
            "X159": -1.067,
            "X160": 0.426,
            "X161": -0.959,
            "X162": -0.188,
            "X163": 0.707,
            "X164": -0.519,
            "X165": 1.789,
            "X166": 1.427,
            "X167": 1.968,
            "X168": 0.55,
            "X169": -1.083,
            "X170": 0.121,
            "X171": -1.903,
            "X172": -0.507,
            "X173": 0.527,
            "X174": -0.588,
            "X175": 1.685,
            "X176": -0.141,
            "X177": 0.566,
            "X178": 0.711,
            "X179": 0.999,
            "X180": -0.113,
            "X181": 1.92,
            "X182": -0.484,
            "X183": -0.698,
            "X184": 0.416,
            "X185": 1.682,
            "X186": 0.165,
            "X187": 1.764,
            "X188": 0.166,
            "X189": 1.184,
            "X190": -0.471,
            "X191": -1.962,
            "X192": -2.337,
            "X193": 1.835,
            "X194": -0.449,
            "X195": -0.416,
            "X196": -0.769,
            "X197": -0.687,
            "X198": -0.852,
            "X199": 0.194,
            "X200": 1.126,
            "X201": -0.685,
            "X202": 2.113,
            "X203": -0.447,
            "X204": 0.205,
            "X205": -1.133,
            "X206": 0.089,
            "X207": 0.1,
            "X208": 0.818,
            "X209": 1.458,
            "X210": -1.025,
            "X211": -0.074,
            "X212": 2.166,
            "X213": -0.31,
            "X214": 1.144,
            "X215": -0.172,
            "X216": -0.454,
            "X217": -0.263,
            "X218": 1.02,
            "X219": -0.995,
            "X220": -1.203,
            "X221": -0.837,
            "X222": 0.64,
            "X223": 0.831,
            "X224": 1.362,
            "X225": -0.123,
            "X226": 0.172,
            "X227": -0.412,
            "X228": -0.015,
            "X229": -0.181,
            "X230": -0.298,
            "X231": 0.507,
            "X232": 0.852,
            "X233": 0.587,
            "X234": -0.359,
            "X235": -1.842,
            "X236": -1.102,
            "X237": -0.465,
            "X238": 1.009,
            "X239": 0.257,
            "X240": -1.184,
            "X241": 1.028,
            "X242": 1.287,
            "X243": -0.298,
            "X244": 0.248,
            "X245": 2.495,
            "X246": 0.267,
            "X247": -1.3,
            "X248": -0.347,
            "X249": -0.331,
            "X250": 0.816,
            "X251": -0.006,
            "X252": 0.023,
            "X253": 1.05,
            "X254": -1.08,
            "X255": -0.515,
            "X256": -0.85,
            "X257": -1.525,
            "X258": 0.319,
            "X259": 1.14,
            "X260": -0.297,
            "X261": 1.587,
            "X262": -1.149,
            "X263": -1.117,
            "X264": 1.471,
            "X265": 0.53,
            "X266": -0.381,
            "X267": 1.138,
            "X268": 1.998,
            "X269": -1.17,
            "X270": 1.348,
            "X271": 0.073,
            "X272": 0.014,
            "X273": 0.949,
            "X274": -0.469,
            "X275": -0.326,
            "X276": 0.837,
            "X277": 1.323,
            "X278": 0.37,
            "X279": -2.357,
            "X280": 2.439,
            "X281": 0.48,
            "X282": -1.339,
            "X283": 0.351,
            "X284": 0.366,
            "X285": 0.949,
            "X286": 0.799,
            "X287": 1.216,
            "X288": 0.441,
            "X289": -1.834,
            "X290": -2.824,
            "X291": 0.729,
            "X292": -0.247,
            "X293": -0.031,
            "X294": -1.241,
            "X295": 1.522,
            "X296": -0.475,
            "X297": -1.053,
            "X298": -1.152,
            "X299": 1.461
          },
          {
            ".pred_class": 1,
            ".pred_0": 0.4735,
            ".pred_1": 0.5265,
            "id": 197,
            "X0": -0.021,
            "X1": -0.281,
            "X2": -1.02,
            "X3": 0.641,
            "X4": -0.675,
            "X5": -1.468,
            "X6": 0.804,
            "X7": 0.736,
            "X8": 0.43,
            "X9": 1.03,
            "X10": -0.482,
            "X11": 0.821,
            "X12": 0.172,
            "X13": -0.803,
            "X14": 1.158,
            "X15": -0.893,
            "X16": -0.588,
            "X17": 1.757,
            "X18": 1.551,
            "X19": 0.618,
            "X20": 1.644,
            "X21": -1.137,
            "X22": -0.932,
            "X23": 0.116,
            "X24": -0.532,
            "X25": 1.715,
            "X26": -0.681,
            "X27": -0.452,
            "X28": -0.999,
            "X29": -1.04,
            "X30": 0.694,
            "X31": -0.055,
            "X32": -0.206,
            "X33": 1.667,
            "X34": -0.969,
            "X35": 0.731,
            "X36": 0.739,
            "X37": 0.399,
            "X38": -0.989,
            "X39": 0.598,
            "X40": -1.699,
            "X41": -0.5,
            "X42": 0.935,
            "X43": 0.518,
            "X44": 2.463,
            "X45": 0.076,
            "X46": -0.161,
            "X47": -0.525,
            "X48": 0.094,
            "X49": 1.778,
            "X50": 0.638,
            "X51": 0.082,
            "X52": 0.066,
            "X53": -0.299,
            "X54": -0.417,
            "X55": 0.839,
            "X56": 0.917,
            "X57": 1.035,
            "X58": 0.99,
            "X59": -1.892,
            "X60": 0.401,
            "X61": 0.407,
            "X62": 0.444,
            "X63": 0.814,
            "X64": 1.356,
            "X65": 0.768,
            "X66": 0.298,
            "X67": 0.215,
            "X68": -0.061,
            "X69": -0.291,
            "X70": 0.056,
            "X71": 0.28,
            "X72": -0.774,
            "X73": -0.187,
            "X74": -1.505,
            "X75": 0.969,
            "X76": -0.799,
            "X77": -0.002,
            "X78": 0.693,
            "X79": -0.658,
            "X80": 0.103,
            "X81": -2.006,
            "X82": 0.268,
            "X83": 0.485,
            "X84": 1.169,
            "X85": 0.985,
            "X86": 0.128,
            "X87": 0.014,
            "X88": -0.388,
            "X89": -0.579,
            "X90": 0.238,
            "X91": -1.449,
            "X92": 0.099,
            "X93": -1.639,
            "X94": -0.712,
            "X95": 0.68,
            "X96": -0.806,
            "X97": -1.817,
            "X98": 1.013,
            "X99": 0.912,
            "X100": -2.15,
            "X101": -0.107,
            "X102": 0.453,
            "X103": 1.652,
            "X104": -1.742,
            "X105": 0.198,
            "X106": 2.699,
            "X107": -0.369,
            "X108": -0.181,
            "X109": -0.758,
            "X110": 0.96,
            "X111": -1.642,
            "X112": -0.695,
            "X113": 0.573,
            "X114": 0.085,
            "X115": 0.277,
            "X116": -0.725,
            "X117": -0.186,
            "X118": 0.566,
            "X119": -0.814,
            "X120": 0.361,
            "X121": 1.474,
            "X122": -0.364,
            "X123": 0.609,
            "X124": 0.048,
            "X125": 0.326,
            "X126": -1.631,
            "X127": 1.124,
            "X128": 0.493,
            "X129": -1.227,
            "X130": -0.692,
            "X131": 1.866,
            "X132": 1.516,
            "X133": 2.1,
            "X134": 0.594,
            "X135": -0.144,
            "X136": -0.99,
            "X137": -0.54,
            "X138": -1.267,
            "X139": -0.632,
            "X140": 0.125,
            "X141": 0.603,
            "X142": 0.472,
            "X143": -0.564,
            "X144": -0.379,
            "X145": -0.827,
            "X146": -2.124,
            "X147": 1.255,
            "X148": 0.12,
            "X149": -0.128,
            "X150": 0.185,
            "X151": 2.433,
            "X152": -1.931,
            "X153": 1.607,
            "X154": -1.23,
            "X155": -2.518,
            "X156": -0.903,
            "X157": -0.173,
            "X158": -1.29,
            "X159": 0.048,
            "X160": 1.326,
            "X161": -1.232,
            "X162": -1.575,
            "X163": -2.034,
            "X164": 1.908,
            "X165": 1.057,
            "X166": 1.645,
            "X167": 1.534,
            "X168": -0.046,
            "X169": -2.219,
            "X170": -0.22,
            "X171": -0.285,
            "X172": 0.55,
            "X173": -1,
            "X174": -1.554,
            "X175": -0.009,
            "X176": 0.019,
            "X177": 0.321,
            "X178": -0.24,
            "X179": -0.005,
            "X180": -0.041,
            "X181": -1.405,
            "X182": -0.414,
            "X183": -0.133,
            "X184": -1.426,
            "X185": -0.624,
            "X186": 0.134,
            "X187": 0.416,
            "X188": -1.394,
            "X189": -0.74,
            "X190": -0.952,
            "X191": -2.233,
            "X192": -0.582,
            "X193": 0.032,
            "X194": 2.083,
            "X195": -0.982,
            "X196": -1.121,
            "X197": 1.025,
            "X198": -1.099,
            "X199": 0.484,
            "X200": 1.023,
            "X201": 0.716,
            "X202": 1.789,
            "X203": -0.519,
            "X204": 1.982,
            "X205": -1.273,
            "X206": -1.569,
            "X207": 0.079,
            "X208": 0.373,
            "X209": -1.736,
            "X210": -0.105,
            "X211": -0.195,
            "X212": 0.735,
            "X213": 1.361,
            "X214": -0.423,
            "X215": 0.093,
            "X216": 0.778,
            "X217": -0.049,
            "X218": 0.605,
            "X219": -0.026,
            "X220": -0.127,
            "X221": 0.339,
            "X222": 1.104,
            "X223": -1.142,
            "X224": 0.5,
            "X225": 0.152,
            "X226": 0.684,
            "X227": -0.64,
            "X228": 0.605,
            "X229": -1.556,
            "X230": 0.115,
            "X231": 0.043,
            "X232": -0.929,
            "X233": 1.358,
            "X234": 0.503,
            "X235": 2.276,
            "X236": -0.689,
            "X237": -0.623,
            "X238": -0.118,
            "X239": 1.008,
            "X240": -1.228,
            "X241": 0.483,
            "X242": -0.769,
            "X243": -1.326,
            "X244": -1.096,
            "X245": 0.528,
            "X246": 0.529,
            "X247": 0.456,
            "X248": -1.528,
            "X249": -1.441,
            "X250": -0.519,
            "X251": -1.448,
            "X252": 0.276,
            "X253": 0.428,
            "X254": 1.173,
            "X255": -0.199,
            "X256": -0.181,
            "X257": 2.024,
            "X258": -0.596,
            "X259": 1.42,
            "X260": 0.349,
            "X261": 0.742,
            "X262": 0.277,
            "X263": 0.434,
            "X264": -0.451,
            "X265": 0.896,
            "X266": -1.195,
            "X267": 1.019,
            "X268": -1.711,
            "X269": -0.326,
            "X270": 0.285,
            "X271": 0.27,
            "X272": 0.535,
            "X273": 0.207,
            "X274": 0.473,
            "X275": -0.043,
            "X276": 0.69,
            "X277": -2.046,
            "X278": 0.876,
            "X279": -0.009,
            "X280": 1.799,
            "X281": 0.499,
            "X282": -0.941,
            "X283": -1.296,
            "X284": 0.721,
            "X285": 0.829,
            "X286": 1.28,
            "X287": -0.954,
            "X288": 0.093,
            "X289": -0.982,
            "X290": 0.821,
            "X291": -0.463,
            "X292": 0.179,
            "X293": 1.992,
            "X294": -2.123,
            "X295": 0.376,
            "X296": -1.896,
            "X297": 0.698,
            "X298": -1.258,
            "X299": -1.725
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7338,
            ".pred_1": 0.2662,
            "id": 184,
            "X0": -0.32,
            "X1": -0.558,
            "X2": -1.449,
            "X3": 0.013,
            "X4": 0.017,
            "X5": 0.208,
            "X6": 1.587,
            "X7": -0.692,
            "X8": -1.367,
            "X9": 0.453,
            "X10": -1.184,
            "X11": 0.014,
            "X12": -0.967,
            "X13": 0.44,
            "X14": 1.351,
            "X15": 2.147,
            "X16": 0.568,
            "X17": -2.199,
            "X18": 0.569,
            "X19": -1.204,
            "X20": 1.073,
            "X21": 1.826,
            "X22": 1.729,
            "X23": 1.335,
            "X24": 0.126,
            "X25": 1.544,
            "X26": 1.114,
            "X27": -0.771,
            "X28": 0.855,
            "X29": -1.192,
            "X30": -0.699,
            "X31": -1.253,
            "X32": 0.786,
            "X33": -1.202,
            "X34": 0.821,
            "X35": 0.107,
            "X36": 1.666,
            "X37": -0.298,
            "X38": -0.219,
            "X39": -0.068,
            "X40": -0.915,
            "X41": 1.737,
            "X42": -0.195,
            "X43": 2.307,
            "X44": 0.216,
            "X45": -0.778,
            "X46": -1.101,
            "X47": 0.107,
            "X48": -0.18,
            "X49": 1.131,
            "X50": 1.332,
            "X51": 1.914,
            "X52": 0.384,
            "X53": -1.462,
            "X54": 0.702,
            "X55": 2.379,
            "X56": 1.96,
            "X57": -0.258,
            "X58": 0.853,
            "X59": 1.7,
            "X60": -1.19,
            "X61": 0.973,
            "X62": 0.954,
            "X63": -0.091,
            "X64": 0.444,
            "X65": 0.884,
            "X66": 0.573,
            "X67": 0.794,
            "X68": 0.567,
            "X69": 0.022,
            "X70": 2.559,
            "X71": 1.176,
            "X72": 0.015,
            "X73": -0.208,
            "X74": 0.308,
            "X75": 0.288,
            "X76": 1.265,
            "X77": 1.294,
            "X78": 0.511,
            "X79": -0.816,
            "X80": -0.246,
            "X81": -1.188,
            "X82": 0.253,
            "X83": 0.166,
            "X84": -0.385,
            "X85": 1.317,
            "X86": -0.332,
            "X87": 0.593,
            "X88": -0.321,
            "X89": 0.321,
            "X90": -0.96,
            "X91": 0.431,
            "X92": 0.83,
            "X93": 0.284,
            "X94": 1.607,
            "X95": -0.95,
            "X96": 1.559,
            "X97": 1.631,
            "X98": -0.288,
            "X99": -0.164,
            "X100": 0.851,
            "X101": -1.06,
            "X102": 0.128,
            "X103": -0.029,
            "X104": -0.525,
            "X105": 0.986,
            "X106": 0.118,
            "X107": 2.117,
            "X108": 1.34,
            "X109": 0.009,
            "X110": 0.622,
            "X111": 1.283,
            "X112": 0.281,
            "X113": -0.073,
            "X114": -1.304,
            "X115": 1.305,
            "X116": 0.462,
            "X117": 0.772,
            "X118": 1.583,
            "X119": -0.923,
            "X120": -0.545,
            "X121": 1.428,
            "X122": -1.882,
            "X123": 0.523,
            "X124": 1.035,
            "X125": 0.715,
            "X126": 0.152,
            "X127": -1.054,
            "X128": 0.19,
            "X129": -0.425,
            "X130": -0.371,
            "X131": -1.628,
            "X132": -0.401,
            "X133": 0.161,
            "X134": -1.076,
            "X135": 1.912,
            "X136": 0.922,
            "X137": -0.775,
            "X138": 1.323,
            "X139": -0.961,
            "X140": -0.597,
            "X141": -1.473,
            "X142": -0.616,
            "X143": -0.116,
            "X144": -0.217,
            "X145": 0.719,
            "X146": 1.281,
            "X147": 0.511,
            "X148": -1.727,
            "X149": -0.709,
            "X150": -0.341,
            "X151": -2.192,
            "X152": 0.792,
            "X153": 0.683,
            "X154": 1.856,
            "X155": 0.821,
            "X156": 0.022,
            "X157": -1.015,
            "X158": -0.634,
            "X159": 1.782,
            "X160": 1.485,
            "X161": 2.165,
            "X162": 1.264,
            "X163": -0.604,
            "X164": -0.322,
            "X165": -0.001,
            "X166": -0.27,
            "X167": -2.293,
            "X168": -1,
            "X169": 0.121,
            "X170": 1.543,
            "X171": -0.6,
            "X172": -0.117,
            "X173": 0.893,
            "X174": -0.208,
            "X175": 0.237,
            "X176": -0.38,
            "X177": -0.928,
            "X178": -0.327,
            "X179": 2.411,
            "X180": 0.175,
            "X181": -0.345,
            "X182": -1.202,
            "X183": -0.846,
            "X184": -0.134,
            "X185": -0.232,
            "X186": 0.288,
            "X187": 1.495,
            "X188": 0.33,
            "X189": 0.422,
            "X190": -1.187,
            "X191": 0.038,
            "X192": 0.226,
            "X193": -2.52,
            "X194": -0.649,
            "X195": -1.165,
            "X196": -0.207,
            "X197": 0.385,
            "X198": 0.461,
            "X199": -1.862,
            "X200": -0.364,
            "X201": 1.79,
            "X202": -0.046,
            "X203": 0.129,
            "X204": -0.47,
            "X205": 0.977,
            "X206": 2.588,
            "X207": 1.581,
            "X208": -0.549,
            "X209": 0.895,
            "X210": -1.005,
            "X211": -1.254,
            "X212": -0.259,
            "X213": 1.21,
            "X214": 0.223,
            "X215": 1.174,
            "X216": -0.585,
            "X217": -3.137,
            "X218": -1.558,
            "X219": -0.716,
            "X220": -0.263,
            "X221": 0.161,
            "X222": -0.134,
            "X223": -0.731,
            "X224": -0.656,
            "X225": -0.727,
            "X226": 0.853,
            "X227": -1.005,
            "X228": -1.095,
            "X229": 0.474,
            "X230": -0.986,
            "X231": 0.276,
            "X232": -0.991,
            "X233": -0.728,
            "X234": 0.786,
            "X235": 2.494,
            "X236": 0.768,
            "X237": -0.855,
            "X238": 0.099,
            "X239": -1.067,
            "X240": 0.14,
            "X241": 0.693,
            "X242": 0.022,
            "X243": 0.223,
            "X244": 0.566,
            "X245": -0.038,
            "X246": 0.522,
            "X247": 0.171,
            "X248": -1.193,
            "X249": -0.619,
            "X250": -0.213,
            "X251": -0.489,
            "X252": -0.145,
            "X253": -0.091,
            "X254": 0.719,
            "X255": 1.226,
            "X256": 0.105,
            "X257": 1.231,
            "X258": 0.818,
            "X259": -0.257,
            "X260": -1.903,
            "X261": 0.357,
            "X262": -0.708,
            "X263": 0.364,
            "X264": -0.323,
            "X265": -0.305,
            "X266": 0.21,
            "X267": 1.099,
            "X268": -0.304,
            "X269": 2.524,
            "X270": 1.701,
            "X271": -0.099,
            "X272": -1.311,
            "X273": 1.568,
            "X274": -0.977,
            "X275": 0.282,
            "X276": -0.871,
            "X277": 0.151,
            "X278": 0.593,
            "X279": 0.526,
            "X280": 0.236,
            "X281": -0.614,
            "X282": -1.374,
            "X283": 1.093,
            "X284": 1.074,
            "X285": -0.011,
            "X286": 0.093,
            "X287": 0.062,
            "X288": 0.317,
            "X289": 1.982,
            "X290": -0.603,
            "X291": -0.744,
            "X292": 3.193,
            "X293": 1.126,
            "X294": -1.056,
            "X295": 0.755,
            "X296": 0.84,
            "X297": -0.043,
            "X298": -0.072,
            "X299": 0.847
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7498,
            ".pred_1": 0.2502,
            "id": 9,
            "X0": 1.279,
            "X1": 0.855,
            "X2": -0.734,
            "X3": -0.25,
            "X4": -0.819,
            "X5": 0.633,
            "X6": 0.447,
            "X7": 1.875,
            "X8": -0.752,
            "X9": 1.143,
            "X10": -0.299,
            "X11": -0.545,
            "X12": -0.668,
            "X13": 0.003,
            "X14": 0.592,
            "X15": -0.471,
            "X16": 0.475,
            "X17": 0.915,
            "X18": -0.685,
            "X19": -0.786,
            "X20": -0.32,
            "X21": -0.507,
            "X22": -0.073,
            "X23": 0.226,
            "X24": -1.057,
            "X25": 0.386,
            "X26": 0.504,
            "X27": 0.388,
            "X28": -0.071,
            "X29": 1.195,
            "X30": 1.345,
            "X31": -1.735,
            "X32": 0.506,
            "X33": -0.575,
            "X34": 0.799,
            "X35": -0.773,
            "X36": 0.413,
            "X37": -0.371,
            "X38": -1.105,
            "X39": -0.276,
            "X40": -1.85,
            "X41": 1.899,
            "X42": -0.308,
            "X43": 0.356,
            "X44": 0.127,
            "X45": -0.249,
            "X46": 0.208,
            "X47": 1.26,
            "X48": -0.307,
            "X49": -0.637,
            "X50": -0.344,
            "X51": -0.484,
            "X52": -0.968,
            "X53": 0.478,
            "X54": -0.147,
            "X55": 0.27,
            "X56": 0.144,
            "X57": -0.136,
            "X58": -2.361,
            "X59": 2.07,
            "X60": 1.751,
            "X61": 0.702,
            "X62": 0.319,
            "X63": -0.263,
            "X64": 0.752,
            "X65": -0.496,
            "X66": -0.331,
            "X67": -0.862,
            "X68": 2.21,
            "X69": -1.572,
            "X70": -0.771,
            "X71": -1.238,
            "X72": -0.575,
            "X73": 1.728,
            "X74": -0.52,
            "X75": 0.29,
            "X76": -0.772,
            "X77": -0.917,
            "X78": 0.724,
            "X79": 0.562,
            "X80": 1.482,
            "X81": -0.279,
            "X82": 0.855,
            "X83": 0.246,
            "X84": 0.979,
            "X85": 0.423,
            "X86": 0.187,
            "X87": 0.809,
            "X88": -1.522,
            "X89": 0.032,
            "X90": 1.257,
            "X91": 1.16,
            "X92": 0.229,
            "X93": 1.397,
            "X94": 0.598,
            "X95": 0.224,
            "X96": -1.619,
            "X97": 1.641,
            "X98": -2.18,
            "X99": 0.232,
            "X100": -0.217,
            "X101": -0.866,
            "X102": -0.57,
            "X103": -0.229,
            "X104": 0.606,
            "X105": 0.543,
            "X106": -0.574,
            "X107": 0.052,
            "X108": 1.28,
            "X109": 1.026,
            "X110": -0.907,
            "X111": 0.829,
            "X112": -0.157,
            "X113": 0.022,
            "X114": 0.496,
            "X115": 0.364,
            "X116": -0.832,
            "X117": 0.373,
            "X118": 0.202,
            "X119": -0.01,
            "X120": -0.839,
            "X121": 0.359,
            "X122": 1.739,
            "X123": -0.043,
            "X124": 1.575,
            "X125": -0.314,
            "X126": 0.908,
            "X127": -0.184,
            "X128": -1.872,
            "X129": 1.145,
            "X130": -1.609,
            "X131": -0.173,
            "X132": -1.007,
            "X133": -0.162,
            "X134": 0.045,
            "X135": 0.926,
            "X136": -0.551,
            "X137": 0.802,
            "X138": 0.847,
            "X139": 0.187,
            "X140": -1.069,
            "X141": -0.918,
            "X142": 1.037,
            "X143": -0.028,
            "X144": -0.916,
            "X145": -0.117,
            "X146": -0.887,
            "X147": -0.39,
            "X148": 0.152,
            "X149": -0.177,
            "X150": -1.142,
            "X151": 0.378,
            "X152": -0.383,
            "X153": -0.152,
            "X154": 1.391,
            "X155": 1.744,
            "X156": 0.184,
            "X157": 1.253,
            "X158": 0.31,
            "X159": -0.448,
            "X160": -0.481,
            "X161": -1.02,
            "X162": 1.342,
            "X163": -0.114,
            "X164": 1.142,
            "X165": 0.346,
            "X166": -0.212,
            "X167": 0.601,
            "X168": 0.11,
            "X169": 0.059,
            "X170": -0.376,
            "X171": 1.07,
            "X172": -1.063,
            "X173": 0.557,
            "X174": -1.75,
            "X175": -1.51,
            "X176": -1.207,
            "X177": -0.193,
            "X178": -0.299,
            "X179": -1.224,
            "X180": -1.305,
            "X181": -0.82,
            "X182": -1.209,
            "X183": 0.81,
            "X184": 1.566,
            "X185": -0.277,
            "X186": 0.053,
            "X187": 1.507,
            "X188": -2.01,
            "X189": 0.026,
            "X190": 0.901,
            "X191": -1.764,
            "X192": -1.12,
            "X193": -0.524,
            "X194": -1.583,
            "X195": -0.549,
            "X196": -0.168,
            "X197": 0.07,
            "X198": -0.726,
            "X199": 1.449,
            "X200": -0.054,
            "X201": 0.652,
            "X202": 1.128,
            "X203": -2.29,
            "X204": 1.103,
            "X205": -0.416,
            "X206": 1.078,
            "X207": -0.848,
            "X208": -0.803,
            "X209": 1.438,
            "X210": -1.284,
            "X211": -1.294,
            "X212": -1.05,
            "X213": -0.086,
            "X214": -0.79,
            "X215": 0.414,
            "X216": -0.014,
            "X217": 1.268,
            "X218": -1.344,
            "X219": 1.189,
            "X220": 0.698,
            "X221": 0.476,
            "X222": -0.682,
            "X223": -0.276,
            "X224": -0.437,
            "X225": 0.958,
            "X226": 0.391,
            "X227": 0.444,
            "X228": -0.379,
            "X229": -1.446,
            "X230": 0.241,
            "X231": -0.442,
            "X232": -1.113,
            "X233": 0.913,
            "X234": -0.571,
            "X235": 1.162,
            "X236": 0.104,
            "X237": 0.729,
            "X238": -1.398,
            "X239": -1.521,
            "X240": -0.424,
            "X241": -0.442,
            "X242": 1.214,
            "X243": -0.461,
            "X244": -1.394,
            "X245": 0.872,
            "X246": -0.956,
            "X247": 0.329,
            "X248": -1.031,
            "X249": -0.444,
            "X250": -0.878,
            "X251": 1.663,
            "X252": 0.497,
            "X253": -1.257,
            "X254": 0.534,
            "X255": -1.249,
            "X256": -0.895,
            "X257": -2.357,
            "X258": 1.663,
            "X259": 0.472,
            "X260": -1.071,
            "X261": 0.584,
            "X262": 1.602,
            "X263": 0.489,
            "X264": -2.107,
            "X265": 0.779,
            "X266": 0.988,
            "X267": -1.719,
            "X268": -0.259,
            "X269": 1.277,
            "X270": -1.51,
            "X271": -1.652,
            "X272": 0.718,
            "X273": 1.547,
            "X274": -1.123,
            "X275": -0.923,
            "X276": -0.077,
            "X277": 2.222,
            "X278": -0.165,
            "X279": 1.626,
            "X280": -0.23,
            "X281": 0.436,
            "X282": -0.6,
            "X283": 0.382,
            "X284": -0.717,
            "X285": 0.065,
            "X286": -0.871,
            "X287": 0.002,
            "X288": 0.899,
            "X289": -0.378,
            "X290": -0.207,
            "X291": 1.583,
            "X292": -1.028,
            "X293": 0.124,
            "X294": -0.051,
            "X295": -0.504,
            "X296": -1.364,
            "X297": -0.81,
            "X298": 0.461,
            "X299": 0.309
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.9302,
            ".pred_1": 0.0698,
            "id": 104,
            "X0": -0.224,
            "X1": 0.176,
            "X2": 1.011,
            "X3": 0.27,
            "X4": 0.029,
            "X5": -0.744,
            "X6": -1.349,
            "X7": 1.178,
            "X8": 0.748,
            "X9": -0.714,
            "X10": -0.938,
            "X11": -0.585,
            "X12": -1.874,
            "X13": 1.682,
            "X14": 0.431,
            "X15": 0.172,
            "X16": 1.128,
            "X17": 0.362,
            "X18": -0.614,
            "X19": 0.828,
            "X20": 0.417,
            "X21": 2.509,
            "X22": -0.96,
            "X23": 0.569,
            "X24": 1.054,
            "X25": 0.307,
            "X26": -0.633,
            "X27": -1.145,
            "X28": -0.473,
            "X29": -0.037,
            "X30": -0.197,
            "X31": 1.105,
            "X32": -0.964,
            "X33": -1.589,
            "X34": 0.133,
            "X35": -0.315,
            "X36": -1.152,
            "X37": -0.768,
            "X38": 0.346,
            "X39": -0.819,
            "X40": -1.493,
            "X41": -0.767,
            "X42": 1.206,
            "X43": -1.439,
            "X44": -1.028,
            "X45": -0.186,
            "X46": -1.321,
            "X47": -0.515,
            "X48": 1.986,
            "X49": -1.062,
            "X50": 0.125,
            "X51": 1.291,
            "X52": 0.623,
            "X53": 0.643,
            "X54": 0.807,
            "X55": 0.541,
            "X56": 0.768,
            "X57": -0.989,
            "X58": 0.517,
            "X59": 0.606,
            "X60": -1.646,
            "X61": -0.045,
            "X62": 1.54,
            "X63": -1.175,
            "X64": 0.187,
            "X65": -0.797,
            "X66": -1.65,
            "X67": 0.081,
            "X68": 0.758,
            "X69": 0.07,
            "X70": 0.424,
            "X71": -0.026,
            "X72": -0.527,
            "X73": 2.421,
            "X74": 1.09,
            "X75": -0.068,
            "X76": -0.728,
            "X77": 1.323,
            "X78": 1.076,
            "X79": -0.682,
            "X80": -1.691,
            "X81": 0.616,
            "X82": -0.044,
            "X83": -2.112,
            "X84": 1.094,
            "X85": 0.073,
            "X86": -0.425,
            "X87": -0.39,
            "X88": 0.09,
            "X89": 0.485,
            "X90": -0.796,
            "X91": 0.112,
            "X92": -0.535,
            "X93": 0.106,
            "X94": -0.023,
            "X95": 0.385,
            "X96": 0.463,
            "X97": -0.415,
            "X98": 0.493,
            "X99": -0.728,
            "X100": -0.716,
            "X101": 0.313,
            "X102": -1.942,
            "X103": -1.183,
            "X104": -0.418,
            "X105": -1.889,
            "X106": 0.963,
            "X107": 0.986,
            "X108": 0.19,
            "X109": -0.639,
            "X110": 2.005,
            "X111": 0.111,
            "X112": 0.655,
            "X113": 0.8,
            "X114": 0.551,
            "X115": 0.422,
            "X116": 0.589,
            "X117": -0.054,
            "X118": 0.598,
            "X119": 1.055,
            "X120": 0.478,
            "X121": -0.743,
            "X122": 2.6,
            "X123": -1.301,
            "X124": 1.362,
            "X125": 0.121,
            "X126": -1.407,
            "X127": -1.337,
            "X128": 0.277,
            "X129": -0.346,
            "X130": -0.811,
            "X131": -2.041,
            "X132": 0.7,
            "X133": 1.007,
            "X134": 0.852,
            "X135": 0.288,
            "X136": -0.887,
            "X137": -2.094,
            "X138": -2.186,
            "X139": -0.221,
            "X140": 0.191,
            "X141": 0.702,
            "X142": -0.971,
            "X143": -0.243,
            "X144": 0.705,
            "X145": -1.064,
            "X146": -0.45,
            "X147": 0.27,
            "X148": 0.489,
            "X149": 0.361,
            "X150": 0.225,
            "X151": -0.377,
            "X152": 0.751,
            "X153": 0.778,
            "X154": -0.378,
            "X155": 0.892,
            "X156": 1.042,
            "X157": 0.048,
            "X158": -0.351,
            "X159": -0.259,
            "X160": -1.371,
            "X161": 0.387,
            "X162": 1.766,
            "X163": -0.168,
            "X164": 1.476,
            "X165": 0.434,
            "X166": 0.183,
            "X167": 0.04,
            "X168": -1.085,
            "X169": 0.296,
            "X170": 0.784,
            "X171": -0.928,
            "X172": 0.69,
            "X173": 0.808,
            "X174": -0.975,
            "X175": -1.16,
            "X176": -0.178,
            "X177": -0.389,
            "X178": -0.644,
            "X179": 0.898,
            "X180": -0.01,
            "X181": 1.12,
            "X182": -0.315,
            "X183": -1.246,
            "X184": 0.3,
            "X185": 1.203,
            "X186": 0.025,
            "X187": 1.797,
            "X188": -0.905,
            "X189": 0.221,
            "X190": 0.426,
            "X191": 1.514,
            "X192": -0.178,
            "X193": 0.804,
            "X194": -0.272,
            "X195": 0.589,
            "X196": 1.485,
            "X197": -1.277,
            "X198": 1.861,
            "X199": 1.826,
            "X200": -2.015,
            "X201": 0.345,
            "X202": -1.668,
            "X203": -1.663,
            "X204": 0.05,
            "X205": 1.339,
            "X206": -0.927,
            "X207": 0.918,
            "X208": -0.452,
            "X209": 1.693,
            "X210": -1.468,
            "X211": 0.47,
            "X212": 0.181,
            "X213": 0.067,
            "X214": -0.307,
            "X215": 0.865,
            "X216": -0.099,
            "X217": 0.571,
            "X218": -1.369,
            "X219": -0.292,
            "X220": 0.621,
            "X221": 0.963,
            "X222": -0.902,
            "X223": 1.188,
            "X224": 0.566,
            "X225": 0.084,
            "X226": -0.006,
            "X227": 1.047,
            "X228": 0.209,
            "X229": -0.096,
            "X230": 0.145,
            "X231": 0.442,
            "X232": 0.97,
            "X233": -0.745,
            "X234": 1.26,
            "X235": 0.547,
            "X236": -0.757,
            "X237": 1.505,
            "X238": 0.102,
            "X239": 0.502,
            "X240": -1.551,
            "X241": 0.268,
            "X242": 1.505,
            "X243": 1.175,
            "X244": -1.13,
            "X245": -0.271,
            "X246": 0.373,
            "X247": 0.171,
            "X248": -0.038,
            "X249": 0.002,
            "X250": 1.341,
            "X251": 1.292,
            "X252": -0.193,
            "X253": 0.168,
            "X254": 0.068,
            "X255": 0.844,
            "X256": -0.867,
            "X257": 0.277,
            "X258": 1.291,
            "X259": -0.003,
            "X260": -2.552,
            "X261": -1.185,
            "X262": 1.336,
            "X263": 1.29,
            "X264": 0.492,
            "X265": -0.54,
            "X266": 0.77,
            "X267": -0.163,
            "X268": -1.292,
            "X269": 0.743,
            "X270": 0.057,
            "X271": 0.998,
            "X272": 0.505,
            "X273": -0.142,
            "X274": 0.795,
            "X275": 0.395,
            "X276": 0.223,
            "X277": 0.949,
            "X278": 2.072,
            "X279": 0.952,
            "X280": -1.984,
            "X281": -1.217,
            "X282": 0.552,
            "X283": -1.585,
            "X284": -0.439,
            "X285": -0.266,
            "X286": -1.354,
            "X287": -0.224,
            "X288": -0.048,
            "X289": 0.585,
            "X290": 1.05,
            "X291": -1.157,
            "X292": 0.044,
            "X293": -0.022,
            "X294": 1.027,
            "X295": 1.007,
            "X296": -2.457,
            "X297": -0.129,
            "X298": 0.041,
            "X299": -0.081
          },
          {
            ".pred_class": 1,
            ".pred_0": 0.4433,
            ".pred_1": 0.5567,
            "id": 200,
            "X0": -0.907,
            "X1": 0.275,
            "X2": -1.556,
            "X3": 0.758,
            "X4": 0.35,
            "X5": -2.057,
            "X6": -0.589,
            "X7": 0.367,
            "X8": 0.371,
            "X9": 0.836,
            "X10": 1.209,
            "X11": -0.355,
            "X12": -0.392,
            "X13": 0.575,
            "X14": -2.024,
            "X15": 0.703,
            "X16": -0.39,
            "X17": -0.5,
            "X18": 1.498,
            "X19": 0.906,
            "X20": -0.903,
            "X21": 0.561,
            "X22": 0.664,
            "X23": -1.762,
            "X24": -1.093,
            "X25": -0.023,
            "X26": -0.088,
            "X27": -0.188,
            "X28": 1.333,
            "X29": 0.101,
            "X30": 1.67,
            "X31": -0.706,
            "X32": -1.327,
            "X33": 0.476,
            "X34": -0.351,
            "X35": 0.019,
            "X36": -1.264,
            "X37": 0.292,
            "X38": -1.984,
            "X39": 0.57,
            "X40": -0.241,
            "X41": -0.727,
            "X42": 0.916,
            "X43": -0.422,
            "X44": 1.137,
            "X45": 1.339,
            "X46": -0.913,
            "X47": 1.175,
            "X48": 0.244,
            "X49": 0.072,
            "X50": 0.831,
            "X51": 0.071,
            "X52": -1.307,
            "X53": -0.44,
            "X54": -0.652,
            "X55": 0.91,
            "X56": -0.767,
            "X57": 0.133,
            "X58": 0.709,
            "X59": -0.499,
            "X60": -1.312,
            "X61": -1.973,
            "X62": 0.115,
            "X63": -0.457,
            "X64": 0.977,
            "X65": -0.138,
            "X66": 0.397,
            "X67": 0.718,
            "X68": 0.13,
            "X69": 1.071,
            "X70": 0.31,
            "X71": -0.55,
            "X72": -0.74,
            "X73": -1.106,
            "X74": 1.924,
            "X75": 0.334,
            "X76": -1.974,
            "X77": 1.035,
            "X78": -0.484,
            "X79": 0.808,
            "X80": 0.164,
            "X81": 1.146,
            "X82": -1.001,
            "X83": -0.665,
            "X84": -0.322,
            "X85": -0.177,
            "X86": 2.762,
            "X87": -0.698,
            "X88": -0.764,
            "X89": 0.593,
            "X90": -0.054,
            "X91": -0.908,
            "X92": 0.028,
            "X93": 0.819,
            "X94": 0.323,
            "X95": -1.187,
            "X96": -0.552,
            "X97": 1.001,
            "X98": 0.437,
            "X99": 0.026,
            "X100": -0.337,
            "X101": 1.382,
            "X102": 0.104,
            "X103": 0.537,
            "X104": 1.976,
            "X105": 2.172,
            "X106": -0.648,
            "X107": 0.081,
            "X108": -1.728,
            "X109": 1.949,
            "X110": -0.583,
            "X111": 0.671,
            "X112": 0.101,
            "X113": 0.276,
            "X114": -0.14,
            "X115": -0.164,
            "X116": -0.998,
            "X117": -1.227,
            "X118": -0.406,
            "X119": 1.048,
            "X120": -2.182,
            "X121": 0.514,
            "X122": 0.633,
            "X123": 1.074,
            "X124": -0.979,
            "X125": -2.112,
            "X126": 0.293,
            "X127": 0.399,
            "X128": 0.581,
            "X129": -0.232,
            "X130": 0.853,
            "X131": 0.849,
            "X132": 3.002,
            "X133": 0.638,
            "X134": 0.573,
            "X135": -0.317,
            "X136": 0.875,
            "X137": 0.435,
            "X138": -0.444,
            "X139": 0.057,
            "X140": -1.932,
            "X141": -1.087,
            "X142": -0.055,
            "X143": 0.154,
            "X144": 0.055,
            "X145": -2.44,
            "X146": -0.587,
            "X147": 1.416,
            "X148": -0.548,
            "X149": -0.487,
            "X150": -0.495,
            "X151": -0.36,
            "X152": 0.809,
            "X153": 1.384,
            "X154": -1.309,
            "X155": 0.471,
            "X156": -0.587,
            "X157": -0.309,
            "X158": -1.486,
            "X159": -1.145,
            "X160": 0.794,
            "X161": 0.588,
            "X162": -1.434,
            "X163": -0.154,
            "X164": 0.144,
            "X165": -1.086,
            "X166": 0.361,
            "X167": 0.084,
            "X168": -0.788,
            "X169": 1.254,
            "X170": 0.821,
            "X171": -1.628,
            "X172": 0.78,
            "X173": 0.727,
            "X174": 1.088,
            "X175": 1.663,
            "X176": -1.064,
            "X177": 0.189,
            "X178": -0.38,
            "X179": -0.435,
            "X180": 0.43,
            "X181": -0.527,
            "X182": 1.148,
            "X183": 0.219,
            "X184": -0.533,
            "X185": -0.54,
            "X186": -1.323,
            "X187": -0.782,
            "X188": 0.07,
            "X189": -1.028,
            "X190": 1.593,
            "X191": 0.015,
            "X192": 0.732,
            "X193": 0.263,
            "X194": 0.85,
            "X195": 0.422,
            "X196": 0.878,
            "X197": 0.066,
            "X198": 0.455,
            "X199": -1.753,
            "X200": -1.48,
            "X201": 0.225,
            "X202": 1.204,
            "X203": -1.143,
            "X204": -2.531,
            "X205": -1.691,
            "X206": -0.476,
            "X207": -0.065,
            "X208": 0.429,
            "X209": 1.161,
            "X210": 3.04,
            "X211": -0.401,
            "X212": -0.027,
            "X213": 0.048,
            "X214": 1.006,
            "X215": -0.663,
            "X216": -0.976,
            "X217": 0.734,
            "X218": 1.476,
            "X219": 0.213,
            "X220": -0.291,
            "X221": -0.984,
            "X222": 1.646,
            "X223": 0.951,
            "X224": 1.363,
            "X225": -0.266,
            "X226": -1.027,
            "X227": 0.546,
            "X228": 0.737,
            "X229": -0.981,
            "X230": 0.956,
            "X231": -0.232,
            "X232": 0.175,
            "X233": -1.586,
            "X234": -0.058,
            "X235": -0.425,
            "X236": -0.33,
            "X237": -0.328,
            "X238": 0.356,
            "X239": -1.485,
            "X240": -1.025,
            "X241": 1.684,
            "X242": 0.446,
            "X243": 0.458,
            "X244": -0.536,
            "X245": 0.696,
            "X246": 0.02,
            "X247": -1.135,
            "X248": -0.172,
            "X249": 1.12,
            "X250": -1.537,
            "X251": -1.365,
            "X252": -0.171,
            "X253": -0.332,
            "X254": 1.801,
            "X255": 1.272,
            "X256": 0.314,
            "X257": 0.272,
            "X258": 0.669,
            "X259": -1.594,
            "X260": -0.474,
            "X261": 0.295,
            "X262": -2.163,
            "X263": -0.828,
            "X264": 0.711,
            "X265": -0.234,
            "X266": -1.19,
            "X267": 0.034,
            "X268": 0.848,
            "X269": 0.28,
            "X270": 0.15,
            "X271": -0.934,
            "X272": 0.155,
            "X273": 3.286,
            "X274": 0.51,
            "X275": -1.813,
            "X276": -0.155,
            "X277": 0.016,
            "X278": 1.305,
            "X279": 1.547,
            "X280": -1.072,
            "X281": -0.651,
            "X282": -1.38,
            "X283": 0.735,
            "X284": 0.526,
            "X285": -0.406,
            "X286": 0.712,
            "X287": -1.123,
            "X288": -0.831,
            "X289": -1.515,
            "X290": 1.97,
            "X291": -1.097,
            "X292": -1.14,
            "X293": -0.202,
            "X294": -0.039,
            "X295": -0.967,
            "X296": -0.66,
            "X297": 0.406,
            "X298": -0.819,
            "X299": 2.55
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8747,
            ".pred_1": 0.1253,
            "id": 216,
            "X0": -1.446,
            "X1": -0.54,
            "X2": 0.595,
            "X3": -0.153,
            "X4": 0.354,
            "X5": -0.289,
            "X6": -0.498,
            "X7": -0.641,
            "X8": 1.645,
            "X9": -0.556,
            "X10": -0.167,
            "X11": -1.861,
            "X12": -0.689,
            "X13": 0.773,
            "X14": 0.36,
            "X15": 0.696,
            "X16": 0.414,
            "X17": 1.373,
            "X18": -0.928,
            "X19": 0.84,
            "X20": -0.087,
            "X21": 0.276,
            "X22": -0.094,
            "X23": 0.647,
            "X24": 1.508,
            "X25": -0.686,
            "X26": -1.036,
            "X27": -0.069,
            "X28": -0.282,
            "X29": 2.054,
            "X30": 0.506,
            "X31": 0.728,
            "X32": 0.308,
            "X33": 0.802,
            "X34": 0.129,
            "X35": -0.796,
            "X36": 1.095,
            "X37": 1.427,
            "X38": 0.574,
            "X39": 1.295,
            "X40": -0.576,
            "X41": 0.418,
            "X42": 0.499,
            "X43": -0.95,
            "X44": -0.669,
            "X45": -1.919,
            "X46": 1.525,
            "X47": 1.292,
            "X48": 0.724,
            "X49": 0.145,
            "X50": 0.085,
            "X51": -1.192,
            "X52": 0.524,
            "X53": -0.678,
            "X54": 0.252,
            "X55": -1.031,
            "X56": -1.003,
            "X57": -1.192,
            "X58": -2.802,
            "X59": -0.903,
            "X60": 1.233,
            "X61": -1.318,
            "X62": 0.91,
            "X63": -1.324,
            "X64": 1.116,
            "X65": -0.094,
            "X66": 0.164,
            "X67": 0.062,
            "X68": 1.714,
            "X69": -0.713,
            "X70": -0.116,
            "X71": 1.234,
            "X72": -0.742,
            "X73": 0.225,
            "X74": -0.115,
            "X75": 0.648,
            "X76": -1.204,
            "X77": -0.65,
            "X78": 0.319,
            "X79": -0.332,
            "X80": 0.86,
            "X81": 0.468,
            "X82": -1.625,
            "X83": -0.262,
            "X84": 0.261,
            "X85": -0.256,
            "X86": 0.471,
            "X87": -1.135,
            "X88": 1.139,
            "X89": 1.5,
            "X90": -0.301,
            "X91": -0.368,
            "X92": -0.946,
            "X93": -0.735,
            "X94": -0.978,
            "X95": 0.71,
            "X96": 0.364,
            "X97": 0.173,
            "X98": -0.316,
            "X99": 2.323,
            "X100": 0.216,
            "X101": 0.537,
            "X102": 0.826,
            "X103": 0.228,
            "X104": 0.492,
            "X105": -0.767,
            "X106": -0.954,
            "X107": 0.305,
            "X108": 0.323,
            "X109": -0.423,
            "X110": 0.463,
            "X111": -1.789,
            "X112": 0.342,
            "X113": 2,
            "X114": -0.66,
            "X115": 0.843,
            "X116": -0.016,
            "X117": 0.129,
            "X118": -0.872,
            "X119": 2.061,
            "X120": -0.906,
            "X121": 0.244,
            "X122": -0.96,
            "X123": 2.262,
            "X124": 1.318,
            "X125": -1.511,
            "X126": 0.119,
            "X127": -1.563,
            "X128": -0.245,
            "X129": -0.372,
            "X130": -1.626,
            "X131": 1.192,
            "X132": 0.539,
            "X133": 1.743,
            "X134": -1.071,
            "X135": -2.966,
            "X136": 0.316,
            "X137": 0.348,
            "X138": -2.017,
            "X139": 1.505,
            "X140": -1.246,
            "X141": 1.951,
            "X142": -1.211,
            "X143": -1.378,
            "X144": 1.389,
            "X145": 0.072,
            "X146": 0.575,
            "X147": 1.31,
            "X148": -1.038,
            "X149": -0.176,
            "X150": 0.744,
            "X151": 0.96,
            "X152": -0.867,
            "X153": 0.088,
            "X154": -0.889,
            "X155": -1.2,
            "X156": 0.379,
            "X157": 0.168,
            "X158": 0.082,
            "X159": 1.495,
            "X160": 1.119,
            "X161": 0.415,
            "X162": -1.609,
            "X163": 0.841,
            "X164": 1.331,
            "X165": -0.237,
            "X166": 0.829,
            "X167": -0.368,
            "X168": -1.144,
            "X169": -0.533,
            "X170": -1.466,
            "X171": 1.392,
            "X172": 0.133,
            "X173": -0.656,
            "X174": -0.363,
            "X175": 0.719,
            "X176": 0.054,
            "X177": 1.113,
            "X178": 0.388,
            "X179": -0.434,
            "X180": 1.568,
            "X181": 0.731,
            "X182": -1.02,
            "X183": 1.766,
            "X184": -1.375,
            "X185": 1.161,
            "X186": -0.262,
            "X187": 1.137,
            "X188": 0.831,
            "X189": -0.037,
            "X190": 2.204,
            "X191": 0.35,
            "X192": -0.61,
            "X193": -1.323,
            "X194": -0.266,
            "X195": 0.577,
            "X196": -2.73,
            "X197": -2.781,
            "X198": -1.291,
            "X199": -0.843,
            "X200": 0.159,
            "X201": -0.392,
            "X202": -0.253,
            "X203": 1.636,
            "X204": 0.991,
            "X205": -1.262,
            "X206": 0.811,
            "X207": 0.112,
            "X208": 0.182,
            "X209": 0.607,
            "X210": 2.313,
            "X211": -1.845,
            "X212": 1.261,
            "X213": -0.503,
            "X214": 0.57,
            "X215": -1.03,
            "X216": -1.242,
            "X217": 0.44,
            "X218": 0.084,
            "X219": 0.757,
            "X220": 0.65,
            "X221": 0.696,
            "X222": 0.057,
            "X223": 0.59,
            "X224": -0.422,
            "X225": -0.153,
            "X226": -1.018,
            "X227": 0.908,
            "X228": -0.945,
            "X229": -1.685,
            "X230": -0.252,
            "X231": 0.325,
            "X232": 0.036,
            "X233": 1.506,
            "X234": -0.339,
            "X235": 0.559,
            "X236": 1.351,
            "X237": -1.729,
            "X238": 0.758,
            "X239": -0.88,
            "X240": 0.119,
            "X241": -0.963,
            "X242": 0.073,
            "X243": -0.98,
            "X244": -0.186,
            "X245": -1.783,
            "X246": 0.185,
            "X247": -1.087,
            "X248": -0.454,
            "X249": -0.474,
            "X250": 1.434,
            "X251": 0.644,
            "X252": -0.903,
            "X253": -0.435,
            "X254": -2.385,
            "X255": 0.825,
            "X256": -0.771,
            "X257": 0.958,
            "X258": -0.609,
            "X259": -1.04,
            "X260": -0.887,
            "X261": 1.626,
            "X262": -0.723,
            "X263": 1.239,
            "X264": 0.17,
            "X265": -1.431,
            "X266": 0.129,
            "X267": 1.184,
            "X268": 0.167,
            "X269": 1.029,
            "X270": 0.371,
            "X271": 0.639,
            "X272": -0.53,
            "X273": 0.626,
            "X274": 0.63,
            "X275": -0.403,
            "X276": -1.208,
            "X277": -0.833,
            "X278": 1.985,
            "X279": -0.544,
            "X280": -0.913,
            "X281": 0.443,
            "X282": 1.351,
            "X283": 0.192,
            "X284": 0.549,
            "X285": -0.483,
            "X286": -1.02,
            "X287": -0.26,
            "X288": 2.442,
            "X289": 0.6,
            "X290": 0.878,
            "X291": 0.547,
            "X292": -0.873,
            "X293": 0.291,
            "X294": -0.228,
            "X295": 1.398,
            "X296": 0.235,
            "X297": -0.589,
            "X298": -1.379,
            "X299": -1.998
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7882,
            ".pred_1": 0.2118,
            "id": 236,
            "X0": -0.943,
            "X1": 0.776,
            "X2": 0.151,
            "X3": -0.708,
            "X4": 1.85,
            "X5": -0.663,
            "X6": -0.824,
            "X7": -0.37,
            "X8": 1.055,
            "X9": 1.933,
            "X10": 1.106,
            "X11": -0.933,
            "X12": 1.479,
            "X13": -0.12,
            "X14": 1.139,
            "X15": 1.183,
            "X16": 0.418,
            "X17": -0.469,
            "X18": -0.497,
            "X19": -0.166,
            "X20": -0.367,
            "X21": -0.818,
            "X22": 0.09,
            "X23": -0.064,
            "X24": -1.326,
            "X25": -1.193,
            "X26": -0.229,
            "X27": 1.884,
            "X28": -0.622,
            "X29": 0.606,
            "X30": -0.652,
            "X31": 3.315,
            "X32": -0.347,
            "X33": 0.874,
            "X34": -0.416,
            "X35": 0.128,
            "X36": 0.289,
            "X37": -0.544,
            "X38": -1.34,
            "X39": -1.15,
            "X40": 0.118,
            "X41": -0.862,
            "X42": 0.252,
            "X43": -0.172,
            "X44": 0.048,
            "X45": -0.12,
            "X46": 0.448,
            "X47": -0.329,
            "X48": 0.992,
            "X49": -0.875,
            "X50": -0.279,
            "X51": -0.383,
            "X52": -0.849,
            "X53": -0.961,
            "X54": 0.71,
            "X55": -1.908,
            "X56": -0.954,
            "X57": -0.137,
            "X58": 2.215,
            "X59": 1.518,
            "X60": 1.851,
            "X61": -0.342,
            "X62": 1.033,
            "X63": -1.035,
            "X64": -1.603,
            "X65": -0.198,
            "X66": 0.758,
            "X67": 1.697,
            "X68": -0.073,
            "X69": 0.034,
            "X70": 0.854,
            "X71": -1.398,
            "X72": 0.093,
            "X73": -0.157,
            "X74": 0.099,
            "X75": 0.283,
            "X76": 2.131,
            "X77": -0.938,
            "X78": 0.317,
            "X79": -1.348,
            "X80": -0.326,
            "X81": 1.407,
            "X82": -0.387,
            "X83": 0.747,
            "X84": -0.812,
            "X85": 0.651,
            "X86": -1.677,
            "X87": -0.166,
            "X88": -0.102,
            "X89": 1.731,
            "X90": 0.549,
            "X91": 1.052,
            "X92": 0.792,
            "X93": -0.49,
            "X94": 2.27,
            "X95": -0.582,
            "X96": 0.13,
            "X97": -1.636,
            "X98": -0.111,
            "X99": 0.653,
            "X100": -1.087,
            "X101": -0.029,
            "X102": -1.114,
            "X103": -0.259,
            "X104": 0.481,
            "X105": 0.457,
            "X106": -0.993,
            "X107": 2.259,
            "X108": 0.051,
            "X109": -0.679,
            "X110": 1.369,
            "X111": 0.088,
            "X112": 0.589,
            "X113": -0.699,
            "X114": 0.145,
            "X115": -1.353,
            "X116": 1.156,
            "X117": -1.13,
            "X118": 0.654,
            "X119": 0.406,
            "X120": 0.581,
            "X121": -0.943,
            "X122": -0.994,
            "X123": 0.609,
            "X124": -0.955,
            "X125": 0.378,
            "X126": -0.018,
            "X127": 0.893,
            "X128": 1.94,
            "X129": 0.685,
            "X130": 0.227,
            "X131": 0.326,
            "X132": -0.216,
            "X133": 0.42,
            "X134": 0.566,
            "X135": 0.58,
            "X136": -0.371,
            "X137": 0.392,
            "X138": -0.041,
            "X139": 0.926,
            "X140": -0.364,
            "X141": 0.011,
            "X142": -0.489,
            "X143": 0.146,
            "X144": -0.006,
            "X145": 2.745,
            "X146": 0.428,
            "X147": 0.272,
            "X148": 0.814,
            "X149": 1.574,
            "X150": -0.945,
            "X151": -0.499,
            "X152": 0.567,
            "X153": -0.731,
            "X154": -0.916,
            "X155": 1.068,
            "X156": 0.598,
            "X157": -0.145,
            "X158": -0.15,
            "X159": -0.258,
            "X160": 1.583,
            "X161": -1.322,
            "X162": 0.988,
            "X163": 0.273,
            "X164": -1.126,
            "X165": 2.15,
            "X166": 0.006,
            "X167": -0.133,
            "X168": -1.228,
            "X169": -0.482,
            "X170": -0.218,
            "X171": -1.051,
            "X172": -0.4,
            "X173": 1.164,
            "X174": 0.135,
            "X175": -0.626,
            "X176": -0.384,
            "X177": 1.317,
            "X178": -1.296,
            "X179": 0.762,
            "X180": 0.371,
            "X181": -0.017,
            "X182": 0.837,
            "X183": 0.312,
            "X184": 0.886,
            "X185": 0.984,
            "X186": -0.558,
            "X187": -1.205,
            "X188": -0.416,
            "X189": -0.262,
            "X190": -0.136,
            "X191": 0.983,
            "X192": -0.722,
            "X193": -0.202,
            "X194": -0.307,
            "X195": -0.957,
            "X196": 0.247,
            "X197": 0.968,
            "X198": 0.788,
            "X199": 0.046,
            "X200": 1.353,
            "X201": -0.995,
            "X202": -1.658,
            "X203": 0.314,
            "X204": 0.4,
            "X205": -0.248,
            "X206": -0.009,
            "X207": -0.268,
            "X208": 1.201,
            "X209": 0.424,
            "X210": 0.155,
            "X211": -1.115,
            "X212": 0.926,
            "X213": -0.013,
            "X214": -0.853,
            "X215": 0.293,
            "X216": -0.208,
            "X217": -1.559,
            "X218": 0.971,
            "X219": 0.266,
            "X220": 0.083,
            "X221": -0.906,
            "X222": -0.489,
            "X223": 1,
            "X224": 0.945,
            "X225": -0.442,
            "X226": -0.42,
            "X227": -0.799,
            "X228": -0.157,
            "X229": -0.147,
            "X230": -0.843,
            "X231": -0.389,
            "X232": 0.088,
            "X233": -0.685,
            "X234": -1.184,
            "X235": 1.073,
            "X236": 0.332,
            "X237": -0.951,
            "X238": -0.792,
            "X239": -2.359,
            "X240": -1.793,
            "X241": -1.311,
            "X242": 0.062,
            "X243": 0.189,
            "X244": -0.486,
            "X245": -2.533,
            "X246": -0.718,
            "X247": -0.579,
            "X248": -0.38,
            "X249": 1.036,
            "X250": 1.484,
            "X251": 0.583,
            "X252": 0.203,
            "X253": 2.207,
            "X254": 0.233,
            "X255": -0.497,
            "X256": -2.019,
            "X257": -0.496,
            "X258": 0.611,
            "X259": -0.449,
            "X260": -0.072,
            "X261": 0.129,
            "X262": -1.52,
            "X263": 0.28,
            "X264": 0.178,
            "X265": -0.267,
            "X266": 1.573,
            "X267": -0.855,
            "X268": -2.292,
            "X269": 0.874,
            "X270": 1.361,
            "X271": 0.862,
            "X272": 0.559,
            "X273": -0.127,
            "X274": -1.292,
            "X275": -0.749,
            "X276": -1.411,
            "X277": -0.796,
            "X278": 0.734,
            "X279": 0.849,
            "X280": -0.293,
            "X281": -1.197,
            "X282": -0.225,
            "X283": -0.038,
            "X284": -0.569,
            "X285": 0.724,
            "X286": 0.234,
            "X287": 1.143,
            "X288": -0.009,
            "X289": -0.667,
            "X290": 0.515,
            "X291": 0.312,
            "X292": -0.351,
            "X293": -0.473,
            "X294": 0.447,
            "X295": 0.443,
            "X296": -1.104,
            "X297": -0.987,
            "X298": 0.642,
            "X299": -0.181
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6608,
            ".pred_1": 0.3392,
            "id": 240,
            "X0": 1.462,
            "X1": 1.68,
            "X2": 1.165,
            "X3": -0.394,
            "X4": -1.098,
            "X5": -1.031,
            "X6": 0.863,
            "X7": -0.242,
            "X8": 1.588,
            "X9": 0.265,
            "X10": -0.94,
            "X11": -1.394,
            "X12": -2.262,
            "X13": 1.778,
            "X14": -0.471,
            "X15": -0.098,
            "X16": 1.305,
            "X17": -0.513,
            "X18": -0.397,
            "X19": 1.405,
            "X20": -0.673,
            "X21": 0.143,
            "X22": 0.216,
            "X23": -0.94,
            "X24": -1.372,
            "X25": 0.595,
            "X26": -0.798,
            "X27": -0.793,
            "X28": -1.445,
            "X29": -1.29,
            "X30": 0.372,
            "X31": -1.384,
            "X32": 1.183,
            "X33": 2.406,
            "X34": 0.547,
            "X35": -0.606,
            "X36": 0.846,
            "X37": -0.022,
            "X38": 0.535,
            "X39": 0.995,
            "X40": 0.521,
            "X41": 0.239,
            "X42": 1.129,
            "X43": -0.732,
            "X44": -0.638,
            "X45": -0.418,
            "X46": -0.094,
            "X47": 1.423,
            "X48": 0.984,
            "X49": 0.148,
            "X50": -0.483,
            "X51": -0.004,
            "X52": 0.537,
            "X53": -0.243,
            "X54": -0.808,
            "X55": 0.957,
            "X56": -0.744,
            "X57": 0.764,
            "X58": -0.283,
            "X59": 0.872,
            "X60": 0.954,
            "X61": -1.185,
            "X62": -1.349,
            "X63": -1.063,
            "X64": 1.215,
            "X65": 0.083,
            "X66": 0.935,
            "X67": -1.302,
            "X68": -1.162,
            "X69": -0.149,
            "X70": 0.052,
            "X71": 0.421,
            "X72": 2.068,
            "X73": -1.035,
            "X74": 0.349,
            "X75": 0.176,
            "X76": -0.707,
            "X77": -1.483,
            "X78": 0.21,
            "X79": -0.877,
            "X80": 1.873,
            "X81": 0.378,
            "X82": 0.233,
            "X83": -1.644,
            "X84": 1.278,
            "X85": -0.078,
            "X86": 0.755,
            "X87": -0.083,
            "X88": -0.139,
            "X89": -0.729,
            "X90": -0.564,
            "X91": 0.493,
            "X92": 0.47,
            "X93": 0.456,
            "X94": -2.63,
            "X95": -0.51,
            "X96": 0.574,
            "X97": -0.953,
            "X98": 0.179,
            "X99": -1.96,
            "X100": 0.854,
            "X101": 2.081,
            "X102": 0.175,
            "X103": 0.461,
            "X104": -1.28,
            "X105": -0.276,
            "X106": -0.644,
            "X107": -0.356,
            "X108": 1.636,
            "X109": 0.925,
            "X110": -0.8,
            "X111": 1.367,
            "X112": -0.306,
            "X113": 0.778,
            "X114": -0.794,
            "X115": 1.11,
            "X116": -0.045,
            "X117": 0.535,
            "X118": 0.047,
            "X119": 0.679,
            "X120": -0.284,
            "X121": 1.525,
            "X122": 0.872,
            "X123": 0.433,
            "X124": 1.174,
            "X125": 1.375,
            "X126": 0.993,
            "X127": 1.562,
            "X128": -0.481,
            "X129": 0.281,
            "X130": -0.319,
            "X131": 0.567,
            "X132": -0.375,
            "X133": 0.111,
            "X134": -0.522,
            "X135": 0.715,
            "X136": -0.222,
            "X137": 1.281,
            "X138": -0.408,
            "X139": 0.807,
            "X140": 0.181,
            "X141": -0.478,
            "X142": -0.756,
            "X143": 1.684,
            "X144": -0.723,
            "X145": 1.194,
            "X146": 0.324,
            "X147": 0.462,
            "X148": -1.797,
            "X149": 0.494,
            "X150": 0.465,
            "X151": -0.646,
            "X152": -2.077,
            "X153": 1.344,
            "X154": 0.051,
            "X155": 1.253,
            "X156": -0.747,
            "X157": 0.625,
            "X158": 0.347,
            "X159": -0.25,
            "X160": -0.147,
            "X161": 1.7,
            "X162": 0.81,
            "X163": -0.798,
            "X164": 0.725,
            "X165": 2.849,
            "X166": 1.076,
            "X167": 0.615,
            "X168": 0.172,
            "X169": 1.599,
            "X170": -0.505,
            "X171": -0.237,
            "X172": -0.374,
            "X173": -2.431,
            "X174": 0.172,
            "X175": 0.435,
            "X176": -1.737,
            "X177": -0.867,
            "X178": -0.29,
            "X179": -0.713,
            "X180": -1.043,
            "X181": -0.497,
            "X182": -0.102,
            "X183": 0.399,
            "X184": 0.838,
            "X185": -0.04,
            "X186": 0.617,
            "X187": 0.585,
            "X188": 1.091,
            "X189": -0.116,
            "X190": 0.945,
            "X191": -0.705,
            "X192": 0.675,
            "X193": 1.46,
            "X194": 1.058,
            "X195": -0.183,
            "X196": 0.587,
            "X197": 1.441,
            "X198": -0.086,
            "X199": -0.44,
            "X200": 0.114,
            "X201": 1.827,
            "X202": 0.392,
            "X203": 1.199,
            "X204": -1.139,
            "X205": -1.649,
            "X206": -0.235,
            "X207": 0.141,
            "X208": -0.574,
            "X209": 0.588,
            "X210": -1.451,
            "X211": -0.785,
            "X212": -1.309,
            "X213": -1.889,
            "X214": -0.163,
            "X215": -0.471,
            "X216": -0.145,
            "X217": 0.753,
            "X218": 0.262,
            "X219": 0.847,
            "X220": 1.902,
            "X221": -0.997,
            "X222": -0.24,
            "X223": 0.433,
            "X224": 0.797,
            "X225": -1.034,
            "X226": 0.968,
            "X227": -0.339,
            "X228": 0.603,
            "X229": -0.406,
            "X230": -0.606,
            "X231": -1.1,
            "X232": -0.39,
            "X233": -0.8,
            "X234": 1.388,
            "X235": -1.058,
            "X236": -0.377,
            "X237": -2.552,
            "X238": 0.191,
            "X239": 0.848,
            "X240": 0.645,
            "X241": -0.677,
            "X242": -1.128,
            "X243": 0.166,
            "X244": 0.715,
            "X245": -0.37,
            "X246": -0.008,
            "X247": 0.271,
            "X248": 0.535,
            "X249": 0.006,
            "X250": 0.655,
            "X251": 0.507,
            "X252": 0.257,
            "X253": -0.872,
            "X254": -0.233,
            "X255": 0.052,
            "X256": 0.508,
            "X257": 0.261,
            "X258": 1.164,
            "X259": -1.309,
            "X260": 0.691,
            "X261": -0.398,
            "X262": -0.484,
            "X263": -0.373,
            "X264": 1.168,
            "X265": 0.541,
            "X266": -0.545,
            "X267": 0.652,
            "X268": -0.925,
            "X269": -0.905,
            "X270": -0.723,
            "X271": -0.501,
            "X272": 0.125,
            "X273": 0.826,
            "X274": 0.02,
            "X275": -0.331,
            "X276": -0.133,
            "X277": 1.101,
            "X278": -0.134,
            "X279": 0.359,
            "X280": -0.088,
            "X281": 0.494,
            "X282": -0.924,
            "X283": 0.026,
            "X284": 0.427,
            "X285": -1.74,
            "X286": -0.556,
            "X287": 0.598,
            "X288": -2.201,
            "X289": 1.031,
            "X290": 2.265,
            "X291": -0.906,
            "X292": 0.91,
            "X293": 1.457,
            "X294": -0.341,
            "X295": 0.526,
            "X296": 1.073,
            "X297": -0.066,
            "X298": 0.917,
            "X299": -0.296
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.764,
            ".pred_1": 0.236,
            "id": 67,
            "X0": 0.18,
            "X1": -0.294,
            "X2": 0.213,
            "X3": -1.315,
            "X4": 1.405,
            "X5": 0.736,
            "X6": 0.86,
            "X7": 0.655,
            "X8": -2.054,
            "X9": -0.822,
            "X10": 2.084,
            "X11": 0.951,
            "X12": -0.831,
            "X13": -0.431,
            "X14": -0.648,
            "X15": 0.482,
            "X16": 0.339,
            "X17": 0.527,
            "X18": 0.531,
            "X19": 0.353,
            "X20": 0.107,
            "X21": -0.931,
            "X22": 0.794,
            "X23": -0.591,
            "X24": 1.477,
            "X25": 0.001,
            "X26": 0.081,
            "X27": -0.079,
            "X28": -0.741,
            "X29": 2.413,
            "X30": 0.729,
            "X31": 0.054,
            "X32": -0.855,
            "X33": -0.654,
            "X34": 0.409,
            "X35": 0.364,
            "X36": -0.694,
            "X37": -1.18,
            "X38": 1.34,
            "X39": 0.809,
            "X40": -0.104,
            "X41": 2.379,
            "X42": 1.158,
            "X43": -0.505,
            "X44": 0.813,
            "X45": -0.283,
            "X46": 0.572,
            "X47": -0.142,
            "X48": -1.451,
            "X49": 0.93,
            "X50": 2.665,
            "X51": 1.229,
            "X52": 0.589,
            "X53": -0.62,
            "X54": 0.845,
            "X55": 0.563,
            "X56": -0.154,
            "X57": -0.784,
            "X58": -0.265,
            "X59": -0.483,
            "X60": 1.205,
            "X61": 0.485,
            "X62": -0.11,
            "X63": -0.407,
            "X64": -1.483,
            "X65": 0.291,
            "X66": 0.863,
            "X67": -0.18,
            "X68": 1.05,
            "X69": -0.648,
            "X70": 0.002,
            "X71": 0.907,
            "X72": 1.149,
            "X73": 0.014,
            "X74": 0.64,
            "X75": 0.238,
            "X76": -1.345,
            "X77": -1.086,
            "X78": -1.019,
            "X79": -0.119,
            "X80": -0.724,
            "X81": 0.279,
            "X82": -1.154,
            "X83": -0.825,
            "X84": -0.257,
            "X85": -1.283,
            "X86": -0.468,
            "X87": 0.875,
            "X88": -1.184,
            "X89": -0.776,
            "X90": 0.124,
            "X91": 0.744,
            "X92": -0.469,
            "X93": 0.617,
            "X94": 1.004,
            "X95": 0.502,
            "X96": -0.036,
            "X97": 1.909,
            "X98": 0.941,
            "X99": -0.232,
            "X100": -0.094,
            "X101": -0.071,
            "X102": 0.216,
            "X103": -0.607,
            "X104": 0.827,
            "X105": 1.441,
            "X106": -0.663,
            "X107": -0.514,
            "X108": -0.319,
            "X109": 0.67,
            "X110": 0.087,
            "X111": -0.761,
            "X112": -1.235,
            "X113": 1.372,
            "X114": 0.597,
            "X115": 0.121,
            "X116": -0.243,
            "X117": -0.141,
            "X118": -0.827,
            "X119": -0.79,
            "X120": 0.428,
            "X121": 0.115,
            "X122": 0.023,
            "X123": -0.154,
            "X124": 1.394,
            "X125": 1.506,
            "X126": -0.266,
            "X127": -0.415,
            "X128": 0.881,
            "X129": -0.707,
            "X130": -0.055,
            "X131": 2.093,
            "X132": -0.02,
            "X133": -0.77,
            "X134": 1.51,
            "X135": 0.36,
            "X136": 0.609,
            "X137": 1.007,
            "X138": -0.117,
            "X139": 1.56,
            "X140": -0.376,
            "X141": 0.39,
            "X142": -0.399,
            "X143": -0.72,
            "X144": -1.929,
            "X145": -0.836,
            "X146": 0.349,
            "X147": -0.039,
            "X148": -0.154,
            "X149": 0.029,
            "X150": 1.555,
            "X151": 0.273,
            "X152": -0.457,
            "X153": 0.422,
            "X154": -1.745,
            "X155": 1.626,
            "X156": -0.424,
            "X157": 0.913,
            "X158": -0.707,
            "X159": 0.023,
            "X160": 0.329,
            "X161": 0.284,
            "X162": 0.874,
            "X163": 0.276,
            "X164": 2.08,
            "X165": 1.747,
            "X166": -0.701,
            "X167": 0.755,
            "X168": 1.021,
            "X169": -0.274,
            "X170": -1.215,
            "X171": -2.269,
            "X172": -0.896,
            "X173": 0.777,
            "X174": 1.64,
            "X175": -1.496,
            "X176": 0.563,
            "X177": 1.035,
            "X178": -1.024,
            "X179": -1.349,
            "X180": -0.117,
            "X181": -1.601,
            "X182": 0.216,
            "X183": 0.888,
            "X184": -0.619,
            "X185": 0.597,
            "X186": -0.957,
            "X187": -1.403,
            "X188": -0.023,
            "X189": 0.444,
            "X190": -1.803,
            "X191": 0.489,
            "X192": -2.153,
            "X193": 0.4,
            "X194": -0.522,
            "X195": -0.474,
            "X196": 0.38,
            "X197": -1.039,
            "X198": -0.042,
            "X199": -0.877,
            "X200": 0.228,
            "X201": -0.252,
            "X202": -0.698,
            "X203": 0.464,
            "X204": -1.086,
            "X205": 0.2,
            "X206": 0.394,
            "X207": -0.041,
            "X208": -0.551,
            "X209": 1.048,
            "X210": -2.437,
            "X211": 0.097,
            "X212": 0.182,
            "X213": -0.404,
            "X214": 0.178,
            "X215": 0.138,
            "X216": 0.284,
            "X217": 0.064,
            "X218": 0.436,
            "X219": -1.609,
            "X220": -0.559,
            "X221": 0.456,
            "X222": 0.977,
            "X223": -0.34,
            "X224": 1.328,
            "X225": 1.278,
            "X226": -0.757,
            "X227": -0.252,
            "X228": 0.682,
            "X229": -1.495,
            "X230": -2.187,
            "X231": 0.681,
            "X232": -1.246,
            "X233": 0.414,
            "X234": -1.132,
            "X235": -0.609,
            "X236": -0.651,
            "X237": 1.365,
            "X238": 0.078,
            "X239": 2.81,
            "X240": 0.372,
            "X241": -0.283,
            "X242": -3.765,
            "X243": 0.645,
            "X244": 1.033,
            "X245": -0.409,
            "X246": -1.276,
            "X247": -2.175,
            "X248": 0.281,
            "X249": -1.078,
            "X250": 0.475,
            "X251": 1.413,
            "X252": 0.029,
            "X253": 0.672,
            "X254": -1.998,
            "X255": 2.193,
            "X256": 0.052,
            "X257": -0.947,
            "X258": -1.941,
            "X259": 0.75,
            "X260": 1.641,
            "X261": 1.002,
            "X262": -0.368,
            "X263": -1.424,
            "X264": 0.11,
            "X265": -0.229,
            "X266": 0.673,
            "X267": -0.439,
            "X268": -1.033,
            "X269": 1.732,
            "X270": 0.595,
            "X271": -0.512,
            "X272": 0.202,
            "X273": -1.249,
            "X274": -0.173,
            "X275": 0.579,
            "X276": 2.107,
            "X277": 0.289,
            "X278": 0.243,
            "X279": -0.694,
            "X280": -1.398,
            "X281": -0.514,
            "X282": 0.17,
            "X283": 0.316,
            "X284": -0.951,
            "X285": -1.762,
            "X286": 0.582,
            "X287": 0.469,
            "X288": -0.278,
            "X289": -0.633,
            "X290": -0.631,
            "X291": -2.533,
            "X292": -0.858,
            "X293": 0.721,
            "X294": -0.962,
            "X295": -3.013,
            "X296": 0.009,
            "X297": 1.739,
            "X298": -0.254,
            "X299": -0.529
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8216,
            ".pred_1": 0.1784,
            "id": 224,
            "X0": 0.717,
            "X1": 1.25,
            "X2": -0.08,
            "X3": -1.397,
            "X4": 0.378,
            "X5": 2.076,
            "X6": -1.145,
            "X7": -1.397,
            "X8": -0.66,
            "X9": -0.765,
            "X10": 0.885,
            "X11": -0.431,
            "X12": -1.259,
            "X13": 0.431,
            "X14": -0.303,
            "X15": 0.593,
            "X16": -0.923,
            "X17": -0.578,
            "X18": -2.026,
            "X19": 0.725,
            "X20": -0.782,
            "X21": -0.459,
            "X22": -0.303,
            "X23": 0.662,
            "X24": 2.164,
            "X25": 0.779,
            "X26": 0.027,
            "X27": 0.326,
            "X28": 0.248,
            "X29": 0.136,
            "X30": 0.51,
            "X31": -0.14,
            "X32": 0.125,
            "X33": 0.682,
            "X34": -1.995,
            "X35": -1.263,
            "X36": 1.834,
            "X37": -0.664,
            "X38": 0.528,
            "X39": 0.582,
            "X40": -0.171,
            "X41": -0.608,
            "X42": -0.597,
            "X43": -0.158,
            "X44": -0.118,
            "X45": -1.25,
            "X46": -0.01,
            "X47": 0.885,
            "X48": 0.415,
            "X49": -1.834,
            "X50": 0.448,
            "X51": -0.202,
            "X52": 0.646,
            "X53": 1.095,
            "X54": 0.147,
            "X55": 0.685,
            "X56": -0.785,
            "X57": 1.856,
            "X58": -1.65,
            "X59": 0.283,
            "X60": -1.015,
            "X61": 2.173,
            "X62": -0.945,
            "X63": 1.253,
            "X64": 0.52,
            "X65": -0.79,
            "X66": -0.652,
            "X67": -0.923,
            "X68": -0.062,
            "X69": -0.413,
            "X70": 0.578,
            "X71": -0.583,
            "X72": 0.418,
            "X73": 1.675,
            "X74": 0.017,
            "X75": 1.025,
            "X76": 0.845,
            "X77": 1.635,
            "X78": 1.128,
            "X79": 1.44,
            "X80": 0.157,
            "X81": -1.555,
            "X82": 0.508,
            "X83": 0.86,
            "X84": -1.3,
            "X85": -0.269,
            "X86": 1.215,
            "X87": 1.39,
            "X88": 0.234,
            "X89": -1.779,
            "X90": 2.629,
            "X91": 0.24,
            "X92": -0.72,
            "X93": -1.879,
            "X94": -0.207,
            "X95": 0.194,
            "X96": -0.126,
            "X97": -0.435,
            "X98": 1.436,
            "X99": -0.508,
            "X100": -0.054,
            "X101": 1.244,
            "X102": 1.275,
            "X103": -0.28,
            "X104": 2.474,
            "X105": 1.169,
            "X106": -0.433,
            "X107": -0.601,
            "X108": -2.074,
            "X109": -1.322,
            "X110": -0.146,
            "X111": -0.272,
            "X112": -0.646,
            "X113": -0.421,
            "X114": 1.939,
            "X115": 0.151,
            "X116": -2.041,
            "X117": -0.159,
            "X118": 0.618,
            "X119": -0.641,
            "X120": -0.381,
            "X121": -0.011,
            "X122": -1.252,
            "X123": 1.864,
            "X124": -0.088,
            "X125": 2.854,
            "X126": 1.615,
            "X127": -1.066,
            "X128": 1.693,
            "X129": 1.573,
            "X130": -0.347,
            "X131": -0.147,
            "X132": -0.32,
            "X133": -0.65,
            "X134": 1.3,
            "X135": -1.938,
            "X136": 0.644,
            "X137": 1.107,
            "X138": 0.397,
            "X139": -0.113,
            "X140": 0.535,
            "X141": -0.942,
            "X142": 0.538,
            "X143": -1.089,
            "X144": -1.305,
            "X145": -0.576,
            "X146": 0.351,
            "X147": -1.622,
            "X148": 1.334,
            "X149": -0.504,
            "X150": 0.125,
            "X151": -2.188,
            "X152": 0.374,
            "X153": 0.118,
            "X154": 0.109,
            "X155": 0.081,
            "X156": 1.891,
            "X157": 1.038,
            "X158": 0.259,
            "X159": 0.256,
            "X160": -1.431,
            "X161": -1.697,
            "X162": 0.192,
            "X163": 0.059,
            "X164": 0.327,
            "X165": 1.067,
            "X166": 0.944,
            "X167": -1.412,
            "X168": 0.798,
            "X169": -1.35,
            "X170": 2.164,
            "X171": -1.14,
            "X172": -1.054,
            "X173": 0.831,
            "X174": -0.869,
            "X175": -2.869,
            "X176": -0.545,
            "X177": -0.884,
            "X178": 0.29,
            "X179": -0.31,
            "X180": -0.821,
            "X181": 0.749,
            "X182": 1.73,
            "X183": 0.874,
            "X184": 0.375,
            "X185": 1.031,
            "X186": -0.124,
            "X187": -0.336,
            "X188": -0.409,
            "X189": 1.787,
            "X190": -1.216,
            "X191": 0.096,
            "X192": -0.128,
            "X193": -0.028,
            "X194": -0.207,
            "X195": -0.387,
            "X196": -0.809,
            "X197": 1.682,
            "X198": -1.291,
            "X199": -1.167,
            "X200": -1.498,
            "X201": 2.018,
            "X202": -1.148,
            "X203": -0.498,
            "X204": 1.928,
            "X205": 1.514,
            "X206": 1.199,
            "X207": 0.792,
            "X208": -0.638,
            "X209": -0.668,
            "X210": 1.163,
            "X211": -0.031,
            "X212": -0.62,
            "X213": -0.483,
            "X214": 1.417,
            "X215": 2.037,
            "X216": -2.109,
            "X217": -0.058,
            "X218": -0.457,
            "X219": -0.725,
            "X220": 0.496,
            "X221": -2.346,
            "X222": -0.734,
            "X223": -0.156,
            "X224": 0.705,
            "X225": -0.388,
            "X226": 1.513,
            "X227": -0.944,
            "X228": 0.94,
            "X229": -0.178,
            "X230": -0.052,
            "X231": 0.125,
            "X232": -0.652,
            "X233": 0.037,
            "X234": 0.123,
            "X235": -0.014,
            "X236": -2.119,
            "X237": 0.024,
            "X238": -0.761,
            "X239": -0.539,
            "X240": 0.475,
            "X241": -1.049,
            "X242": -0.781,
            "X243": -1.861,
            "X244": -0.96,
            "X245": 0.31,
            "X246": -0.355,
            "X247": 1.257,
            "X248": 1.536,
            "X249": 0.112,
            "X250": -0.168,
            "X251": 0.865,
            "X252": -0.015,
            "X253": 0.025,
            "X254": -1.041,
            "X255": -0.546,
            "X256": -1.607,
            "X257": -0.797,
            "X258": 0.053,
            "X259": -0.668,
            "X260": -0.272,
            "X261": 1.026,
            "X262": -0.198,
            "X263": -1.413,
            "X264": 1.001,
            "X265": -0.846,
            "X266": -0.193,
            "X267": -0.105,
            "X268": -1.139,
            "X269": -0.514,
            "X270": 0.604,
            "X271": 0.678,
            "X272": -0.36,
            "X273": 0.456,
            "X274": 0.547,
            "X275": -0.204,
            "X276": 1.171,
            "X277": -1.899,
            "X278": 0.06,
            "X279": -1.287,
            "X280": 1.067,
            "X281": -0.186,
            "X282": 1.508,
            "X283": 1.235,
            "X284": 0.694,
            "X285": -0.295,
            "X286": -0.588,
            "X287": 0.284,
            "X288": -2.451,
            "X289": 0.14,
            "X290": -0.208,
            "X291": 0.596,
            "X292": 0.856,
            "X293": -0.555,
            "X294": 1.477,
            "X295": -0.368,
            "X296": 0.626,
            "X297": 1.229,
            "X298": -0.078,
            "X299": 0.504
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.5526,
            ".pred_1": 0.4474,
            "id": 194,
            "X0": 0.459,
            "X1": -0.407,
            "X2": 0.486,
            "X3": 0.082,
            "X4": 0.984,
            "X5": -0.1,
            "X6": -0.284,
            "X7": -0.837,
            "X8": -0.184,
            "X9": 0.469,
            "X10": -1.869,
            "X11": 0.118,
            "X12": -0.235,
            "X13": -1.534,
            "X14": -0.521,
            "X15": -1.327,
            "X16": -0.343,
            "X17": -0.106,
            "X18": 1.332,
            "X19": 1.26,
            "X20": 0.369,
            "X21": 0.919,
            "X22": -1.494,
            "X23": 0.564,
            "X24": 0.22,
            "X25": -0.948,
            "X26": 0.013,
            "X27": 0.213,
            "X28": -0.062,
            "X29": 1.116,
            "X30": 1.837,
            "X31": -1.628,
            "X32": -0.553,
            "X33": 0.524,
            "X34": 0.289,
            "X35": -0.065,
            "X36": 0.56,
            "X37": -0.721,
            "X38": 1.028,
            "X39": -0.458,
            "X40": -0.696,
            "X41": -0.347,
            "X42": -1.333,
            "X43": 0.44,
            "X44": 0.391,
            "X45": -0.442,
            "X46": 0.951,
            "X47": -0.152,
            "X48": -0.334,
            "X49": -0.631,
            "X50": 0.63,
            "X51": 0.7,
            "X52": -1.507,
            "X53": 0.68,
            "X54": -2.308,
            "X55": -1.608,
            "X56": -0.667,
            "X57": -1.441,
            "X58": 0.506,
            "X59": -1.549,
            "X60": 1.107,
            "X61": 0.636,
            "X62": -0.054,
            "X63": -0.992,
            "X64": -2.255,
            "X65": 2.907,
            "X66": -0.544,
            "X67": 0.173,
            "X68": -1.977,
            "X69": -1.082,
            "X70": -0.035,
            "X71": 1.562,
            "X72": -2.382,
            "X73": -1.141,
            "X74": -0.746,
            "X75": 0.856,
            "X76": -0.323,
            "X77": 0.139,
            "X78": 0.243,
            "X79": 0.285,
            "X80": 0.137,
            "X81": 0.727,
            "X82": -0.942,
            "X83": 0.955,
            "X84": 1.145,
            "X85": 0.419,
            "X86": -2.486,
            "X87": 0.23,
            "X88": -0.447,
            "X89": -1.761,
            "X90": -0.648,
            "X91": 1.596,
            "X92": -0.01,
            "X93": -0.784,
            "X94": 1.254,
            "X95": 0.252,
            "X96": -0.92,
            "X97": -1.967,
            "X98": 0.35,
            "X99": -1.953,
            "X100": -0.062,
            "X101": -1.586,
            "X102": -0.811,
            "X103": -0.446,
            "X104": -0.073,
            "X105": -0.197,
            "X106": -0.754,
            "X107": 3.084,
            "X108": -1.705,
            "X109": 1.235,
            "X110": -0.441,
            "X111": -0.377,
            "X112": 2.369,
            "X113": 1.064,
            "X114": -0.654,
            "X115": -0.265,
            "X116": 0.421,
            "X117": -2.592,
            "X118": -0.894,
            "X119": 0.962,
            "X120": -1.65,
            "X121": -0.042,
            "X122": 1.444,
            "X123": -1.844,
            "X124": 0.74,
            "X125": 0.071,
            "X126": -1.571,
            "X127": 0.243,
            "X128": -0.016,
            "X129": 0.862,
            "X130": 0.829,
            "X131": -0.074,
            "X132": -2.436,
            "X133": 0.021,
            "X134": 1.589,
            "X135": -1.79,
            "X136": -0.502,
            "X137": -3.098,
            "X138": 0.358,
            "X139": 0.668,
            "X140": 1.636,
            "X141": 0.3,
            "X142": 1.972,
            "X143": 1.509,
            "X144": 0.638,
            "X145": 0.558,
            "X146": -0.193,
            "X147": 0.426,
            "X148": -1.293,
            "X149": -0.491,
            "X150": -1.677,
            "X151": 0.475,
            "X152": 0.201,
            "X153": 0.001,
            "X154": -1.158,
            "X155": 0.515,
            "X156": -2.368,
            "X157": -0.901,
            "X158": -0.622,
            "X159": 0.964,
            "X160": 2.016,
            "X161": -0.542,
            "X162": -2.244,
            "X163": -0.246,
            "X164": -0.673,
            "X165": -0.738,
            "X166": -0.194,
            "X167": 0.419,
            "X168": 0.377,
            "X169": 1.64,
            "X170": -0.373,
            "X171": -0.309,
            "X172": 1.13,
            "X173": -0.134,
            "X174": 0.847,
            "X175": -1.054,
            "X176": 0.352,
            "X177": 2.318,
            "X178": 0.079,
            "X179": -1.412,
            "X180": 0.842,
            "X181": 0.662,
            "X182": 1.045,
            "X183": 0.273,
            "X184": 0.946,
            "X185": 0.191,
            "X186": -0.075,
            "X187": 0.743,
            "X188": -1.145,
            "X189": 1.866,
            "X190": -1.132,
            "X191": 0.36,
            "X192": 0.314,
            "X193": -1.901,
            "X194": 1.819,
            "X195": -2.005,
            "X196": 0.435,
            "X197": -0.755,
            "X198": -0.14,
            "X199": -1.101,
            "X200": 0.253,
            "X201": 0.381,
            "X202": -0.118,
            "X203": -2.42,
            "X204": -0.845,
            "X205": 0.086,
            "X206": -1.203,
            "X207": 0.111,
            "X208": 0.227,
            "X209": 0.793,
            "X210": 0.427,
            "X211": 1.4,
            "X212": 1.727,
            "X213": -1.085,
            "X214": 1.02,
            "X215": 0.113,
            "X216": -0.304,
            "X217": 0.996,
            "X218": -0.087,
            "X219": -2.71,
            "X220": 0.622,
            "X221": -0.59,
            "X222": 0.041,
            "X223": 0.153,
            "X224": 1.497,
            "X225": 0.858,
            "X226": 0.485,
            "X227": -0.56,
            "X228": -0.476,
            "X229": -0.441,
            "X230": 0.071,
            "X231": -0.669,
            "X232": -1.846,
            "X233": -3.066,
            "X234": 0.93,
            "X235": -1.206,
            "X236": 0.932,
            "X237": 0.562,
            "X238": -0.703,
            "X239": 0.8,
            "X240": 0.781,
            "X241": -0.625,
            "X242": 0.496,
            "X243": -0.544,
            "X244": -0.151,
            "X245": -1.285,
            "X246": -0.367,
            "X247": -0.271,
            "X248": 0.866,
            "X249": -0.598,
            "X250": 0.695,
            "X251": -0.634,
            "X252": 0.646,
            "X253": -1.379,
            "X254": 0.64,
            "X255": -0.427,
            "X256": 0.985,
            "X257": -0.275,
            "X258": -0.7,
            "X259": -0.597,
            "X260": -0.796,
            "X261": -0.652,
            "X262": 0.772,
            "X263": -0.362,
            "X264": -1.29,
            "X265": -0.071,
            "X266": 0.239,
            "X267": -1.255,
            "X268": -2.297,
            "X269": -0.339,
            "X270": -0.505,
            "X271": 0.176,
            "X272": 2.865,
            "X273": -0.773,
            "X274": 1.817,
            "X275": 0.774,
            "X276": 0.034,
            "X277": -0.964,
            "X278": 0.902,
            "X279": -0.204,
            "X280": -0.204,
            "X281": 0.692,
            "X282": 0.084,
            "X283": 0.282,
            "X284": -0.948,
            "X285": -0.664,
            "X286": 0.95,
            "X287": 0.345,
            "X288": -0.91,
            "X289": -0.843,
            "X290": -0.563,
            "X291": -1.208,
            "X292": -1.781,
            "X293": 0.153,
            "X294": -0.695,
            "X295": -0.9,
            "X296": 0.252,
            "X297": -0.3,
            "X298": 1.011,
            "X299": 1.727
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7587,
            ".pred_1": 0.2413,
            "id": 15,
            "X0": -1.925,
            "X1": 0.327,
            "X2": 0.273,
            "X3": 0.453,
            "X4": 0.543,
            "X5": -0.829,
            "X6": -0.432,
            "X7": 0.661,
            "X8": 0.12,
            "X9": -0.541,
            "X10": 0.276,
            "X11": -0.421,
            "X12": -0.168,
            "X13": 0.798,
            "X14": 1.229,
            "X15": 2.202,
            "X16": 1.212,
            "X17": 0.262,
            "X18": -0.661,
            "X19": -1.717,
            "X20": 0.109,
            "X21": 0.284,
            "X22": 0.654,
            "X23": 1.836,
            "X24": 0.477,
            "X25": -2.058,
            "X26": 0.744,
            "X27": -0.534,
            "X28": 1.524,
            "X29": 0.093,
            "X30": -0.76,
            "X31": 1.001,
            "X32": 0.612,
            "X33": 0.662,
            "X34": -1.162,
            "X35": 1.272,
            "X36": -0.262,
            "X37": -0.373,
            "X38": 0.614,
            "X39": -0.385,
            "X40": -0.168,
            "X41": -0.691,
            "X42": 0.268,
            "X43": -1.101,
            "X44": 0.114,
            "X45": 0.838,
            "X46": 0.064,
            "X47": 0.115,
            "X48": -0.578,
            "X49": -0.392,
            "X50": 0.132,
            "X51": -0.317,
            "X52": -0.709,
            "X53": -0.361,
            "X54": 1.005,
            "X55": 1.548,
            "X56": 1.321,
            "X57": -0.512,
            "X58": -1.282,
            "X59": -0.889,
            "X60": 0.303,
            "X61": 0.148,
            "X62": 0.519,
            "X63": 0.877,
            "X64": -0.621,
            "X65": 0.499,
            "X66": 0.81,
            "X67": 0.903,
            "X68": 1.006,
            "X69": -0.627,
            "X70": 0.261,
            "X71": -2.159,
            "X72": 2.004,
            "X73": -0.046,
            "X74": -0.949,
            "X75": 0.706,
            "X76": 0.825,
            "X77": -0.311,
            "X78": -0.973,
            "X79": 1.178,
            "X80": 1.865,
            "X81": -0.9,
            "X82": 0.104,
            "X83": -0.301,
            "X84": 0.572,
            "X85": -2.637,
            "X86": -1.622,
            "X87": -1.372,
            "X88": 1.094,
            "X89": 0.844,
            "X90": -0.787,
            "X91": -0.055,
            "X92": -1.141,
            "X93": -0.594,
            "X94": -0.668,
            "X95": 0.056,
            "X96": -0.794,
            "X97": 0.124,
            "X98": 1.696,
            "X99": 1.496,
            "X100": 1.462,
            "X101": 0.523,
            "X102": 2.774,
            "X103": -0.767,
            "X104": -0.893,
            "X105": 1.774,
            "X106": 0.304,
            "X107": -0.639,
            "X108": 0.613,
            "X109": 0.515,
            "X110": 0.271,
            "X111": -0.7,
            "X112": -0.667,
            "X113": -1.729,
            "X114": 0.546,
            "X115": -0.677,
            "X116": 0.108,
            "X117": -1.348,
            "X118": 1.842,
            "X119": -1.046,
            "X120": -0.416,
            "X121": -2.329,
            "X122": -1.548,
            "X123": 0.389,
            "X124": -0.386,
            "X125": -0.484,
            "X126": 0.427,
            "X127": 0.296,
            "X128": 2.16,
            "X129": -1.982,
            "X130": 0.048,
            "X131": -0.561,
            "X132": -1.138,
            "X133": -0.375,
            "X134": 1.159,
            "X135": 0.101,
            "X136": 0.048,
            "X137": 0.603,
            "X138": 1.224,
            "X139": 0.407,
            "X140": -1.013,
            "X141": 0.696,
            "X142": -1.04,
            "X143": -0.406,
            "X144": 0.512,
            "X145": 1.886,
            "X146": -1.105,
            "X147": -2.482,
            "X148": -0.052,
            "X149": 2.152,
            "X150": 1.853,
            "X151": 0.413,
            "X152": 0.04,
            "X153": 0.367,
            "X154": 1.471,
            "X155": -1.831,
            "X156": -2.404,
            "X157": -0.858,
            "X158": 0.004,
            "X159": 1.236,
            "X160": 0.056,
            "X161": 2.663,
            "X162": 0.546,
            "X163": -0.479,
            "X164": -1.007,
            "X165": -1.07,
            "X166": -0.773,
            "X167": 2.217,
            "X168": 0.878,
            "X169": -0.79,
            "X170": 0.656,
            "X171": 1.079,
            "X172": 0.622,
            "X173": 1.413,
            "X174": 1.101,
            "X175": 1.603,
            "X176": -1.424,
            "X177": -0.109,
            "X178": -2.211,
            "X179": -0.697,
            "X180": 1.541,
            "X181": -0.558,
            "X182": -0.462,
            "X183": 1.158,
            "X184": -0.561,
            "X185": -1.228,
            "X186": -0.889,
            "X187": -0.173,
            "X188": 1.444,
            "X189": -0.075,
            "X190": -0.38,
            "X191": 0.631,
            "X192": -0.815,
            "X193": -1.159,
            "X194": -0.223,
            "X195": 0.471,
            "X196": -0.278,
            "X197": 0.302,
            "X198": -1.603,
            "X199": 0.139,
            "X200": -0.553,
            "X201": -0.073,
            "X202": -1.521,
            "X203": 0.527,
            "X204": -0.363,
            "X205": -1.195,
            "X206": -0.726,
            "X207": -1.71,
            "X208": 1.69,
            "X209": -0.641,
            "X210": 0.417,
            "X211": -0.772,
            "X212": -1.136,
            "X213": -0.631,
            "X214": 0.782,
            "X215": 1.938,
            "X216": 0.185,
            "X217": 1.35,
            "X218": 0.305,
            "X219": -0.591,
            "X220": -1.404,
            "X221": -1.116,
            "X222": 0.285,
            "X223": 1.048,
            "X224": 0.283,
            "X225": 0.087,
            "X226": -1.008,
            "X227": -0.993,
            "X228": -1.119,
            "X229": -1.412,
            "X230": -0.966,
            "X231": 0.049,
            "X232": 0.918,
            "X233": 0.521,
            "X234": -0.814,
            "X235": -0.404,
            "X236": -2.407,
            "X237": 0.492,
            "X238": 1.816,
            "X239": -1.036,
            "X240": -0.069,
            "X241": -0.493,
            "X242": -0.63,
            "X243": 1.488,
            "X244": -0.051,
            "X245": -1.346,
            "X246": -0.697,
            "X247": -1.814,
            "X248": -1.088,
            "X249": 0.345,
            "X250": 1.239,
            "X251": 0.178,
            "X252": -0.32,
            "X253": 0.308,
            "X254": -1.345,
            "X255": -1.032,
            "X256": 0.873,
            "X257": -0.383,
            "X258": -1.408,
            "X259": -0.009,
            "X260": -1.94,
            "X261": -1.238,
            "X262": -2.014,
            "X263": -1.105,
            "X264": -0.325,
            "X265": -0.412,
            "X266": 0.26,
            "X267": -1.402,
            "X268": -0.763,
            "X269": 0.531,
            "X270": -1.824,
            "X271": 1.073,
            "X272": -0.223,
            "X273": -1.149,
            "X274": 0.476,
            "X275": -0.775,
            "X276": 0.456,
            "X277": -1.882,
            "X278": -0.044,
            "X279": -0.044,
            "X280": 0.628,
            "X281": 0.336,
            "X282": 0.729,
            "X283": -0.484,
            "X284": -0.147,
            "X285": 0.088,
            "X286": -0.068,
            "X287": 1.316,
            "X288": 0.148,
            "X289": 0.396,
            "X290": 0.055,
            "X291": -1.439,
            "X292": -1.363,
            "X293": -1.607,
            "X294": -0.475,
            "X295": -1.495,
            "X296": 0.017,
            "X297": 0.507,
            "X298": 0.183,
            "X299": -0.356
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8233,
            ".pred_1": 0.1767,
            "id": 177,
            "X0": 0.513,
            "X1": -0.022,
            "X2": 0.283,
            "X3": -0.577,
            "X4": 1.158,
            "X5": -0.596,
            "X6": -0.162,
            "X7": -0.28,
            "X8": -0.429,
            "X9": 2.354,
            "X10": -0.994,
            "X11": -0.491,
            "X12": 0.964,
            "X13": -2.135,
            "X14": 0.073,
            "X15": -0.829,
            "X16": -0.714,
            "X17": 0.004,
            "X18": -0.695,
            "X19": -0.905,
            "X20": -1.146,
            "X21": 0.318,
            "X22": 1.158,
            "X23": 2.607,
            "X24": 0.545,
            "X25": 0.085,
            "X26": -1.097,
            "X27": 1.039,
            "X28": -0.705,
            "X29": 0.361,
            "X30": 0.652,
            "X31": -1.449,
            "X32": -1.069,
            "X33": 0.861,
            "X34": 0.675,
            "X35": 1.467,
            "X36": 1.083,
            "X37": 1.251,
            "X38": 1.863,
            "X39": 0.322,
            "X40": 1.157,
            "X41": 0.086,
            "X42": -0.12,
            "X43": 0.528,
            "X44": -0.565,
            "X45": -1.94,
            "X46": -2.3,
            "X47": -0.753,
            "X48": 1.495,
            "X49": 0.637,
            "X50": -0.08,
            "X51": -0.232,
            "X52": 0.541,
            "X53": -0.05,
            "X54": 0.482,
            "X55": 0.529,
            "X56": -1.077,
            "X57": 0.445,
            "X58": -1.197,
            "X59": 1.155,
            "X60": -0.261,
            "X61": 0.498,
            "X62": 1.877,
            "X63": -0.309,
            "X64": -0.919,
            "X65": 1.343,
            "X66": 0.398,
            "X67": 1.594,
            "X68": 0.52,
            "X69": 1.223,
            "X70": -0.605,
            "X71": 0.897,
            "X72": -0.104,
            "X73": 0.586,
            "X74": 0.382,
            "X75": -0.565,
            "X76": 1.23,
            "X77": 1.56,
            "X78": -0.042,
            "X79": 0.625,
            "X80": 0.774,
            "X81": 0.888,
            "X82": -0.259,
            "X83": 0.475,
            "X84": -0.698,
            "X85": -1.601,
            "X86": 1.016,
            "X87": 1.329,
            "X88": 1.7,
            "X89": -0.731,
            "X90": 0.472,
            "X91": -0.2,
            "X92": -0.43,
            "X93": 0.87,
            "X94": -0.076,
            "X95": 0.353,
            "X96": 1.617,
            "X97": -1.206,
            "X98": 0.959,
            "X99": 0.132,
            "X100": -0.404,
            "X101": -0.271,
            "X102": 0.12,
            "X103": -0.766,
            "X104": -0.193,
            "X105": 0.237,
            "X106": 0.371,
            "X107": 1.474,
            "X108": 0.131,
            "X109": -0.654,
            "X110": 0.181,
            "X111": -3.03,
            "X112": -0.411,
            "X113": -0.272,
            "X114": -1.987,
            "X115": -0.012,
            "X116": 0.071,
            "X117": -0.058,
            "X118": 1.102,
            "X119": 0.168,
            "X120": 1.09,
            "X121": 0.705,
            "X122": -0.626,
            "X123": 0.692,
            "X124": 1.373,
            "X125": 1.401,
            "X126": 0.337,
            "X127": -0.573,
            "X128": -0.905,
            "X129": -0.383,
            "X130": 0.454,
            "X131": -2.35,
            "X132": -1.911,
            "X133": -0.499,
            "X134": -0.871,
            "X135": -0.15,
            "X136": -1.404,
            "X137": 1.095,
            "X138": -0.084,
            "X139": 1.016,
            "X140": 1.358,
            "X141": 1.117,
            "X142": 0.877,
            "X143": -0.206,
            "X144": -0.09,
            "X145": -1.41,
            "X146": 1.79,
            "X147": -0.474,
            "X148": 1.073,
            "X149": 1.194,
            "X150": 0.487,
            "X151": -2.822,
            "X152": -0.148,
            "X153": 0.013,
            "X154": -1.472,
            "X155": 1.332,
            "X156": -1.328,
            "X157": -0.726,
            "X158": -1.078,
            "X159": -0.095,
            "X160": -0.629,
            "X161": 0.929,
            "X162": -2.108,
            "X163": 0.488,
            "X164": -1.501,
            "X165": -0.78,
            "X166": -0.428,
            "X167": -0.952,
            "X168": -0.574,
            "X169": -1.291,
            "X170": -0.252,
            "X171": 1.379,
            "X172": -0.449,
            "X173": 0.059,
            "X174": -0.055,
            "X175": -1.13,
            "X176": 0.655,
            "X177": -0.678,
            "X178": -1.171,
            "X179": 0.855,
            "X180": -0.758,
            "X181": -0.242,
            "X182": -0.815,
            "X183": 0.034,
            "X184": -0.442,
            "X185": 0.983,
            "X186": -0.655,
            "X187": 1.585,
            "X188": -1.019,
            "X189": -0.201,
            "X190": 1.508,
            "X191": -0.75,
            "X192": 0.771,
            "X193": 0.55,
            "X194": -0.264,
            "X195": -0.533,
            "X196": 0.261,
            "X197": -0.119,
            "X198": -2.425,
            "X199": 0.456,
            "X200": 0.833,
            "X201": 0.745,
            "X202": 0.227,
            "X203": 0.315,
            "X204": -1.895,
            "X205": -0.066,
            "X206": -0.799,
            "X207": 1.91,
            "X208": -1.296,
            "X209": -1.084,
            "X210": -1.044,
            "X211": -1.854,
            "X212": -0.823,
            "X213": -0.282,
            "X214": 0.174,
            "X215": -1.422,
            "X216": 0.225,
            "X217": 0.488,
            "X218": -0.256,
            "X219": 0.158,
            "X220": 0.334,
            "X221": -0.916,
            "X222": -0.672,
            "X223": -0.046,
            "X224": 0.974,
            "X225": -0.765,
            "X226": 1.535,
            "X227": -1.872,
            "X228": -1.265,
            "X229": 1.183,
            "X230": -0.194,
            "X231": -1.581,
            "X232": -0.789,
            "X233": 0.982,
            "X234": 0.1,
            "X235": 0.041,
            "X236": -0.368,
            "X237": -0.563,
            "X238": 0.854,
            "X239": 0.717,
            "X240": -0.099,
            "X241": -2.267,
            "X242": -1.121,
            "X243": -0.005,
            "X244": -0.713,
            "X245": 0.677,
            "X246": -0.899,
            "X247": -0.386,
            "X248": 1.029,
            "X249": 0.578,
            "X250": 1.466,
            "X251": 0.56,
            "X252": 1.763,
            "X253": -0.117,
            "X254": 0.438,
            "X255": -1.719,
            "X256": 1.681,
            "X257": 0.375,
            "X258": 0.741,
            "X259": -0.458,
            "X260": 3.459,
            "X261": 0.104,
            "X262": -0.345,
            "X263": 0.343,
            "X264": 0.141,
            "X265": -1.443,
            "X266": 1.364,
            "X267": -1.42,
            "X268": 1.278,
            "X269": 0.173,
            "X270": 0.886,
            "X271": -0.737,
            "X272": 1.048,
            "X273": 0.981,
            "X274": 0.121,
            "X275": 0.621,
            "X276": -1.369,
            "X277": -1.409,
            "X278": -0.023,
            "X279": 1.092,
            "X280": -1.81,
            "X281": 0.723,
            "X282": 0.57,
            "X283": -0.385,
            "X284": -0.934,
            "X285": -0.504,
            "X286": -0.016,
            "X287": 0.297,
            "X288": -0.024,
            "X289": -0.567,
            "X290": -0.768,
            "X291": 1.436,
            "X292": 1.042,
            "X293": 0.01,
            "X294": -0.329,
            "X295": -1.513,
            "X296": 0.959,
            "X297": -0.149,
            "X298": 1.096,
            "X299": 0.003
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8397,
            ".pred_1": 0.1603,
            "id": 24,
            "X0": -0.001,
            "X1": 1.968,
            "X2": -0.017,
            "X3": -0.281,
            "X4": -1.052,
            "X5": -0.078,
            "X6": -0.566,
            "X7": 1.292,
            "X8": -0.977,
            "X9": 1.392,
            "X10": -1.549,
            "X11": -0.711,
            "X12": -0.608,
            "X13": -0.355,
            "X14": -0.234,
            "X15": -0.43,
            "X16": -0.31,
            "X17": 0.158,
            "X18": -1.411,
            "X19": 0.29,
            "X20": -0.735,
            "X21": -0.718,
            "X22": -0.736,
            "X23": -0.336,
            "X24": 1.103,
            "X25": -1.53,
            "X26": -1.164,
            "X27": 0.271,
            "X28": -0.551,
            "X29": -0.214,
            "X30": -0.136,
            "X31": 0.732,
            "X32": -1.638,
            "X33": 2.26,
            "X34": 0.17,
            "X35": 0.936,
            "X36": 1.697,
            "X37": 0.138,
            "X38": 0.345,
            "X39": -0.853,
            "X40": -0.045,
            "X41": 2.116,
            "X42": -0.744,
            "X43": -3.901,
            "X44": -0.968,
            "X45": -0.072,
            "X46": 0.582,
            "X47": 0.319,
            "X48": 1.859,
            "X49": 1.055,
            "X50": 0.006,
            "X51": 0.888,
            "X52": -0.109,
            "X53": 0.237,
            "X54": 0.438,
            "X55": -0.107,
            "X56": -1.228,
            "X57": 1.563,
            "X58": -0.082,
            "X59": 0.828,
            "X60": 0.524,
            "X61": 0.718,
            "X62": 0.547,
            "X63": 0.529,
            "X64": -0.252,
            "X65": -0.77,
            "X66": -1.23,
            "X67": -0.806,
            "X68": -1.99,
            "X69": -0.972,
            "X70": -0.494,
            "X71": -0.655,
            "X72": -0.869,
            "X73": -1.31,
            "X74": -0.22,
            "X75": -0.907,
            "X76": -0.057,
            "X77": -0.517,
            "X78": -0.866,
            "X79": -0.275,
            "X80": -1.809,
            "X81": 0.115,
            "X82": -0.523,
            "X83": -0.545,
            "X84": 0.679,
            "X85": 1.135,
            "X86": 1.178,
            "X87": -0.634,
            "X88": -0.319,
            "X89": -1.31,
            "X90": 1.028,
            "X91": -0.028,
            "X92": -1.957,
            "X93": 1.386,
            "X94": 1.385,
            "X95": -0.309,
            "X96": 1.243,
            "X97": 1.58,
            "X98": 1.782,
            "X99": 0.321,
            "X100": -1.357,
            "X101": -1.619,
            "X102": -1.391,
            "X103": -0.499,
            "X104": 1.636,
            "X105": 0.85,
            "X106": -0.249,
            "X107": -2.019,
            "X108": -0.714,
            "X109": -0.054,
            "X110": 1.775,
            "X111": -0.336,
            "X112": 0.419,
            "X113": -0.907,
            "X114": 0.043,
            "X115": 0.444,
            "X116": 1.011,
            "X117": -0.089,
            "X118": 0.142,
            "X119": 0.028,
            "X120": 1.277,
            "X121": 2.488,
            "X122": 1.317,
            "X123": 1.195,
            "X124": 0.369,
            "X125": -0.75,
            "X126": 0.248,
            "X127": -1.115,
            "X128": -0.667,
            "X129": 0.165,
            "X130": -0.2,
            "X131": 0.698,
            "X132": 0.244,
            "X133": 1.676,
            "X134": 1.188,
            "X135": 0.025,
            "X136": -0.392,
            "X137": -0.023,
            "X138": 0.296,
            "X139": -2.284,
            "X140": 0.878,
            "X141": -0.917,
            "X142": 0.478,
            "X143": -1.058,
            "X144": 0.121,
            "X145": 0.431,
            "X146": -1.453,
            "X147": 0.543,
            "X148": -0.44,
            "X149": 0.653,
            "X150": 0.671,
            "X151": -0.77,
            "X152": -0.714,
            "X153": 0.615,
            "X154": -0.885,
            "X155": 0.809,
            "X156": -0.837,
            "X157": -0.317,
            "X158": 1.281,
            "X159": -0.966,
            "X160": 0.095,
            "X161": 0.382,
            "X162": -0.253,
            "X163": 1.937,
            "X164": 0.388,
            "X165": 1.107,
            "X166": 0.423,
            "X167": -2.51,
            "X168": -0.083,
            "X169": -0.892,
            "X170": 0.43,
            "X171": -1.348,
            "X172": 0.116,
            "X173": -0.831,
            "X174": -2.852,
            "X175": 1.775,
            "X176": -0.636,
            "X177": -1.17,
            "X178": -0.339,
            "X179": -0.668,
            "X180": -0.049,
            "X181": -1.067,
            "X182": 0.178,
            "X183": -1.277,
            "X184": 0.116,
            "X185": -0.544,
            "X186": 0.752,
            "X187": 0.297,
            "X188": -0.171,
            "X189": 0.024,
            "X190": 1.836,
            "X191": 1.087,
            "X192": -1.66,
            "X193": 0.126,
            "X194": -1.48,
            "X195": -0.975,
            "X196": -0.413,
            "X197": -0.955,
            "X198": -0.986,
            "X199": -0.759,
            "X200": 1.968,
            "X201": -0.947,
            "X202": 0.251,
            "X203": -0.182,
            "X204": -1.383,
            "X205": 0.326,
            "X206": 0.316,
            "X207": -1.224,
            "X208": -1.763,
            "X209": -1.856,
            "X210": 0.664,
            "X211": 0.963,
            "X212": 1.307,
            "X213": -2.104,
            "X214": 1.029,
            "X215": 0.205,
            "X216": -0.987,
            "X217": 0.248,
            "X218": -0.09,
            "X219": 1.287,
            "X220": 0.545,
            "X221": -0.962,
            "X222": -1.188,
            "X223": 0.103,
            "X224": 0.048,
            "X225": -0.309,
            "X226": -0.631,
            "X227": 1.266,
            "X228": 0.55,
            "X229": -0.06,
            "X230": 0.166,
            "X231": -0.263,
            "X232": 0.199,
            "X233": 1.091,
            "X234": 0.232,
            "X235": -0.459,
            "X236": -1.025,
            "X237": -0.005,
            "X238": -0.673,
            "X239": 1.964,
            "X240": -0.572,
            "X241": 1.009,
            "X242": 1.259,
            "X243": 2.415,
            "X244": -0.306,
            "X245": -0.998,
            "X246": -0.598,
            "X247": -2.277,
            "X248": -0.503,
            "X249": 0.977,
            "X250": -0.211,
            "X251": 0.854,
            "X252": -0.329,
            "X253": -0.81,
            "X254": 1.137,
            "X255": 0.823,
            "X256": -1.32,
            "X257": -1.196,
            "X258": -1.606,
            "X259": 0.457,
            "X260": -0.595,
            "X261": 0.337,
            "X262": -1.54,
            "X263": -0.893,
            "X264": -0.911,
            "X265": -1.687,
            "X266": -1.726,
            "X267": -0.373,
            "X268": 0.52,
            "X269": 0.139,
            "X270": 0.115,
            "X271": -0.097,
            "X272": 1.183,
            "X273": 0.265,
            "X274": 1.924,
            "X275": -0.453,
            "X276": -0.636,
            "X277": -0.385,
            "X278": 0.239,
            "X279": -1.558,
            "X280": -1.1,
            "X281": 0.28,
            "X282": -0.222,
            "X283": -0.433,
            "X284": 0.881,
            "X285": 0.883,
            "X286": 0.571,
            "X287": -0.651,
            "X288": 0.222,
            "X289": -0.452,
            "X290": -0.943,
            "X291": -0.256,
            "X292": -0.365,
            "X293": -1.246,
            "X294": 0.287,
            "X295": -1.016,
            "X296": 0.219,
            "X297": -0.295,
            "X298": -1.413,
            "X299": 0.891
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7235,
            ".pred_1": 0.2765,
            "id": 247,
            "X0": -2.327,
            "X1": -1.834,
            "X2": -0.762,
            "X3": 0.66,
            "X4": -0.858,
            "X5": -2.764,
            "X6": -0.539,
            "X7": -0.065,
            "X8": 0.549,
            "X9": 1.474,
            "X10": 0.565,
            "X11": -1.128,
            "X12": 1.6,
            "X13": -0.291,
            "X14": 0.484,
            "X15": 1.559,
            "X16": -0.69,
            "X17": 0.267,
            "X18": -0.502,
            "X19": 0.189,
            "X20": 0.504,
            "X21": 1.799,
            "X22": 0.852,
            "X23": 1.018,
            "X24": -0.193,
            "X25": -0.981,
            "X26": 0.142,
            "X27": 1.037,
            "X28": -0.124,
            "X29": -0.219,
            "X30": -0.739,
            "X31": -0.238,
            "X32": -0.561,
            "X33": -0.79,
            "X34": -1.842,
            "X35": 0.274,
            "X36": -0.066,
            "X37": 0.399,
            "X38": -0.393,
            "X39": -0.042,
            "X40": -0.743,
            "X41": -1.949,
            "X42": -2.039,
            "X43": 0.375,
            "X44": -0.568,
            "X45": 0.04,
            "X46": 0.135,
            "X47": -1.239,
            "X48": -2.421,
            "X49": -0.342,
            "X50": -1.215,
            "X51": -2.047,
            "X52": 0.023,
            "X53": -1.444,
            "X54": -0.178,
            "X55": -0.677,
            "X56": 0.751,
            "X57": -0.699,
            "X58": -1.69,
            "X59": 0.594,
            "X60": -0.957,
            "X61": -1.502,
            "X62": 0.861,
            "X63": 1.221,
            "X64": -0.187,
            "X65": -0.29,
            "X66": -0.28,
            "X67": 0.386,
            "X68": -1.531,
            "X69": 2.778,
            "X70": 0.463,
            "X71": 1.564,
            "X72": 0.481,
            "X73": -0.173,
            "X74": -0.947,
            "X75": 1.338,
            "X76": 0.609,
            "X77": 0.234,
            "X78": -0.456,
            "X79": 1.792,
            "X80": -0.407,
            "X81": 0.538,
            "X82": -0.181,
            "X83": -0.509,
            "X84": -1.26,
            "X85": 1.046,
            "X86": 0.486,
            "X87": 1.263,
            "X88": 1.617,
            "X89": 2.045,
            "X90": -0.371,
            "X91": -0.111,
            "X92": 0.018,
            "X93": -0.694,
            "X94": -1.829,
            "X95": 0.639,
            "X96": -1.301,
            "X97": 0.216,
            "X98": -1.494,
            "X99": 0.231,
            "X100": -0.791,
            "X101": -2.352,
            "X102": -0.808,
            "X103": -0.412,
            "X104": -0.358,
            "X105": -0.951,
            "X106": 1.34,
            "X107": 0.159,
            "X108": -0.476,
            "X109": 0.179,
            "X110": 0.671,
            "X111": 0.161,
            "X112": 1.817,
            "X113": -0.794,
            "X114": 0.39,
            "X115": -0.832,
            "X116": -2.439,
            "X117": 0.012,
            "X118": -0.685,
            "X119": -1.278,
            "X120": 0.908,
            "X121": 0.895,
            "X122": 0.456,
            "X123": -0.934,
            "X124": -0.081,
            "X125": 2.256,
            "X126": 1.846,
            "X127": 0.053,
            "X128": -0.949,
            "X129": 0.915,
            "X130": 0.27,
            "X131": -1.194,
            "X132": 1.036,
            "X133": -0.347,
            "X134": 1.723,
            "X135": -0.825,
            "X136": 0.248,
            "X137": 1.416,
            "X138": 0.342,
            "X139": 0.348,
            "X140": 0.843,
            "X141": -1.24,
            "X142": -1.439,
            "X143": 0.072,
            "X144": 0.488,
            "X145": 0.082,
            "X146": 0.057,
            "X147": -0.419,
            "X148": -0.323,
            "X149": -0.428,
            "X150": -0.534,
            "X151": 1.105,
            "X152": -1.033,
            "X153": -0.353,
            "X154": -0.093,
            "X155": 0.18,
            "X156": 0.679,
            "X157": 0.867,
            "X158": -0.432,
            "X159": -2.635,
            "X160": 1.022,
            "X161": -0.083,
            "X162": 1.007,
            "X163": -0.856,
            "X164": -0.763,
            "X165": -0.691,
            "X166": -0.395,
            "X167": -0.645,
            "X168": -1.421,
            "X169": -1.61,
            "X170": -0.177,
            "X171": -0.146,
            "X172": -0.708,
            "X173": 0.373,
            "X174": 2.407,
            "X175": 1.458,
            "X176": 0.403,
            "X177": -1.308,
            "X178": -1.451,
            "X179": -0.301,
            "X180": -0.915,
            "X181": 0.075,
            "X182": 0.366,
            "X183": 1.359,
            "X184": 1.343,
            "X185": -0.91,
            "X186": 0.15,
            "X187": -0.839,
            "X188": 1.018,
            "X189": 1.732,
            "X190": 1.012,
            "X191": -0.023,
            "X192": 0.444,
            "X193": 0.82,
            "X194": 1.232,
            "X195": -0.438,
            "X196": 0.801,
            "X197": 1.578,
            "X198": 2.746,
            "X199": -0.434,
            "X200": 1.116,
            "X201": 0.245,
            "X202": 0.035,
            "X203": 0.615,
            "X204": 0.325,
            "X205": -0.243,
            "X206": 1.702,
            "X207": 0.398,
            "X208": -0.411,
            "X209": -2.786,
            "X210": -0.053,
            "X211": -0.359,
            "X212": -1.333,
            "X213": -0.224,
            "X214": 0.218,
            "X215": 1.731,
            "X216": 0.897,
            "X217": -0.213,
            "X218": -1.221,
            "X219": -1.368,
            "X220": 0.424,
            "X221": -0.188,
            "X222": 0.926,
            "X223": 0.214,
            "X224": 0.422,
            "X225": -0.576,
            "X226": 0.92,
            "X227": -0.826,
            "X228": -0.031,
            "X229": 1.12,
            "X230": 0.945,
            "X231": 1.249,
            "X232": 0.019,
            "X233": -0.304,
            "X234": 0.03,
            "X235": -1.512,
            "X236": 1.276,
            "X237": 1.911,
            "X238": -0.047,
            "X239": 0.668,
            "X240": -0.47,
            "X241": -0.771,
            "X242": -0.715,
            "X243": 0.336,
            "X244": -0.491,
            "X245": -0.271,
            "X246": -0.498,
            "X247": -0.576,
            "X248": -1.106,
            "X249": -0.531,
            "X250": 0.391,
            "X251": 0.069,
            "X252": -0.783,
            "X253": 0.288,
            "X254": 1.4,
            "X255": 0.787,
            "X256": 1.341,
            "X257": -1.325,
            "X258": -1.641,
            "X259": -0.037,
            "X260": 1.022,
            "X261": 0.656,
            "X262": -1.247,
            "X263": -1.249,
            "X264": -0.729,
            "X265": 1.951,
            "X266": -1.258,
            "X267": -0.692,
            "X268": 0.083,
            "X269": 0.319,
            "X270": 0.953,
            "X271": -0.809,
            "X272": -0.303,
            "X273": 0.297,
            "X274": 0.05,
            "X275": 0.792,
            "X276": -0.75,
            "X277": -0.456,
            "X278": 0.549,
            "X279": -0.492,
            "X280": -0.288,
            "X281": -0.138,
            "X282": -0.649,
            "X283": 0.199,
            "X284": 0.657,
            "X285": 1.015,
            "X286": -1.07,
            "X287": -0.235,
            "X288": 0.168,
            "X289": -0.938,
            "X290": -1.237,
            "X291": -0.62,
            "X292": 0.67,
            "X293": -2.01,
            "X294": 0.438,
            "X295": 1.972,
            "X296": -0.379,
            "X297": 0.676,
            "X298": -1.22,
            "X299": -0.855
          },
          {
            ".pred_class": 1,
            ".pred_0": 0.476,
            ".pred_1": 0.524,
            "id": 19,
            "X0": -1.056,
            "X1": 0.586,
            "X2": 0.995,
            "X3": -0.873,
            "X4": -0.021,
            "X5": 2.497,
            "X6": -0.526,
            "X7": 0.696,
            "X8": 0.417,
            "X9": -0.492,
            "X10": -1.324,
            "X11": 0.331,
            "X12": -0.014,
            "X13": 0.536,
            "X14": 0.19,
            "X15": 3.542,
            "X16": 0.39,
            "X17": -0.471,
            "X18": -0.434,
            "X19": 0.116,
            "X20": -0.422,
            "X21": -1.63,
            "X22": 0.82,
            "X23": -2.744,
            "X24": 0.203,
            "X25": 0.57,
            "X26": 0.349,
            "X27": 1.731,
            "X28": 1.567,
            "X29": 0.432,
            "X30": 0.072,
            "X31": -2.027,
            "X32": 0.739,
            "X33": -0.596,
            "X34": 1.012,
            "X35": -1.292,
            "X36": 0.905,
            "X37": 1.067,
            "X38": 0.723,
            "X39": 1.487,
            "X40": -0.016,
            "X41": 1.099,
            "X42": 0.971,
            "X43": -0.427,
            "X44": -0.525,
            "X45": 0.234,
            "X46": 0.619,
            "X47": -0.689,
            "X48": 0.878,
            "X49": -0.876,
            "X50": -2.441,
            "X51": 0.131,
            "X52": 1.291,
            "X53": 0.763,
            "X54": 0.954,
            "X55": -0.253,
            "X56": -0.645,
            "X57": 1.098,
            "X58": -1.146,
            "X59": 0.897,
            "X60": 1.433,
            "X61": 1.624,
            "X62": 0.76,
            "X63": 0.664,
            "X64": 0.359,
            "X65": -1.43,
            "X66": -0.023,
            "X67": -0.728,
            "X68": 0.527,
            "X69": 0.418,
            "X70": -0.818,
            "X71": 0.234,
            "X72": 1.282,
            "X73": -0.801,
            "X74": -0.744,
            "X75": -1.467,
            "X76": 0.611,
            "X77": -1.316,
            "X78": 0.195,
            "X79": 1.991,
            "X80": -0.079,
            "X81": -0.076,
            "X82": 0.886,
            "X83": -0.781,
            "X84": 0.033,
            "X85": -0.431,
            "X86": -1.192,
            "X87": -0.013,
            "X88": 0.282,
            "X89": -0.062,
            "X90": 0.668,
            "X91": -0.66,
            "X92": 1.205,
            "X93": -0.53,
            "X94": 0.22,
            "X95": -0.37,
            "X96": -0.247,
            "X97": 0.089,
            "X98": 0.182,
            "X99": -3.263,
            "X100": -0.823,
            "X101": -1.053,
            "X102": 1.688,
            "X103": 0.992,
            "X104": 0.911,
            "X105": -0.948,
            "X106": -2.812,
            "X107": 1.358,
            "X108": -0.529,
            "X109": 1.072,
            "X110": -0.39,
            "X111": 0.369,
            "X112": 0.392,
            "X113": -0.801,
            "X114": 1.552,
            "X115": 0.543,
            "X116": 1.498,
            "X117": -0.182,
            "X118": -0.543,
            "X119": -1.566,
            "X120": 0.575,
            "X121": 0.421,
            "X122": 0.617,
            "X123": -0.96,
            "X124": -1.362,
            "X125": -0.909,
            "X126": -0.913,
            "X127": 1.762,
            "X128": -0.221,
            "X129": -0.516,
            "X130": 0.507,
            "X131": -0.963,
            "X132": -0.795,
            "X133": -0.435,
            "X134": -0.231,
            "X135": -1.853,
            "X136": -0.585,
            "X137": 1.447,
            "X138": -0.779,
            "X139": -0.631,
            "X140": -0.565,
            "X141": -0.067,
            "X142": -0.276,
            "X143": -0.231,
            "X144": -1.567,
            "X145": -0.621,
            "X146": -0.148,
            "X147": 1.922,
            "X148": -1.086,
            "X149": 1.006,
            "X150": 0.316,
            "X151": 1.134,
            "X152": -2.628,
            "X153": -0.17,
            "X154": 2.192,
            "X155": 0.072,
            "X156": -1.194,
            "X157": 1.519,
            "X158": -0.724,
            "X159": 0.154,
            "X160": 0.11,
            "X161": -0.832,
            "X162": 0.295,
            "X163": 0.259,
            "X164": -0.123,
            "X165": -0.66,
            "X166": -0.635,
            "X167": -1.29,
            "X168": -0.065,
            "X169": 0.507,
            "X170": -1.72,
            "X171": -1.373,
            "X172": -1.04,
            "X173": -0.166,
            "X174": -1.813,
            "X175": -0.283,
            "X176": 1.01,
            "X177": -0.247,
            "X178": -0.577,
            "X179": -0.921,
            "X180": -2.212,
            "X181": 0.68,
            "X182": -0.322,
            "X183": 0.394,
            "X184": 0.448,
            "X185": 0.715,
            "X186": 0.026,
            "X187": -0.54,
            "X188": -0.663,
            "X189": 0.221,
            "X190": 0.026,
            "X191": -1.013,
            "X192": 0.172,
            "X193": 0.355,
            "X194": -0.062,
            "X195": 1.326,
            "X196": 1.636,
            "X197": 0.055,
            "X198": -0.126,
            "X199": 0.055,
            "X200": 0.359,
            "X201": 1.633,
            "X202": 0.475,
            "X203": -0.359,
            "X204": -1.58,
            "X205": -0.375,
            "X206": -0.331,
            "X207": 1.288,
            "X208": 0.982,
            "X209": -0.618,
            "X210": 1.048,
            "X211": -0.27,
            "X212": 1.475,
            "X213": -0.474,
            "X214": 1.889,
            "X215": -1.936,
            "X216": -0.226,
            "X217": 1.929,
            "X218": 1.366,
            "X219": 1.123,
            "X220": -1.171,
            "X221": 1.148,
            "X222": -0.19,
            "X223": 1.491,
            "X224": 0.255,
            "X225": 0.726,
            "X226": 0.184,
            "X227": 0.066,
            "X228": 1.826,
            "X229": -0.648,
            "X230": 0.375,
            "X231": -2.55,
            "X232": -0.792,
            "X233": -0.709,
            "X234": -1.297,
            "X235": -1.591,
            "X236": 0.206,
            "X237": 1.705,
            "X238": -1.602,
            "X239": -0.824,
            "X240": 1.202,
            "X241": 2.06,
            "X242": 1.081,
            "X243": 0.127,
            "X244": -0.904,
            "X245": 2.461,
            "X246": 1.694,
            "X247": 0.337,
            "X248": 0.008,
            "X249": 1.198,
            "X250": -1.375,
            "X251": -2.201,
            "X252": -1.48,
            "X253": -0.18,
            "X254": 0.993,
            "X255": 0.233,
            "X256": -0.982,
            "X257": 0.525,
            "X258": -0.697,
            "X259": -0.069,
            "X260": -0.189,
            "X261": 1.625,
            "X262": -0.616,
            "X263": 0.642,
            "X264": 0.126,
            "X265": -0.595,
            "X266": 0.772,
            "X267": 0.714,
            "X268": -0.587,
            "X269": -1.619,
            "X270": 1.507,
            "X271": 0.023,
            "X272": -0.715,
            "X273": 0.815,
            "X274": 1.307,
            "X275": 1.101,
            "X276": 1.063,
            "X277": -0.402,
            "X278": 2.096,
            "X279": 0.397,
            "X280": -2.081,
            "X281": 0.899,
            "X282": -0.16,
            "X283": -0.077,
            "X284": 0.712,
            "X285": 0.394,
            "X286": 0.475,
            "X287": -0.779,
            "X288": 1.771,
            "X289": 0.263,
            "X290": -1.564,
            "X291": 0.333,
            "X292": 1.774,
            "X293": -1.706,
            "X294": 0.234,
            "X295": 1.239,
            "X296": -0.467,
            "X297": 0.204,
            "X298": 0.896,
            "X299": -1.25
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6403,
            ".pred_1": 0.3597,
            "id": 114,
            "X0": 1.207,
            "X1": 0.407,
            "X2": -0.087,
            "X3": 0.229,
            "X4": -0.682,
            "X5": 1.03,
            "X6": 0.053,
            "X7": 0.628,
            "X8": 0.388,
            "X9": -1.232,
            "X10": -2.345,
            "X11": -1.053,
            "X12": -0.459,
            "X13": -0.189,
            "X14": -1.817,
            "X15": 0.595,
            "X16": 0.173,
            "X17": 0.484,
            "X18": 0.576,
            "X19": 0.218,
            "X20": 1.279,
            "X21": -1.033,
            "X22": -2.303,
            "X23": 1.154,
            "X24": 0.862,
            "X25": -1.891,
            "X26": -0.017,
            "X27": -1.094,
            "X28": -0.015,
            "X29": -1.465,
            "X30": -0.578,
            "X31": 0.856,
            "X32": -0.233,
            "X33": -0.025,
            "X34": -1.465,
            "X35": 1.088,
            "X36": 0.183,
            "X37": 0.269,
            "X38": -0.586,
            "X39": 0.46,
            "X40": -1.414,
            "X41": 0.872,
            "X42": 0.11,
            "X43": -2.274,
            "X44": -0.168,
            "X45": 1.208,
            "X46": -0.53,
            "X47": 0.255,
            "X48": -1.325,
            "X49": -1.047,
            "X50": 1.415,
            "X51": -1.811,
            "X52": -0.841,
            "X53": -0.119,
            "X54": 0.878,
            "X55": 0.725,
            "X56": -1.586,
            "X57": 0.393,
            "X58": 0.421,
            "X59": 0.054,
            "X60": 0.707,
            "X61": 0.255,
            "X62": 0.486,
            "X63": -1.056,
            "X64": 0.698,
            "X65": 0.014,
            "X66": -0.013,
            "X67": -1.762,
            "X68": -0.436,
            "X69": 0.681,
            "X70": 0.697,
            "X71": 0.675,
            "X72": 1.794,
            "X73": 1.468,
            "X74": -0.807,
            "X75": 0.906,
            "X76": -0.732,
            "X77": 0.301,
            "X78": 1.835,
            "X79": -0.33,
            "X80": -1.92,
            "X81": -0.031,
            "X82": -0.546,
            "X83": -0.624,
            "X84": -0.829,
            "X85": 1.144,
            "X86": -0.453,
            "X87": -0.186,
            "X88": 0.313,
            "X89": -0.907,
            "X90": -0.417,
            "X91": -0.052,
            "X92": 0.307,
            "X93": -0.079,
            "X94": 0.991,
            "X95": -0.248,
            "X96": 0.534,
            "X97": -0.431,
            "X98": -0.024,
            "X99": -0.127,
            "X100": -0.652,
            "X101": 1.144,
            "X102": -0.305,
            "X103": -0.464,
            "X104": 0.839,
            "X105": -0.144,
            "X106": -1.129,
            "X107": 0.692,
            "X108": 0.162,
            "X109": -0.83,
            "X110": -0.333,
            "X111": 0.608,
            "X112": 0.084,
            "X113": -1.996,
            "X114": 1.244,
            "X115": 1.019,
            "X116": 0.468,
            "X117": -0.466,
            "X118": 0.003,
            "X119": 1.632,
            "X120": 0.171,
            "X121": -0.204,
            "X122": -0.396,
            "X123": -0.606,
            "X124": -2.758,
            "X125": 0.805,
            "X126": -0.181,
            "X127": -1.572,
            "X128": -0.75,
            "X129": 0.161,
            "X130": -1.128,
            "X131": -0.253,
            "X132": 1.132,
            "X133": 0.352,
            "X134": 3.062,
            "X135": -0.029,
            "X136": 0.712,
            "X137": 2.609,
            "X138": -0.597,
            "X139": -0.715,
            "X140": -0.505,
            "X141": 0.056,
            "X142": -1.602,
            "X143": -1.027,
            "X144": -0.748,
            "X145": -0.368,
            "X146": -0.585,
            "X147": -1.206,
            "X148": -1.795,
            "X149": 1.251,
            "X150": 1.425,
            "X151": 0.696,
            "X152": 0.58,
            "X153": -0.854,
            "X154": -1.694,
            "X155": -0.196,
            "X156": 1.516,
            "X157": -1.34,
            "X158": -1.048,
            "X159": -0.403,
            "X160": 2.115,
            "X161": -0.84,
            "X162": 0.092,
            "X163": -0.792,
            "X164": -0.123,
            "X165": -1.453,
            "X166": 0.922,
            "X167": -0.25,
            "X168": 0.921,
            "X169": 0.527,
            "X170": -0.283,
            "X171": 0.437,
            "X172": 1.895,
            "X173": 2.011,
            "X174": 0.815,
            "X175": -0.397,
            "X176": -0.361,
            "X177": -1.15,
            "X178": -0.173,
            "X179": 1.864,
            "X180": -0.139,
            "X181": 1.724,
            "X182": 0.165,
            "X183": -1.48,
            "X184": -0.59,
            "X185": 0.396,
            "X186": 1.002,
            "X187": -0.252,
            "X188": 0.969,
            "X189": -0.356,
            "X190": 1.699,
            "X191": -1.023,
            "X192": 0.733,
            "X193": -1.191,
            "X194": 1.108,
            "X195": 0.155,
            "X196": 1.687,
            "X197": -0.202,
            "X198": 1.198,
            "X199": -0.313,
            "X200": 1.535,
            "X201": -1.493,
            "X202": -0.287,
            "X203": -0.905,
            "X204": -0.111,
            "X205": -1.083,
            "X206": -0.409,
            "X207": -0.204,
            "X208": -0.701,
            "X209": -1.165,
            "X210": -1.374,
            "X211": -0.452,
            "X212": 1.856,
            "X213": -1.082,
            "X214": -0.349,
            "X215": 0.793,
            "X216": 0.496,
            "X217": -0.574,
            "X218": 3.032,
            "X219": 1.059,
            "X220": 0.8,
            "X221": -2.785,
            "X222": -0.141,
            "X223": 0.102,
            "X224": -1.728,
            "X225": -0.924,
            "X226": 0.342,
            "X227": 0.966,
            "X228": 1.132,
            "X229": 0.22,
            "X230": -1.235,
            "X231": 1.795,
            "X232": -0.792,
            "X233": -0.213,
            "X234": 1.052,
            "X235": -0.257,
            "X236": 1.185,
            "X237": -0.444,
            "X238": 1.304,
            "X239": 1.95,
            "X240": -0.811,
            "X241": 0.394,
            "X242": -0.786,
            "X243": -1.891,
            "X244": 0.705,
            "X245": -1.113,
            "X246": 1.418,
            "X247": -0.664,
            "X248": 0.635,
            "X249": 1.621,
            "X250": 0.951,
            "X251": -0.573,
            "X252": -0.902,
            "X253": 0.445,
            "X254": 0.735,
            "X255": 0.837,
            "X256": 0.266,
            "X257": 0.527,
            "X258": -1.832,
            "X259": 1.768,
            "X260": -0.258,
            "X261": 2.489,
            "X262": -0.073,
            "X263": 0.71,
            "X264": -0.379,
            "X265": 0.644,
            "X266": 1.137,
            "X267": 0.764,
            "X268": 0.313,
            "X269": -0.557,
            "X270": 0.869,
            "X271": -0.798,
            "X272": 0.487,
            "X273": -0.779,
            "X274": 0.849,
            "X275": -2.085,
            "X276": 1.266,
            "X277": 1.354,
            "X278": -0.713,
            "X279": -0.538,
            "X280": 1.384,
            "X281": -0.331,
            "X282": 0.391,
            "X283": 0.223,
            "X284": -0.631,
            "X285": 0.945,
            "X286": -2.02,
            "X287": -0.26,
            "X288": -1.809,
            "X289": 0.008,
            "X290": 0.154,
            "X291": 0.28,
            "X292": 1.208,
            "X293": 0.87,
            "X294": -0.774,
            "X295": -1.211,
            "X296": -1.056,
            "X297": -0.076,
            "X298": 0.765,
            "X299": -0.335
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6167,
            ".pred_1": 0.3833,
            "id": 68,
            "X0": -1.595,
            "X1": -0.181,
            "X2": 1.5,
            "X3": -1.306,
            "X4": 0.327,
            "X5": 1.113,
            "X6": 1.037,
            "X7": 1.763,
            "X8": -2.296,
            "X9": 0.208,
            "X10": -0.131,
            "X11": 0.302,
            "X12": -1.914,
            "X13": -1.1,
            "X14": 1.277,
            "X15": 0.575,
            "X16": -1.76,
            "X17": -2.437,
            "X18": 0.639,
            "X19": -0.44,
            "X20": -0.231,
            "X21": -0.167,
            "X22": 0.216,
            "X23": 1.492,
            "X24": 0.092,
            "X25": -0.763,
            "X26": -0.109,
            "X27": 1.527,
            "X28": -1.192,
            "X29": 2.601,
            "X30": 0.388,
            "X31": -1.033,
            "X32": -0.295,
            "X33": -0.477,
            "X34": -0.517,
            "X35": -1.532,
            "X36": 0.963,
            "X37": -0.079,
            "X38": -1.173,
            "X39": -0.058,
            "X40": 0.87,
            "X41": 1.537,
            "X42": 1.573,
            "X43": 0.784,
            "X44": -0.522,
            "X45": 0.81,
            "X46": -0.995,
            "X47": -0.244,
            "X48": -1.388,
            "X49": -0.593,
            "X50": 1.633,
            "X51": -0.509,
            "X52": 1.594,
            "X53": 1.466,
            "X54": 0.37,
            "X55": 2.035,
            "X56": -1.139,
            "X57": -1.762,
            "X58": -0.445,
            "X59": -1.813,
            "X60": 0.728,
            "X61": 1.047,
            "X62": 1.138,
            "X63": -0.293,
            "X64": -0.138,
            "X65": 0.996,
            "X66": 0.382,
            "X67": 1.447,
            "X68": 2.066,
            "X69": 2.44,
            "X70": -0.734,
            "X71": 1.067,
            "X72": -0.519,
            "X73": 0.16,
            "X74": -0.45,
            "X75": 0.542,
            "X76": -1.85,
            "X77": -0.112,
            "X78": -0.935,
            "X79": -0.128,
            "X80": 0.15,
            "X81": 0.277,
            "X82": -0.404,
            "X83": 0.262,
            "X84": -0.254,
            "X85": 0.447,
            "X86": -0.175,
            "X87": 2.019,
            "X88": 0.658,
            "X89": 0.204,
            "X90": -1.988,
            "X91": 0.688,
            "X92": 1.176,
            "X93": 0.51,
            "X94": -1.365,
            "X95": 0.352,
            "X96": 1.12,
            "X97": -0.023,
            "X98": 0.374,
            "X99": 1.337,
            "X100": 0.654,
            "X101": -0.495,
            "X102": 0.248,
            "X103": -1.207,
            "X104": -1.8,
            "X105": 1.756,
            "X106": -0.066,
            "X107": 0.03,
            "X108": 1.006,
            "X109": -0.097,
            "X110": -1.534,
            "X111": -0.184,
            "X112": -0.608,
            "X113": 0.205,
            "X114": -0.104,
            "X115": -1.721,
            "X116": 0.623,
            "X117": -0.484,
            "X118": -0.143,
            "X119": -0.057,
            "X120": 1.242,
            "X121": -1.261,
            "X122": -0.169,
            "X123": -0.326,
            "X124": -0.946,
            "X125": -0.364,
            "X126": -0.999,
            "X127": 0.839,
            "X128": -0.884,
            "X129": 0.31,
            "X130": 0.751,
            "X131": -0.483,
            "X132": -0.414,
            "X133": -0.585,
            "X134": -0.799,
            "X135": -0.327,
            "X136": 0.291,
            "X137": 0.095,
            "X138": 1.586,
            "X139": 0.533,
            "X140": -0.675,
            "X141": -1.205,
            "X142": 0.171,
            "X143": 0.119,
            "X144": 0.548,
            "X145": 0.518,
            "X146": -1.366,
            "X147": 1.499,
            "X148": -0.637,
            "X149": 1.462,
            "X150": 0.429,
            "X151": -0.725,
            "X152": -0.822,
            "X153": 0.153,
            "X154": -0.228,
            "X155": -0.316,
            "X156": -0.316,
            "X157": -0.549,
            "X158": -0.367,
            "X159": 1.441,
            "X160": -0.855,
            "X161": 1.405,
            "X162": 0.06,
            "X163": 0.632,
            "X164": -0.916,
            "X165": 0.945,
            "X166": 0.052,
            "X167": 1.216,
            "X168": -0.608,
            "X169": -0.087,
            "X170": -0.488,
            "X171": 1.027,
            "X172": -1.821,
            "X173": 1.034,
            "X174": 0.747,
            "X175": 0.542,
            "X176": 1.264,
            "X177": -0.201,
            "X178": -1.257,
            "X179": 0.737,
            "X180": 0.743,
            "X181": -2.005,
            "X182": 0.614,
            "X183": 0.505,
            "X184": 1.233,
            "X185": 0.41,
            "X186": 0.294,
            "X187": 1.811,
            "X188": 0.548,
            "X189": -1.267,
            "X190": -1.2,
            "X191": -1.25,
            "X192": -0.838,
            "X193": 0.178,
            "X194": 1.109,
            "X195": -1.669,
            "X196": 0.403,
            "X197": 0.587,
            "X198": -1.387,
            "X199": -0.717,
            "X200": 0.376,
            "X201": 0.134,
            "X202": 0.06,
            "X203": -0.644,
            "X204": -1.102,
            "X205": 0.306,
            "X206": -1.399,
            "X207": 1.718,
            "X208": 1.705,
            "X209": 0.095,
            "X210": 0.857,
            "X211": -0.132,
            "X212": 0.842,
            "X213": 0.359,
            "X214": 2.282,
            "X215": 0.175,
            "X216": 0.88,
            "X217": 0.662,
            "X218": 0.571,
            "X219": -0.441,
            "X220": 0.66,
            "X221": -1.305,
            "X222": 1.034,
            "X223": 1.237,
            "X224": 0.474,
            "X225": -1.378,
            "X226": 0.271,
            "X227": 0.71,
            "X228": 0.499,
            "X229": 0.611,
            "X230": 1.678,
            "X231": 0.319,
            "X232": 0.434,
            "X233": 0.996,
            "X234": -0.232,
            "X235": 1.208,
            "X236": 0.09,
            "X237": 1.053,
            "X238": -0.684,
            "X239": -0.246,
            "X240": -0.28,
            "X241": -0.421,
            "X242": 0.985,
            "X243": 0.868,
            "X244": 1.236,
            "X245": 1.028,
            "X246": 1.82,
            "X247": 0.961,
            "X248": 0.81,
            "X249": -2.201,
            "X250": 0.163,
            "X251": 0.004,
            "X252": 0.26,
            "X253": -0.153,
            "X254": -0.31,
            "X255": -1.767,
            "X256": -0.64,
            "X257": -0.059,
            "X258": 0.895,
            "X259": -1.256,
            "X260": 0.539,
            "X261": 1.076,
            "X262": -0.752,
            "X263": 0.176,
            "X264": -0.218,
            "X265": -0.552,
            "X266": 0.816,
            "X267": -0.973,
            "X268": 0.313,
            "X269": -0.297,
            "X270": 0.109,
            "X271": 1.656,
            "X272": 0.376,
            "X273": 0.682,
            "X274": -0.223,
            "X275": 0.557,
            "X276": -0.409,
            "X277": 0.173,
            "X278": 0.206,
            "X279": 0.673,
            "X280": 0.077,
            "X281": -0.803,
            "X282": 0.159,
            "X283": 1.617,
            "X284": 1.664,
            "X285": 0.146,
            "X286": 0.158,
            "X287": -1.318,
            "X288": 0.167,
            "X289": -1.014,
            "X290": -0.743,
            "X291": -0.428,
            "X292": -0.156,
            "X293": -0.003,
            "X294": -0.239,
            "X295": 0.304,
            "X296": 0.244,
            "X297": -0.696,
            "X298": -0.612,
            "X299": -0.076
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8566,
            ".pred_1": 0.1434,
            "id": 176,
            "X0": 0.34,
            "X1": 0.578,
            "X2": -0.734,
            "X3": 0.257,
            "X4": 0.496,
            "X5": 1.391,
            "X6": -0.392,
            "X7": -0.086,
            "X8": 0.557,
            "X9": 2.213,
            "X10": 1.079,
            "X11": 0.044,
            "X12": 1.043,
            "X13": 0.839,
            "X14": -0.496,
            "X15": -0.298,
            "X16": -0.32,
            "X17": -1.462,
            "X18": -0.766,
            "X19": 0.639,
            "X20": -1.144,
            "X21": 1.522,
            "X22": -1.348,
            "X23": -0.038,
            "X24": 1.718,
            "X25": -0.886,
            "X26": -0.111,
            "X27": -0.394,
            "X28": -1.337,
            "X29": -0.192,
            "X30": 1.416,
            "X31": 0.631,
            "X32": 0.714,
            "X33": -1.491,
            "X34": -0.434,
            "X35": -0.459,
            "X36": 0.858,
            "X37": -0.248,
            "X38": 0.42,
            "X39": -0.397,
            "X40": -1.025,
            "X41": 1.074,
            "X42": 0.549,
            "X43": -1.436,
            "X44": -0.388,
            "X45": -0.585,
            "X46": 0.104,
            "X47": -0.069,
            "X48": 0.174,
            "X49": 1.164,
            "X50": -0.022,
            "X51": 0.786,
            "X52": -0.471,
            "X53": -0.064,
            "X54": -1.93,
            "X55": 0.476,
            "X56": 0.524,
            "X57": 0.421,
            "X58": 0.099,
            "X59": 0.313,
            "X60": 2.452,
            "X61": -0.411,
            "X62": -0.275,
            "X63": 0.343,
            "X64": -0.63,
            "X65": 0.204,
            "X66": 0.781,
            "X67": -0.121,
            "X68": -0.189,
            "X69": 0.089,
            "X70": 0.188,
            "X71": -0.937,
            "X72": 1.714,
            "X73": 1.256,
            "X74": 0.511,
            "X75": 0.802,
            "X76": 1.279,
            "X77": 0.483,
            "X78": -1.256,
            "X79": -0.042,
            "X80": -0.047,
            "X81": 1.621,
            "X82": 0.226,
            "X83": 0.016,
            "X84": -0.69,
            "X85": 0.182,
            "X86": -0.14,
            "X87": 0.363,
            "X88": -0.114,
            "X89": -0.112,
            "X90": -0.507,
            "X91": -0.485,
            "X92": 1.485,
            "X93": 0.614,
            "X94": -0.468,
            "X95": 0.055,
            "X96": -0.233,
            "X97": -0.193,
            "X98": 1.631,
            "X99": 0.146,
            "X100": -0.647,
            "X101": 1.046,
            "X102": 0.235,
            "X103": 0.138,
            "X104": 1.034,
            "X105": -0.417,
            "X106": -1.032,
            "X107": -0.854,
            "X108": 0.453,
            "X109": 0.617,
            "X110": 0.473,
            "X111": -3.413,
            "X112": 0.323,
            "X113": -0.287,
            "X114": -0.415,
            "X115": -1.759,
            "X116": -0.84,
            "X117": -2.091,
            "X118": 0.468,
            "X119": 0.052,
            "X120": -2.663,
            "X121": 0.087,
            "X122": -0.203,
            "X123": -0.346,
            "X124": -0.224,
            "X125": -0.012,
            "X126": -0.108,
            "X127": -1.292,
            "X128": -1.666,
            "X129": 1.394,
            "X130": 0.016,
            "X131": -1.069,
            "X132": -0.707,
            "X133": 1.025,
            "X134": -2.49,
            "X135": -1.122,
            "X136": -0.041,
            "X137": -1.089,
            "X138": 0.208,
            "X139": 1.612,
            "X140": -0.677,
            "X141": -0.891,
            "X142": 0.154,
            "X143": -0.827,
            "X144": 0.74,
            "X145": 0.319,
            "X146": 1.588,
            "X147": -0.366,
            "X148": 0.607,
            "X149": -0.727,
            "X150": 0.918,
            "X151": 0.406,
            "X152": 0.109,
            "X153": -0.553,
            "X154": 0.87,
            "X155": -1.417,
            "X156": -0.03,
            "X157": -0.05,
            "X158": -1.534,
            "X159": 0.842,
            "X160": 0.581,
            "X161": -0.39,
            "X162": 0.373,
            "X163": -2.087,
            "X164": -0.007,
            "X165": 1.159,
            "X166": 1.41,
            "X167": 1.492,
            "X168": -0.486,
            "X169": -0.639,
            "X170": 0.798,
            "X171": -0.915,
            "X172": -0.469,
            "X173": 0.537,
            "X174": -0.809,
            "X175": 0.793,
            "X176": 0.423,
            "X177": -0.429,
            "X178": -0.754,
            "X179": 0.247,
            "X180": -0.414,
            "X181": -0.875,
            "X182": -1.983,
            "X183": 1.001,
            "X184": 1.244,
            "X185": -0.101,
            "X186": 0.68,
            "X187": -1.559,
            "X188": 0.468,
            "X189": 0.968,
            "X190": -0.513,
            "X191": 0.198,
            "X192": -1.475,
            "X193": 0.004,
            "X194": 0.011,
            "X195": 0.074,
            "X196": -1.298,
            "X197": 0.547,
            "X198": 0.63,
            "X199": 0.074,
            "X200": 0.264,
            "X201": 1.135,
            "X202": -0.326,
            "X203": 0.768,
            "X204": 1.156,
            "X205": -1.4,
            "X206": -1.211,
            "X207": 1.959,
            "X208": -0.361,
            "X209": 0.828,
            "X210": 0.233,
            "X211": 1.453,
            "X212": -0.281,
            "X213": -1.024,
            "X214": 0.759,
            "X215": 0.883,
            "X216": -0.836,
            "X217": 0.168,
            "X218": 1.828,
            "X219": 0.705,
            "X220": 0.301,
            "X221": -1.364,
            "X222": -0.215,
            "X223": 0.352,
            "X224": -0.45,
            "X225": 0.206,
            "X226": 1.204,
            "X227": 0.938,
            "X228": -0.262,
            "X229": -1.718,
            "X230": -0.602,
            "X231": -0.635,
            "X232": 0.843,
            "X233": -0.649,
            "X234": 0.096,
            "X235": 0.083,
            "X236": 0.326,
            "X237": -0.014,
            "X238": -1.561,
            "X239": -0.559,
            "X240": 0.046,
            "X241": -0.205,
            "X242": 0.465,
            "X243": 0.806,
            "X244": 0.912,
            "X245": 0.29,
            "X246": 0.133,
            "X247": -2,
            "X248": 0.554,
            "X249": -0.512,
            "X250": 0.92,
            "X251": 0.385,
            "X252": 0.129,
            "X253": 0.708,
            "X254": -1.162,
            "X255": -0.885,
            "X256": 0.078,
            "X257": -2.15,
            "X258": 1.018,
            "X259": 0.547,
            "X260": -0.833,
            "X261": 0.179,
            "X262": -0.249,
            "X263": -0.878,
            "X264": 0.343,
            "X265": 0.606,
            "X266": 0.342,
            "X267": -0.272,
            "X268": 0.209,
            "X269": 0.034,
            "X270": 0.19,
            "X271": 0.555,
            "X272": -0.264,
            "X273": -1.825,
            "X274": -0.638,
            "X275": 1.006,
            "X276": -0.207,
            "X277": 0.191,
            "X278": 0.101,
            "X279": 1.73,
            "X280": 0.61,
            "X281": 1.196,
            "X282": -0.043,
            "X283": -0.987,
            "X284": -0.129,
            "X285": -0.601,
            "X286": -1.431,
            "X287": -1.26,
            "X288": 2.231,
            "X289": 0.775,
            "X290": -1.942,
            "X291": 0.933,
            "X292": -0.214,
            "X293": -1.106,
            "X294": -0.72,
            "X295": 2.126,
            "X296": -0.18,
            "X297": 2.388,
            "X298": -0.794,
            "X299": 0.11
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7693,
            ".pred_1": 0.2307,
            "id": 45,
            "X0": -0.556,
            "X1": 0.032,
            "X2": -2.097,
            "X3": 0.348,
            "X4": 2.122,
            "X5": 0.915,
            "X6": 0.034,
            "X7": -1,
            "X8": -0.345,
            "X9": -0.284,
            "X10": 0.747,
            "X11": -1.114,
            "X12": 0.265,
            "X13": -0.244,
            "X14": 0.848,
            "X15": 1.2,
            "X16": -1.356,
            "X17": 1.206,
            "X18": 0.337,
            "X19": 2.142,
            "X20": -0.219,
            "X21": -0.621,
            "X22": 0.368,
            "X23": 0.193,
            "X24": -1.153,
            "X25": -0.681,
            "X26": -0.656,
            "X27": 0.97,
            "X28": 0.075,
            "X29": 0.021,
            "X30": 1.736,
            "X31": -1.387,
            "X32": 0.076,
            "X33": -0.431,
            "X34": 0.737,
            "X35": -0.389,
            "X36": 0.748,
            "X37": -1.179,
            "X38": 0.251,
            "X39": 2.133,
            "X40": 0.273,
            "X41": -0.8,
            "X42": -0.724,
            "X43": 0.406,
            "X44": -0.85,
            "X45": -0.868,
            "X46": -2.173,
            "X47": 0.416,
            "X48": -1.725,
            "X49": -0.226,
            "X50": -1.472,
            "X51": 0.832,
            "X52": 0.438,
            "X53": 0.632,
            "X54": 0.464,
            "X55": 0.106,
            "X56": -1.117,
            "X57": 0.227,
            "X58": -0.487,
            "X59": 0.814,
            "X60": -1.086,
            "X61": 2.066,
            "X62": -2.128,
            "X63": 2.136,
            "X64": -0.565,
            "X65": 2.2,
            "X66": 1.166,
            "X67": -1.009,
            "X68": 2.461,
            "X69": -0.958,
            "X70": -0.038,
            "X71": 0.5,
            "X72": -1.696,
            "X73": 1.023,
            "X74": 0.092,
            "X75": -0.227,
            "X76": -0.132,
            "X77": 1.257,
            "X78": -0.478,
            "X79": -0.497,
            "X80": 0.089,
            "X81": 0.444,
            "X82": -0.763,
            "X83": -0.111,
            "X84": -1.211,
            "X85": -1.463,
            "X86": 0.387,
            "X87": -0.158,
            "X88": -0.123,
            "X89": -0.969,
            "X90": 0.505,
            "X91": -1.325,
            "X92": -1,
            "X93": 0.906,
            "X94": 0.645,
            "X95": 2.351,
            "X96": 0.404,
            "X97": 2.388,
            "X98": 0.061,
            "X99": 0.59,
            "X100": 1.1,
            "X101": -0.053,
            "X102": 0.357,
            "X103": -0.096,
            "X104": -0.744,
            "X105": -0.725,
            "X106": 0.411,
            "X107": -1.129,
            "X108": 0.876,
            "X109": -0.219,
            "X110": 0.7,
            "X111": 0.806,
            "X112": 0.197,
            "X113": -0.117,
            "X114": 0.123,
            "X115": -0.283,
            "X116": -1.098,
            "X117": 0.742,
            "X118": 0.478,
            "X119": -2.267,
            "X120": -0.225,
            "X121": 0.239,
            "X122": 0.169,
            "X123": 2.734,
            "X124": 1.308,
            "X125": 0.373,
            "X126": 0.645,
            "X127": -1.018,
            "X128": -2.303,
            "X129": 1.294,
            "X130": -0.5,
            "X131": 0.519,
            "X132": -0.874,
            "X133": -0.532,
            "X134": 0.253,
            "X135": 0.607,
            "X136": -0.93,
            "X137": 0.631,
            "X138": 1.386,
            "X139": -0.155,
            "X140": -0.657,
            "X141": 0.179,
            "X142": 1.215,
            "X143": -0.594,
            "X144": -0.801,
            "X145": -0.01,
            "X146": 0.189,
            "X147": -0.766,
            "X148": 2.885,
            "X149": -1.22,
            "X150": -0.074,
            "X151": 0.022,
            "X152": 0.923,
            "X153": 0.524,
            "X154": 0.444,
            "X155": 2.378,
            "X156": -0.161,
            "X157": -0.381,
            "X158": 0.297,
            "X159": 0.306,
            "X160": 0.625,
            "X161": -0.159,
            "X162": 1.097,
            "X163": 0.143,
            "X164": 0.324,
            "X165": 1.695,
            "X166": -0.306,
            "X167": -1.71,
            "X168": -1.167,
            "X169": 0.863,
            "X170": 0.416,
            "X171": -1.119,
            "X172": 0.272,
            "X173": -0.271,
            "X174": 0.709,
            "X175": -1.776,
            "X176": 1.083,
            "X177": 1.957,
            "X178": -1.498,
            "X179": -0.366,
            "X180": -0.62,
            "X181": 0.813,
            "X182": 1.425,
            "X183": -0.923,
            "X184": 2.262,
            "X185": 0.153,
            "X186": -0.144,
            "X187": -0.779,
            "X188": -0.589,
            "X189": -1.837,
            "X190": 0.083,
            "X191": -0.016,
            "X192": -0.812,
            "X193": -0.543,
            "X194": 0.077,
            "X195": -2.25,
            "X196": 0.797,
            "X197": -0.336,
            "X198": 0.393,
            "X199": 0.896,
            "X200": -0.959,
            "X201": -0.74,
            "X202": -1.399,
            "X203": 0.206,
            "X204": 0.149,
            "X205": 1.217,
            "X206": 0.296,
            "X207": -1.028,
            "X208": 0.054,
            "X209": -0.261,
            "X210": -1.948,
            "X211": -2.918,
            "X212": 1.27,
            "X213": -1.122,
            "X214": 1.022,
            "X215": 1.095,
            "X216": 1.159,
            "X217": 1.475,
            "X218": -0.985,
            "X219": 0.649,
            "X220": -0.078,
            "X221": 0.492,
            "X222": -0.453,
            "X223": 1.097,
            "X224": -0.695,
            "X225": -0.233,
            "X226": 0.669,
            "X227": -0.729,
            "X228": -0.702,
            "X229": -0.579,
            "X230": 1.338,
            "X231": -0.575,
            "X232": -0.286,
            "X233": -0.021,
            "X234": 0.248,
            "X235": 0.362,
            "X236": 0.96,
            "X237": 0.689,
            "X238": -0.223,
            "X239": 0.4,
            "X240": -0.282,
            "X241": 0.558,
            "X242": -1.204,
            "X243": 0.424,
            "X244": 0.525,
            "X245": -1.49,
            "X246": 1.048,
            "X247": 1.275,
            "X248": 0.177,
            "X249": 1.194,
            "X250": 0.116,
            "X251": 0.802,
            "X252": 1.052,
            "X253": 1.423,
            "X254": -1.376,
            "X255": -2.087,
            "X256": 0.563,
            "X257": -0.569,
            "X258": 0.003,
            "X259": 1.685,
            "X260": 0.966,
            "X261": 0.483,
            "X262": 1.174,
            "X263": 0.507,
            "X264": 1.056,
            "X265": 0.75,
            "X266": -2.887,
            "X267": 0.168,
            "X268": -0.22,
            "X269": 0.704,
            "X270": -1.723,
            "X271": -0.307,
            "X272": -0.214,
            "X273": 0.505,
            "X274": -1.054,
            "X275": 0.76,
            "X276": 0.847,
            "X277": -0.01,
            "X278": 0.297,
            "X279": -0.539,
            "X280": -0.984,
            "X281": 0.135,
            "X282": -2.027,
            "X283": -0.143,
            "X284": 0.45,
            "X285": -0.446,
            "X286": -0.692,
            "X287": -2.606,
            "X288": -1.101,
            "X289": -0.184,
            "X290": 0.458,
            "X291": 1.032,
            "X292": -0.811,
            "X293": 0.578,
            "X294": -2.099,
            "X295": 0.567,
            "X296": 0.424,
            "X297": -2.665,
            "X298": 0.137,
            "X299": 1.162
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8824,
            ".pred_1": 0.1176,
            "id": 55,
            "X0": 0.721,
            "X1": 0.33,
            "X2": 1.452,
            "X3": -0.85,
            "X4": 0.421,
            "X5": 0.021,
            "X6": 1.823,
            "X7": 1.216,
            "X8": -0.307,
            "X9": -0.444,
            "X10": -0.219,
            "X11": 0.685,
            "X12": 0.616,
            "X13": -0.622,
            "X14": 0.062,
            "X15": -2.318,
            "X16": -0.631,
            "X17": 1.443,
            "X18": -0.961,
            "X19": -0.409,
            "X20": 0.973,
            "X21": -0.65,
            "X22": -1.435,
            "X23": -0.001,
            "X24": 0.657,
            "X25": 0.598,
            "X26": 0.057,
            "X27": 1.338,
            "X28": -0.206,
            "X29": -0.43,
            "X30": 0.376,
            "X31": 0.608,
            "X32": 0.006,
            "X33": -0.041,
            "X34": -0.476,
            "X35": -2.181,
            "X36": 0.258,
            "X37": -0.416,
            "X38": 0.018,
            "X39": -0.205,
            "X40": -0.131,
            "X41": -0.072,
            "X42": -0.2,
            "X43": 0.38,
            "X44": -2.6,
            "X45": 0.563,
            "X46": 1.573,
            "X47": -0.378,
            "X48": 1.119,
            "X49": 0.305,
            "X50": 3.139,
            "X51": 0.184,
            "X52": -0.233,
            "X53": -2.454,
            "X54": -0.323,
            "X55": 0.041,
            "X56": 0.021,
            "X57": -0.038,
            "X58": -0.427,
            "X59": 0.725,
            "X60": 2.086,
            "X61": -0.694,
            "X62": -0.684,
            "X63": 0.248,
            "X64": 0.658,
            "X65": 1.364,
            "X66": 0.275,
            "X67": 0.701,
            "X68": 0.937,
            "X69": -0.975,
            "X70": -0.421,
            "X71": -1.079,
            "X72": -0.872,
            "X73": 0.65,
            "X74": -0.175,
            "X75": -0.627,
            "X76": 0.933,
            "X77": 0.017,
            "X78": -1.069,
            "X79": -1.298,
            "X80": 1.038,
            "X81": -1.013,
            "X82": -0.646,
            "X83": -0.112,
            "X84": -1.737,
            "X85": -1.578,
            "X86": -1.125,
            "X87": 0.472,
            "X88": 0.326,
            "X89": 0.284,
            "X90": 0.296,
            "X91": 0.502,
            "X92": -1.777,
            "X93": -2.103,
            "X94": 1.951,
            "X95": -0.103,
            "X96": 0.481,
            "X97": -0.536,
            "X98": -1.119,
            "X99": -1.184,
            "X100": -1.556,
            "X101": -0.255,
            "X102": 0.856,
            "X103": 0.371,
            "X104": -0.061,
            "X105": 0.723,
            "X106": -0.701,
            "X107": 0.488,
            "X108": -0.98,
            "X109": 0.161,
            "X110": -0.416,
            "X111": 1.541,
            "X112": -1.446,
            "X113": 0.961,
            "X114": 0.142,
            "X115": 0.349,
            "X116": -1.128,
            "X117": -0.351,
            "X118": -1.171,
            "X119": -1.55,
            "X120": 0.114,
            "X121": -2.447,
            "X122": -2.05,
            "X123": 0.265,
            "X124": 0.565,
            "X125": -0.552,
            "X126": 1.525,
            "X127": -1.319,
            "X128": 0.923,
            "X129": 0.996,
            "X130": 0.086,
            "X131": 0.169,
            "X132": 1.226,
            "X133": -0.099,
            "X134": -0.093,
            "X135": -0.563,
            "X136": 0.483,
            "X137": -0.636,
            "X138": -0.102,
            "X139": 0.385,
            "X140": -0.641,
            "X141": 0.185,
            "X142": 0.111,
            "X143": 0.189,
            "X144": 0.997,
            "X145": -0.598,
            "X146": -1.213,
            "X147": 0.552,
            "X148": 0.822,
            "X149": -1.269,
            "X150": 0.678,
            "X151": 0.614,
            "X152": -0.143,
            "X153": 0.306,
            "X154": -0.496,
            "X155": 0.09,
            "X156": -0.304,
            "X157": 0.637,
            "X158": -0.327,
            "X159": -0.394,
            "X160": 0.485,
            "X161": 1.018,
            "X162": 0.52,
            "X163": 0.765,
            "X164": 0.325,
            "X165": -0.273,
            "X166": 1.312,
            "X167": -0.679,
            "X168": -0.585,
            "X169": -0.865,
            "X170": 0.937,
            "X171": 0.638,
            "X172": 0.186,
            "X173": 0.212,
            "X174": 0.368,
            "X175": 0.201,
            "X176": 0.681,
            "X177": 2.034,
            "X178": 1.209,
            "X179": -1.398,
            "X180": -0.347,
            "X181": 0.41,
            "X182": -1.477,
            "X183": 0.724,
            "X184": -0.423,
            "X185": -0.77,
            "X186": 0.022,
            "X187": 0.013,
            "X188": -0.603,
            "X189": 0.195,
            "X190": -0.648,
            "X191": 0.353,
            "X192": -1.558,
            "X193": -1.317,
            "X194": 1.593,
            "X195": 0.174,
            "X196": -0.535,
            "X197": 0.047,
            "X198": -1.369,
            "X199": 0.441,
            "X200": -1.072,
            "X201": -0.111,
            "X202": 1.361,
            "X203": -0.301,
            "X204": -0.847,
            "X205": 1.444,
            "X206": 0.746,
            "X207": -0.507,
            "X208": 1.179,
            "X209": 1.129,
            "X210": -0.455,
            "X211": -0.042,
            "X212": -0.504,
            "X213": 0.264,
            "X214": -0.239,
            "X215": 0.346,
            "X216": 1.7,
            "X217": 2.301,
            "X218": -0.109,
            "X219": 1.354,
            "X220": 1.089,
            "X221": 1.236,
            "X222": -1.136,
            "X223": 0.689,
            "X224": 0.267,
            "X225": 1.069,
            "X226": 0.197,
            "X227": 0.662,
            "X228": 0.532,
            "X229": 1.857,
            "X230": -0.083,
            "X231": -0.679,
            "X232": -0.933,
            "X233": 1.075,
            "X234": 0.952,
            "X235": 0.795,
            "X236": 0.767,
            "X237": -0.518,
            "X238": -0.376,
            "X239": 0.927,
            "X240": -0.239,
            "X241": -0.406,
            "X242": -0.98,
            "X243": 0.404,
            "X244": 0.216,
            "X245": -0.373,
            "X246": 0.066,
            "X247": -0.254,
            "X248": -0.968,
            "X249": -0.803,
            "X250": -0.129,
            "X251": -0.376,
            "X252": 1.788,
            "X253": 1.08,
            "X254": 0.39,
            "X255": -1.496,
            "X256": -0.583,
            "X257": 0.068,
            "X258": 0.549,
            "X259": 1.034,
            "X260": 0.637,
            "X261": 1.08,
            "X262": 0.405,
            "X263": 0.099,
            "X264": 0.956,
            "X265": -0.084,
            "X266": -0.856,
            "X267": -0.265,
            "X268": 0.632,
            "X269": -0.095,
            "X270": -0.459,
            "X271": 0.275,
            "X272": -0.123,
            "X273": -0.933,
            "X274": -0.315,
            "X275": -0.39,
            "X276": 0.352,
            "X277": -0.753,
            "X278": 0.244,
            "X279": -0.896,
            "X280": -0.699,
            "X281": -0.246,
            "X282": 1.464,
            "X283": -0.156,
            "X284": 0.255,
            "X285": -1.709,
            "X286": 0.338,
            "X287": -0.578,
            "X288": -0.32,
            "X289": -0.187,
            "X290": -0.815,
            "X291": 0.761,
            "X292": -0.373,
            "X293": 0.166,
            "X294": -0.381,
            "X295": 1.278,
            "X296": -1.428,
            "X297": -0.629,
            "X298": -1.419,
            "X299": -1.541
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8576,
            ".pred_1": 0.1424,
            "id": 10,
            "X0": 1.947,
            "X1": 0.557,
            "X2": 0.174,
            "X3": -0.862,
            "X4": 0.205,
            "X5": 0.018,
            "X6": 1.229,
            "X7": -2.221,
            "X8": 1.43,
            "X9": -0.354,
            "X10": 0.048,
            "X11": 1.208,
            "X12": 0.156,
            "X13": -0.866,
            "X14": -0.557,
            "X15": -0.937,
            "X16": -0.507,
            "X17": 0.476,
            "X18": -1.867,
            "X19": -0.76,
            "X20": -0.988,
            "X21": 0.885,
            "X22": -0.015,
            "X23": 1.079,
            "X24": 0.627,
            "X25": -0.368,
            "X26": 0.692,
            "X27": -1.288,
            "X28": 0.413,
            "X29": -0.71,
            "X30": -0.046,
            "X31": -0.216,
            "X32": -1.519,
            "X33": -2.704,
            "X34": 1.138,
            "X35": -0.963,
            "X36": -0.52,
            "X37": -0.678,
            "X38": 2.002,
            "X39": 0.442,
            "X40": -0.716,
            "X41": -0.635,
            "X42": 2.129,
            "X43": 0.216,
            "X44": -1.058,
            "X45": 0.822,
            "X46": 0.202,
            "X47": -0.385,
            "X48": 0.632,
            "X49": 0.393,
            "X50": -1.61,
            "X51": 0.903,
            "X52": -0.552,
            "X53": -1.283,
            "X54": -1.028,
            "X55": 1.399,
            "X56": 2.523,
            "X57": 0.134,
            "X58": -0.824,
            "X59": 0.236,
            "X60": -1.142,
            "X61": -0.737,
            "X62": 0.088,
            "X63": -0.588,
            "X64": 1.193,
            "X65": -0.737,
            "X66": 0.994,
            "X67": -1.351,
            "X68": -1.415,
            "X69": 4.111,
            "X70": -1.456,
            "X71": -0.249,
            "X72": -0.343,
            "X73": 0.258,
            "X74": -0.253,
            "X75": 0.808,
            "X76": -0.319,
            "X77": 1.045,
            "X78": 0.28,
            "X79": 0.463,
            "X80": 0.094,
            "X81": 0.475,
            "X82": -0.398,
            "X83": -0.412,
            "X84": -0.352,
            "X85": -0.673,
            "X86": 0.837,
            "X87": 0.526,
            "X88": -0.383,
            "X89": 0.018,
            "X90": 1.063,
            "X91": -1.142,
            "X92": 0.422,
            "X93": -0.134,
            "X94": -0.313,
            "X95": 0.067,
            "X96": -0.768,
            "X97": -0.478,
            "X98": -0.392,
            "X99": -0.94,
            "X100": -2.366,
            "X101": -0.96,
            "X102": 0.219,
            "X103": -0.45,
            "X104": -0.849,
            "X105": 0.265,
            "X106": -0.255,
            "X107": -0.849,
            "X108": -0.685,
            "X109": -0.473,
            "X110": 1.048,
            "X111": -0.453,
            "X112": 0.304,
            "X113": -0.143,
            "X114": 0.405,
            "X115": -0.382,
            "X116": 0.513,
            "X117": -0.527,
            "X118": 1.518,
            "X119": 0.551,
            "X120": -2.205,
            "X121": 0.111,
            "X122": 0.792,
            "X123": -0.808,
            "X124": 1.513,
            "X125": 1.458,
            "X126": -0.872,
            "X127": 1.563,
            "X128": -1.464,
            "X129": -1.352,
            "X130": -1.131,
            "X131": 0.714,
            "X132": -0.606,
            "X133": 1.595,
            "X134": 0.697,
            "X135": -0.226,
            "X136": -0.911,
            "X137": -1.905,
            "X138": 0.411,
            "X139": 1.044,
            "X140": 0.954,
            "X141": -1.229,
            "X142": 2.26,
            "X143": 0.505,
            "X144": -1.208,
            "X145": -1.089,
            "X146": -0.059,
            "X147": 0.452,
            "X148": -0.189,
            "X149": -1.098,
            "X150": 1.549,
            "X151": -0.061,
            "X152": -0.026,
            "X153": -1.461,
            "X154": -0.832,
            "X155": -1.015,
            "X156": -0.64,
            "X157": 0.431,
            "X158": 0.807,
            "X159": 1.066,
            "X160": -0.86,
            "X161": -0.358,
            "X162": -1.463,
            "X163": -0.947,
            "X164": -1.33,
            "X165": 0.217,
            "X166": -1.174,
            "X167": -0.065,
            "X168": 0.127,
            "X169": -0.612,
            "X170": -1.759,
            "X171": 1.88,
            "X172": 0.666,
            "X173": -0.798,
            "X174": -0.333,
            "X175": -2.957,
            "X176": 0.913,
            "X177": -1.978,
            "X178": 1.777,
            "X179": -0.598,
            "X180": -0.528,
            "X181": 0.763,
            "X182": 0.709,
            "X183": -0.448,
            "X184": 0.382,
            "X185": 1.454,
            "X186": 1.077,
            "X187": 0.104,
            "X188": 0.545,
            "X189": 0.437,
            "X190": 1.081,
            "X191": -2.156,
            "X192": 0.093,
            "X193": 1.565,
            "X194": 0.122,
            "X195": 0.321,
            "X196": 0.537,
            "X197": -2.091,
            "X198": 1.023,
            "X199": 2.303,
            "X200": 0.756,
            "X201": -1.033,
            "X202": -1.08,
            "X203": -1.025,
            "X204": -0.684,
            "X205": -0.695,
            "X206": 1.013,
            "X207": 0.345,
            "X208": -0.129,
            "X209": 0.172,
            "X210": -0.753,
            "X211": -0.531,
            "X212": -1.056,
            "X213": 0.953,
            "X214": -1.081,
            "X215": -0.421,
            "X216": 0.498,
            "X217": 0.047,
            "X218": 0.606,
            "X219": 1.056,
            "X220": 0.866,
            "X221": 0.316,
            "X222": -0.836,
            "X223": 0.636,
            "X224": 1.144,
            "X225": -1.326,
            "X226": 1.058,
            "X227": 0.558,
            "X228": 0.9,
            "X229": -0.422,
            "X230": 0.43,
            "X231": 1.424,
            "X232": -0.402,
            "X233": 1.887,
            "X234": 0.953,
            "X235": -0.835,
            "X236": 1.585,
            "X237": -0.652,
            "X238": 0.502,
            "X239": 2.287,
            "X240": -0.325,
            "X241": -0.401,
            "X242": 1.701,
            "X243": -1.02,
            "X244": 0.166,
            "X245": 0.387,
            "X246": -0.285,
            "X247": -1.101,
            "X248": 0.927,
            "X249": -1.871,
            "X250": 0.42,
            "X251": 0.558,
            "X252": -1.92,
            "X253": 0.042,
            "X254": -2.333,
            "X255": 0.901,
            "X256": 0.384,
            "X257": 0.007,
            "X258": 0.115,
            "X259": 0.767,
            "X260": 0.61,
            "X261": -1.707,
            "X262": -0.407,
            "X263": 0.247,
            "X264": -0.21,
            "X265": -0.09,
            "X266": 1.916,
            "X267": 1.097,
            "X268": -0.75,
            "X269": -0.155,
            "X270": 0.833,
            "X271": 1.284,
            "X272": 1.371,
            "X273": -1.367,
            "X274": -0.738,
            "X275": 2.159,
            "X276": 1.64,
            "X277": 0.889,
            "X278": -0.434,
            "X279": 1.002,
            "X280": -0.555,
            "X281": 0.484,
            "X282": 1.268,
            "X283": 0.627,
            "X284": -0.473,
            "X285": -0.17,
            "X286": -0.174,
            "X287": 0.374,
            "X288": -1.208,
            "X289": 0.862,
            "X290": -0.893,
            "X291": -0.961,
            "X292": 0.507,
            "X293": -0.149,
            "X294": 2.716,
            "X295": -0.847,
            "X296": -0.504,
            "X297": 0.741,
            "X298": 0.85,
            "X299": 1.354
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.5912,
            ".pred_1": 0.4088,
            "id": 30,
            "X0": -0.664,
            "X1": -0.735,
            "X2": -0.284,
            "X3": 0.451,
            "X4": -0.383,
            "X5": -0.739,
            "X6": 0.512,
            "X7": 0.568,
            "X8": 0.378,
            "X9": 1.239,
            "X10": -0.093,
            "X11": 0.396,
            "X12": -1.089,
            "X13": 0.917,
            "X14": 0.098,
            "X15": 0.212,
            "X16": 0.247,
            "X17": -2.015,
            "X18": -0.702,
            "X19": 0.357,
            "X20": -1.506,
            "X21": -1.265,
            "X22": 0.789,
            "X23": -0.594,
            "X24": 0.003,
            "X25": 0.349,
            "X26": 0.364,
            "X27": 1.25,
            "X28": -0.679,
            "X29": 1.943,
            "X30": -0.476,
            "X31": 0.335,
            "X32": 2.521,
            "X33": -0.724,
            "X34": 1.573,
            "X35": -0.293,
            "X36": 0.567,
            "X37": -3.298,
            "X38": -0.193,
            "X39": 0.086,
            "X40": -1.064,
            "X41": 1.031,
            "X42": 0.534,
            "X43": -0.552,
            "X44": 0.349,
            "X45": -1.203,
            "X46": 0.223,
            "X47": -0.173,
            "X48": 0.346,
            "X49": -2.034,
            "X50": -0.793,
            "X51": 0.524,
            "X52": 2.415,
            "X53": -1.114,
            "X54": -0.383,
            "X55": -1.121,
            "X56": 1.106,
            "X57": 0.198,
            "X58": -0.178,
            "X59": -0.124,
            "X60": -1.537,
            "X61": 0.855,
            "X62": 0.961,
            "X63": -1.13,
            "X64": -0.821,
            "X65": 0.876,
            "X66": 1.17,
            "X67": 1.732,
            "X68": -0.462,
            "X69": -0.532,
            "X70": -0.656,
            "X71": 0.103,
            "X72": 0.47,
            "X73": 1.058,
            "X74": -1.003,
            "X75": -1.408,
            "X76": -0.33,
            "X77": 1.75,
            "X78": 0.368,
            "X79": 0.723,
            "X80": -1.289,
            "X81": 0.004,
            "X82": -0.503,
            "X83": -0.305,
            "X84": -1.645,
            "X85": 0.798,
            "X86": -0.371,
            "X87": -1.931,
            "X88": -0.593,
            "X89": -2.071,
            "X90": 0.453,
            "X91": -0.074,
            "X92": -0.123,
            "X93": -0.223,
            "X94": -0.251,
            "X95": -2.034,
            "X96": 1.477,
            "X97": 0.03,
            "X98": 0.257,
            "X99": 0.166,
            "X100": -0.766,
            "X101": -0.489,
            "X102": -1.56,
            "X103": 2.422,
            "X104": 0.897,
            "X105": 0.882,
            "X106": -1.734,
            "X107": 0.448,
            "X108": -0.102,
            "X109": 2.073,
            "X110": -2.673,
            "X111": 1.525,
            "X112": 1.886,
            "X113": 0.079,
            "X114": -0.837,
            "X115": -0.291,
            "X116": -1.906,
            "X117": 0.369,
            "X118": 0.117,
            "X119": -0.888,
            "X120": -0.344,
            "X121": -0.221,
            "X122": 0.6,
            "X123": -0.729,
            "X124": -1.199,
            "X125": -0.751,
            "X126": 0.48,
            "X127": -0.292,
            "X128": -1.062,
            "X129": 2.358,
            "X130": 0.793,
            "X131": -1.743,
            "X132": -0.709,
            "X133": -0.638,
            "X134": -0.793,
            "X135": 0.417,
            "X136": 0.066,
            "X137": 0.636,
            "X138": 0.882,
            "X139": 1.003,
            "X140": -0.152,
            "X141": -0.203,
            "X142": -0.649,
            "X143": 1.198,
            "X144": 0.611,
            "X145": -0.324,
            "X146": 0.592,
            "X147": 0.809,
            "X148": -0.958,
            "X149": 0.431,
            "X150": -0.734,
            "X151": -0.038,
            "X152": 1.165,
            "X153": -0.433,
            "X154": -0.052,
            "X155": -0.889,
            "X156": 0.55,
            "X157": 1.921,
            "X158": 0.052,
            "X159": 0.149,
            "X160": 0.049,
            "X161": 1.425,
            "X162": -1.092,
            "X163": 1.395,
            "X164": 0.74,
            "X165": 0.151,
            "X166": 1.707,
            "X167": -0.24,
            "X168": -1.331,
            "X169": 0.684,
            "X170": -1.109,
            "X171": 0.036,
            "X172": -0.412,
            "X173": -1.367,
            "X174": 0.993,
            "X175": -0.814,
            "X176": -1.123,
            "X177": -1.096,
            "X178": 0,
            "X179": -2.541,
            "X180": 1.227,
            "X181": -0.953,
            "X182": -0.331,
            "X183": 0.166,
            "X184": -0.877,
            "X185": -0.156,
            "X186": -0.867,
            "X187": -0.435,
            "X188": 1.84,
            "X189": -1.736,
            "X190": 0.463,
            "X191": 2.383,
            "X192": -1.01,
            "X193": 0.022,
            "X194": 0.115,
            "X195": -1.62,
            "X196": 0.481,
            "X197": 0.297,
            "X198": -0.693,
            "X199": 0.806,
            "X200": 1.161,
            "X201": 0.496,
            "X202": 0.729,
            "X203": -0.009,
            "X204": -1.528,
            "X205": -1.249,
            "X206": 0.238,
            "X207": -0.966,
            "X208": 0.234,
            "X209": 0.268,
            "X210": 0.911,
            "X211": -0.876,
            "X212": -0.714,
            "X213": 0.165,
            "X214": 0.719,
            "X215": 0.039,
            "X216": 0.666,
            "X217": 0.986,
            "X218": 0.459,
            "X219": 0.538,
            "X220": -0.927,
            "X221": -0.323,
            "X222": 0.71,
            "X223": -0.969,
            "X224": -0.59,
            "X225": 0.994,
            "X226": -0.33,
            "X227": -1.724,
            "X228": -0.592,
            "X229": 0.402,
            "X230": -1.422,
            "X231": -0.028,
            "X232": 1.146,
            "X233": 0.434,
            "X234": -1.719,
            "X235": -0.103,
            "X236": -0.737,
            "X237": 1.372,
            "X238": -0.907,
            "X239": -1.1,
            "X240": -0.176,
            "X241": 1.224,
            "X242": 1.118,
            "X243": 0.076,
            "X244": 0.951,
            "X245": -0.966,
            "X246": -0.631,
            "X247": -1.055,
            "X248": -1.207,
            "X249": -0.084,
            "X250": 0.819,
            "X251": 0.175,
            "X252": -0.428,
            "X253": -0.014,
            "X254": 0.962,
            "X255": 0.047,
            "X256": -1.138,
            "X257": -0.834,
            "X258": 1.317,
            "X259": 0.882,
            "X260": -0.104,
            "X261": -0.128,
            "X262": 1.176,
            "X263": -0.071,
            "X264": 1.165,
            "X265": 0.565,
            "X266": 0.832,
            "X267": 0.925,
            "X268": -0.46,
            "X269": 0.259,
            "X270": 0.578,
            "X271": -0.33,
            "X272": 0.369,
            "X273": 1.712,
            "X274": -1.369,
            "X275": 0.308,
            "X276": -0.141,
            "X277": 1.371,
            "X278": 0.782,
            "X279": -0.365,
            "X280": 0.651,
            "X281": -0.157,
            "X282": -0.982,
            "X283": -0.156,
            "X284": 0.878,
            "X285": 0.733,
            "X286": 0.395,
            "X287": -0.757,
            "X288": -1.385,
            "X289": 1.429,
            "X290": -1.106,
            "X291": -0.991,
            "X292": -0.05,
            "X293": -3.071,
            "X294": -0.31,
            "X295": -0.127,
            "X296": 1.594,
            "X297": -0.195,
            "X298": -1.19,
            "X299": -0.479
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6544,
            ".pred_1": 0.3456,
            "id": 120,
            "X0": 1.495,
            "X1": 0.772,
            "X2": 0.783,
            "X3": -0.696,
            "X4": -0.305,
            "X5": -0.679,
            "X6": -0.688,
            "X7": 0.182,
            "X8": -0.242,
            "X9": -1.404,
            "X10": 0.389,
            "X11": -1.149,
            "X12": -0.783,
            "X13": -0.17,
            "X14": 0.05,
            "X15": 1.387,
            "X16": -1.1,
            "X17": -0.26,
            "X18": -0.119,
            "X19": -1.013,
            "X20": -0.783,
            "X21": -1.204,
            "X22": 1.728,
            "X23": -0.707,
            "X24": 0.194,
            "X25": -0.927,
            "X26": -0.16,
            "X27": 1.318,
            "X28": -0.284,
            "X29": -0.95,
            "X30": -0.249,
            "X31": 0.351,
            "X32": -1.699,
            "X33": -0.482,
            "X34": 0.248,
            "X35": 1.14,
            "X36": -1.047,
            "X37": -1.573,
            "X38": -3.397,
            "X39": -0.31,
            "X40": 1.163,
            "X41": -0.839,
            "X42": 0.219,
            "X43": 0.932,
            "X44": 0.526,
            "X45": -0.793,
            "X46": -0.346,
            "X47": 0.542,
            "X48": 0.258,
            "X49": 0.081,
            "X50": -0.525,
            "X51": -0.094,
            "X52": 0.406,
            "X53": -1.023,
            "X54": -1.167,
            "X55": 1.229,
            "X56": -1.858,
            "X57": 0.287,
            "X58": -1.41,
            "X59": 1.346,
            "X60": 0.301,
            "X61": 0.017,
            "X62": -0.045,
            "X63": -0.351,
            "X64": -0.8,
            "X65": 0.913,
            "X66": 0.897,
            "X67": -1.681,
            "X68": 0.58,
            "X69": 1.45,
            "X70": 1.394,
            "X71": 1.238,
            "X72": -1.494,
            "X73": -0.269,
            "X74": -1.714,
            "X75": 0.558,
            "X76": 0.883,
            "X77": -0.809,
            "X78": -0.249,
            "X79": -0.078,
            "X80": -0.624,
            "X81": -0.431,
            "X82": 0.394,
            "X83": 1.861,
            "X84": 0.159,
            "X85": 0.299,
            "X86": 0.25,
            "X87": -0.512,
            "X88": 0.555,
            "X89": 0.277,
            "X90": -0.939,
            "X91": -0.558,
            "X92": -0.65,
            "X93": 0.361,
            "X94": 0.582,
            "X95": -1.632,
            "X96": 0.944,
            "X97": 0.035,
            "X98": 0.134,
            "X99": -1.823,
            "X100": 2.022,
            "X101": -1.487,
            "X102": -0.811,
            "X103": 0.943,
            "X104": -0.843,
            "X105": -0.627,
            "X106": 1.241,
            "X107": 1.638,
            "X108": -2.02,
            "X109": 0.253,
            "X110": -0.598,
            "X111": 0.363,
            "X112": -1.251,
            "X113": -0.554,
            "X114": 0.799,
            "X115": 0.109,
            "X116": 0.492,
            "X117": 0.445,
            "X118": 1.836,
            "X119": 2.204,
            "X120": -0.762,
            "X121": -0.278,
            "X122": 0.181,
            "X123": -1.514,
            "X124": -1.465,
            "X125": 0.533,
            "X126": 0.69,
            "X127": -0.816,
            "X128": -0.271,
            "X129": -0.726,
            "X130": 0.752,
            "X131": 0.301,
            "X132": -0.632,
            "X133": -0.266,
            "X134": 2.127,
            "X135": 0.127,
            "X136": -1.134,
            "X137": -1.112,
            "X138": -0.217,
            "X139": -0.792,
            "X140": -0.584,
            "X141": -0.305,
            "X142": -0.096,
            "X143": 0.897,
            "X144": -0.678,
            "X145": 1.442,
            "X146": 1.149,
            "X147": 0.865,
            "X148": -1.314,
            "X149": 0.606,
            "X150": -2.113,
            "X151": -0.619,
            "X152": -0.215,
            "X153": 0.903,
            "X154": -1.301,
            "X155": -0.946,
            "X156": -0.406,
            "X157": 1.255,
            "X158": 0.587,
            "X159": 0.934,
            "X160": 0.054,
            "X161": -0.969,
            "X162": 2.044,
            "X163": 0.828,
            "X164": 0.768,
            "X165": -2.092,
            "X166": 0.855,
            "X167": -1.579,
            "X168": 0.299,
            "X169": 1.052,
            "X170": -0.696,
            "X171": 0.259,
            "X172": 1.628,
            "X173": -1.137,
            "X174": -0.208,
            "X175": 2.92,
            "X176": 0.129,
            "X177": 0.962,
            "X178": 0.708,
            "X179": -0.334,
            "X180": -2.091,
            "X181": -0.26,
            "X182": 1.892,
            "X183": 0.26,
            "X184": -0.674,
            "X185": 0.22,
            "X186": 0.657,
            "X187": 1.54,
            "X188": -0.455,
            "X189": -0.4,
            "X190": -0.969,
            "X191": 0.745,
            "X192": -0.162,
            "X193": -1.674,
            "X194": 0.35,
            "X195": -1.175,
            "X196": -0.161,
            "X197": -1.631,
            "X198": -0.956,
            "X199": 0.892,
            "X200": -0.745,
            "X201": -0.826,
            "X202": 0.862,
            "X203": 0.291,
            "X204": -1.006,
            "X205": 0.621,
            "X206": -0.55,
            "X207": 0.274,
            "X208": 0.475,
            "X209": -2.919,
            "X210": -0.193,
            "X211": -0.09,
            "X212": -1.534,
            "X213": 0.033,
            "X214": -0.085,
            "X215": -0.586,
            "X216": 0.065,
            "X217": -1.728,
            "X218": 0.415,
            "X219": -0.292,
            "X220": 0.023,
            "X221": 0.212,
            "X222": -0.235,
            "X223": 0.142,
            "X224": -0.463,
            "X225": 0.413,
            "X226": -1.654,
            "X227": -1.082,
            "X228": 0.466,
            "X229": -0.476,
            "X230": 0.96,
            "X231": 0.765,
            "X232": 0.583,
            "X233": -0.644,
            "X234": -1.022,
            "X235": 1.218,
            "X236": -0.414,
            "X237": 2.043,
            "X238": 0.852,
            "X239": 1.424,
            "X240": 1.429,
            "X241": -1.05,
            "X242": 1.249,
            "X243": -0.76,
            "X244": -0.634,
            "X245": -2.876,
            "X246": 0.707,
            "X247": -0.885,
            "X248": 0.163,
            "X249": -0.263,
            "X250": 0.915,
            "X251": 1.283,
            "X252": 0.529,
            "X253": 0.753,
            "X254": -0.836,
            "X255": 1.114,
            "X256": -1.347,
            "X257": -0.038,
            "X258": -0.235,
            "X259": 1.069,
            "X260": -0.419,
            "X261": 0.501,
            "X262": 0.184,
            "X263": 1.311,
            "X264": 0.914,
            "X265": 1.336,
            "X266": 1.486,
            "X267": 0.314,
            "X268": -1.015,
            "X269": 0.402,
            "X270": 0.793,
            "X271": -0.833,
            "X272": -0.88,
            "X273": -0.02,
            "X274": 0.244,
            "X275": -0.344,
            "X276": 0.948,
            "X277": -1.69,
            "X278": 0.04,
            "X279": -0.326,
            "X280": 0.396,
            "X281": -0.723,
            "X282": 1.185,
            "X283": -0.396,
            "X284": 1.2,
            "X285": -0.728,
            "X286": -1.064,
            "X287": -0.506,
            "X288": -0.289,
            "X289": 3.113,
            "X290": 0.393,
            "X291": -0.767,
            "X292": -1.02,
            "X293": -0.178,
            "X294": 1.225,
            "X295": 1.567,
            "X296": 2.435,
            "X297": -1.519,
            "X298": 0.136,
            "X299": 0.414
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.9303,
            ".pred_1": 0.0697,
            "id": 201,
            "X0": 1.184,
            "X1": -0.58,
            "X2": 0.329,
            "X3": 0.321,
            "X4": -0.089,
            "X5": 1.173,
            "X6": 0.589,
            "X7": -0.098,
            "X8": 0.493,
            "X9": -0.586,
            "X10": 1.288,
            "X11": 0.122,
            "X12": 0.836,
            "X13": 0.428,
            "X14": -0.226,
            "X15": 0.26,
            "X16": 0.874,
            "X17": 0.003,
            "X18": -1.667,
            "X19": 0.617,
            "X20": 0.518,
            "X21": 0.001,
            "X22": 0.251,
            "X23": -0.19,
            "X24": -0.035,
            "X25": -0.334,
            "X26": -0.422,
            "X27": -2.008,
            "X28": -0.982,
            "X29": -0.649,
            "X30": -1.468,
            "X31": 1.111,
            "X32": 0.176,
            "X33": 0.255,
            "X34": 0.401,
            "X35": 0.915,
            "X36": -0.057,
            "X37": -0.62,
            "X38": 0.743,
            "X39": 1.229,
            "X40": 1.278,
            "X41": 1.264,
            "X42": 0.186,
            "X43": -1.186,
            "X44": 0.213,
            "X45": 1.973,
            "X46": -0.946,
            "X47": 0.009,
            "X48": -0.677,
            "X49": 1.772,
            "X50": 0.492,
            "X51": 0.064,
            "X52": -0.766,
            "X53": 1.563,
            "X54": 1.057,
            "X55": -1.31,
            "X56": 1.097,
            "X57": 1.265,
            "X58": -0.839,
            "X59": -1.188,
            "X60": 1.078,
            "X61": 0.127,
            "X62": 2.114,
            "X63": -0.053,
            "X64": -0.062,
            "X65": 0.448,
            "X66": -0.557,
            "X67": -1.921,
            "X68": 0.628,
            "X69": 0.481,
            "X70": -0.615,
            "X71": 0.094,
            "X72": -0.914,
            "X73": 0.801,
            "X74": 1.178,
            "X75": 0.504,
            "X76": -0.204,
            "X77": 0.827,
            "X78": 0.795,
            "X79": -0.55,
            "X80": -1.37,
            "X81": 0.417,
            "X82": 1.988,
            "X83": 1.484,
            "X84": 1.553,
            "X85": -1.101,
            "X86": 1.738,
            "X87": -0.169,
            "X88": -0.1,
            "X89": 0.006,
            "X90": -0.037,
            "X91": 0.126,
            "X92": 0.392,
            "X93": 0.509,
            "X94": 0.182,
            "X95": -1.962,
            "X96": 2.124,
            "X97": -0.762,
            "X98": 0.42,
            "X99": -0.613,
            "X100": -0.582,
            "X101": -1.851,
            "X102": 0.931,
            "X103": -0.504,
            "X104": -1.056,
            "X105": 1.61,
            "X106": -0.4,
            "X107": -0.341,
            "X108": -0.524,
            "X109": 0.396,
            "X110": -0.317,
            "X111": -1.888,
            "X112": 0.388,
            "X113": 0.527,
            "X114": 0.38,
            "X115": 2.439,
            "X116": 0.667,
            "X117": 0.105,
            "X118": -0.454,
            "X119": -0.302,
            "X120": 0.082,
            "X121": -0.646,
            "X122": -0.443,
            "X123": 1.197,
            "X124": 1.294,
            "X125": 0.515,
            "X126": 0.962,
            "X127": -0.923,
            "X128": -0.231,
            "X129": -0.438,
            "X130": 0.465,
            "X131": -1.113,
            "X132": 0.772,
            "X133": -1.073,
            "X134": 0.835,
            "X135": -0.728,
            "X136": 0.044,
            "X137": 0.219,
            "X138": 0.309,
            "X139": -0.407,
            "X140": 0.724,
            "X141": 0.174,
            "X142": -0.214,
            "X143": 0.932,
            "X144": -1.044,
            "X145": 1.097,
            "X146": -0.775,
            "X147": -0.771,
            "X148": -0.29,
            "X149": 0.717,
            "X150": -0.089,
            "X151": 1.324,
            "X152": 0.014,
            "X153": 0.68,
            "X154": -0.553,
            "X155": 0.581,
            "X156": -0.096,
            "X157": -1.215,
            "X158": 0.351,
            "X159": -0.243,
            "X160": 0.371,
            "X161": -0.603,
            "X162": -1.367,
            "X163": 1.389,
            "X164": -1.158,
            "X165": -0.235,
            "X166": 0.028,
            "X167": 0.003,
            "X168": 0.016,
            "X169": 0.281,
            "X170": 0.372,
            "X171": -1.112,
            "X172": -0.882,
            "X173": 0.04,
            "X174": -0.164,
            "X175": 0.464,
            "X176": 1.022,
            "X177": 1.039,
            "X178": -0.846,
            "X179": -0.385,
            "X180": -1.058,
            "X181": 0.959,
            "X182": 2.036,
            "X183": 0.723,
            "X184": 1.703,
            "X185": -0.225,
            "X186": 0.471,
            "X187": -0.61,
            "X188": 1.577,
            "X189": -0.675,
            "X190": -0.027,
            "X191": -1.285,
            "X192": -0.2,
            "X193": -0.981,
            "X194": 1.519,
            "X195": -1.026,
            "X196": 0.517,
            "X197": -0.636,
            "X198": -1.448,
            "X199": 0.072,
            "X200": -0.083,
            "X201": 0.223,
            "X202": 0.507,
            "X203": 1.705,
            "X204": 0.978,
            "X205": -0.652,
            "X206": -0.818,
            "X207": -1.113,
            "X208": -1.291,
            "X209": -0.272,
            "X210": -0.579,
            "X211": 1.097,
            "X212": -0.468,
            "X213": 1.219,
            "X214": -0.014,
            "X215": -0.736,
            "X216": 0.446,
            "X217": -1.772,
            "X218": -0.38,
            "X219": 0.471,
            "X220": 0.81,
            "X221": 0.699,
            "X222": 0.101,
            "X223": 1.816,
            "X224": -0.103,
            "X225": 0.034,
            "X226": -1.356,
            "X227": -0.18,
            "X228": 0.099,
            "X229": 0.464,
            "X230": 0.19,
            "X231": -0.779,
            "X232": 0.337,
            "X233": 0.674,
            "X234": 0.047,
            "X235": -0.357,
            "X236": -1.585,
            "X237": 0.067,
            "X238": 0.1,
            "X239": 0.207,
            "X240": 1.085,
            "X241": 0.452,
            "X242": 1.25,
            "X243": 0.22,
            "X244": -0.556,
            "X245": -1.145,
            "X246": 0.474,
            "X247": -0.524,
            "X248": -0.85,
            "X249": 0.131,
            "X250": -0.448,
            "X251": -0.161,
            "X252": 0.066,
            "X253": -0.934,
            "X254": -1.339,
            "X255": 0.508,
            "X256": 0.464,
            "X257": 0.223,
            "X258": -0.458,
            "X259": 0.829,
            "X260": -1.846,
            "X261": -2.144,
            "X262": -1.269,
            "X263": -0.497,
            "X264": 0.453,
            "X265": -1.085,
            "X266": -0.225,
            "X267": -1.575,
            "X268": 1.305,
            "X269": -0.4,
            "X270": -1.693,
            "X271": 0.985,
            "X272": 1.406,
            "X273": 1.634,
            "X274": 0.406,
            "X275": 0.819,
            "X276": -0.854,
            "X277": 0.722,
            "X278": -0.453,
            "X279": 0.835,
            "X280": -0.026,
            "X281": -1.377,
            "X282": -0.031,
            "X283": -1.119,
            "X284": -0.792,
            "X285": -0.538,
            "X286": 0.067,
            "X287": 0.758,
            "X288": 0.399,
            "X289": -0.258,
            "X290": 0.933,
            "X291": 1.514,
            "X292": -1.151,
            "X293": -0.583,
            "X294": -0.438,
            "X295": 0.148,
            "X296": -0.758,
            "X297": 0.038,
            "X298": -1.73,
            "X299": 0.712
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7012,
            ".pred_1": 0.2988,
            "id": 33,
            "X0": 0.803,
            "X1": -0.662,
            "X2": 0.949,
            "X3": -1.27,
            "X4": 0.654,
            "X5": -2.651,
            "X6": -0.184,
            "X7": 1.022,
            "X8": 0.455,
            "X9": 0.814,
            "X10": 1.146,
            "X11": -0.129,
            "X12": 0.902,
            "X13": -1.25,
            "X14": -0.881,
            "X15": 0.312,
            "X16": 1.844,
            "X17": 0.412,
            "X18": 0.89,
            "X19": -0.298,
            "X20": -1.165,
            "X21": 0.407,
            "X22": -0.459,
            "X23": 1.995,
            "X24": 0.714,
            "X25": -0.107,
            "X26": -2.005,
            "X27": -1.361,
            "X28": 0.492,
            "X29": -0.055,
            "X30": -1.187,
            "X31": 1.982,
            "X32": -1.947,
            "X33": -1.006,
            "X34": -0.903,
            "X35": 0.682,
            "X36": -0.117,
            "X37": 0.584,
            "X38": 0.315,
            "X39": -0.173,
            "X40": 1.271,
            "X41": -0.988,
            "X42": 0.949,
            "X43": -0.703,
            "X44": -0.864,
            "X45": -0.402,
            "X46": 0.688,
            "X47": -0.042,
            "X48": 1.91,
            "X49": 0.873,
            "X50": 0.141,
            "X51": 1.028,
            "X52": 0.329,
            "X53": -0.347,
            "X54": 0.289,
            "X55": -0.373,
            "X56": 2.244,
            "X57": 0.235,
            "X58": -0.835,
            "X59": -0.094,
            "X60": 0.738,
            "X61": -1.418,
            "X62": -0.192,
            "X63": 0.405,
            "X64": -0.44,
            "X65": 0.315,
            "X66": -0.845,
            "X67": -0.106,
            "X68": 0.932,
            "X69": -2.299,
            "X70": -2.854,
            "X71": 1.132,
            "X72": -0.034,
            "X73": 0.776,
            "X74": -2.177,
            "X75": -1.296,
            "X76": -1.534,
            "X77": 1.451,
            "X78": -0.607,
            "X79": 1.224,
            "X80": 0.603,
            "X81": 0.126,
            "X82": -0.408,
            "X83": -1.789,
            "X84": 1.132,
            "X85": -0.735,
            "X86": 1.088,
            "X87": -0.231,
            "X88": -0.214,
            "X89": 0.128,
            "X90": -0.646,
            "X91": -0.976,
            "X92": 0.24,
            "X93": 0.098,
            "X94": -1.629,
            "X95": -0.015,
            "X96": -1.673,
            "X97": -0.772,
            "X98": -0.559,
            "X99": -0.911,
            "X100": -0.677,
            "X101": -1.216,
            "X102": 0.178,
            "X103": 0.941,
            "X104": -1.066,
            "X105": 0.17,
            "X106": -1.515,
            "X107": -2.434,
            "X108": -0.699,
            "X109": 0.725,
            "X110": 1.86,
            "X111": -0.107,
            "X112": 0.486,
            "X113": -0.36,
            "X114": -0.63,
            "X115": 0.942,
            "X116": -0.37,
            "X117": 0.425,
            "X118": -0.989,
            "X119": 0.998,
            "X120": -0.459,
            "X121": -0.694,
            "X122": 0.621,
            "X123": -0.639,
            "X124": -0.294,
            "X125": 1.383,
            "X126": -1.183,
            "X127": -0.315,
            "X128": -0.273,
            "X129": 0.08,
            "X130": 1.095,
            "X131": -2.114,
            "X132": -0.442,
            "X133": -1.684,
            "X134": 0.18,
            "X135": 1.487,
            "X136": 0.417,
            "X137": -0.079,
            "X138": -0.293,
            "X139": 0.641,
            "X140": -0.866,
            "X141": 1.512,
            "X142": 0.669,
            "X143": 0.452,
            "X144": 1.357,
            "X145": -0.585,
            "X146": 0.075,
            "X147": -0.214,
            "X148": -1.116,
            "X149": 0.436,
            "X150": -1.79,
            "X151": 1.252,
            "X152": 1.993,
            "X153": -0.516,
            "X154": -1.079,
            "X155": -0.305,
            "X156": 0.29,
            "X157": -1.42,
            "X158": -0.708,
            "X159": 0.665,
            "X160": -0.575,
            "X161": 0.7,
            "X162": 0.816,
            "X163": -0.445,
            "X164": -2.229,
            "X165": 1.74,
            "X166": 0.754,
            "X167": 1.665,
            "X168": -0.21,
            "X169": -0.292,
            "X170": 0.078,
            "X171": -0.517,
            "X172": 0.004,
            "X173": -1.572,
            "X174": -0.034,
            "X175": 0.89,
            "X176": -0.079,
            "X177": -0.533,
            "X178": 0.849,
            "X179": -0.427,
            "X180": -1.291,
            "X181": 1.31,
            "X182": 0.137,
            "X183": -0.205,
            "X184": 1.744,
            "X185": -0.115,
            "X186": -1.014,
            "X187": 2.543,
            "X188": -0.769,
            "X189": 0.549,
            "X190": -0.061,
            "X191": -1.96,
            "X192": 0.487,
            "X193": -0.296,
            "X194": 0.686,
            "X195": -0.863,
            "X196": -1.579,
            "X197": 0.43,
            "X198": 1.752,
            "X199": -0.385,
            "X200": 0.716,
            "X201": 0.078,
            "X202": -0.908,
            "X203": -0.057,
            "X204": 1.042,
            "X205": 2.17,
            "X206": -1.423,
            "X207": -0.019,
            "X208": 0.155,
            "X209": -0.661,
            "X210": 1.111,
            "X211": -1.338,
            "X212": -0.332,
            "X213": -0.378,
            "X214": 0.321,
            "X215": -0.402,
            "X216": 1.205,
            "X217": 0.451,
            "X218": 1.709,
            "X219": -0.085,
            "X220": -0.929,
            "X221": 0.732,
            "X222": -1.01,
            "X223": 1.042,
            "X224": 0.868,
            "X225": -2.121,
            "X226": -0.86,
            "X227": -1.049,
            "X228": 0.711,
            "X229": -0.123,
            "X230": 0.544,
            "X231": 1.674,
            "X232": -1.174,
            "X233": 0.4,
            "X234": -0.773,
            "X235": 0.039,
            "X236": 0.506,
            "X237": 0.485,
            "X238": -2.537,
            "X239": 1.741,
            "X240": -0.956,
            "X241": 1.219,
            "X242": 1.227,
            "X243": -0.519,
            "X244": -0.303,
            "X245": -0.571,
            "X246": -0.308,
            "X247": -0.638,
            "X248": 1.162,
            "X249": 1.57,
            "X250": -0.507,
            "X251": 0.387,
            "X252": -2.12,
            "X253": -0.062,
            "X254": -0.485,
            "X255": 0.304,
            "X256": 0.554,
            "X257": 0.508,
            "X258": -0.838,
            "X259": 0.092,
            "X260": -0.006,
            "X261": 1.809,
            "X262": 0.506,
            "X263": -0.411,
            "X264": 0.412,
            "X265": -0.763,
            "X266": 0.037,
            "X267": -0.849,
            "X268": 1.251,
            "X269": -0.495,
            "X270": 0.071,
            "X271": 0.907,
            "X272": -2.247,
            "X273": -0.505,
            "X274": -0.1,
            "X275": -0.616,
            "X276": 0.302,
            "X277": 0.367,
            "X278": -0.419,
            "X279": -0.419,
            "X280": 0.838,
            "X281": -0.426,
            "X282": 0.62,
            "X283": -0.597,
            "X284": -0.775,
            "X285": 0.396,
            "X286": 0.344,
            "X287": 1.57,
            "X288": 0.808,
            "X289": -0.128,
            "X290": -1.448,
            "X291": 2.561,
            "X292": 0.988,
            "X293": -0.585,
            "X294": 2.344,
            "X295": -0.913,
            "X296": -1.4,
            "X297": 1.856,
            "X298": -0.623,
            "X299": 0.061
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7039,
            ".pred_1": 0.2961,
            "id": 141,
            "X0": -0.616,
            "X1": 0.035,
            "X2": -0.386,
            "X3": -0.14,
            "X4": -0.07,
            "X5": 0.378,
            "X6": 0.402,
            "X7": 0.921,
            "X8": -0.06,
            "X9": -1.399,
            "X10": -1.144,
            "X11": -1.767,
            "X12": 1.14,
            "X13": 2.861,
            "X14": -1.698,
            "X15": 0.07,
            "X16": -1.889,
            "X17": 0.909,
            "X18": -0.79,
            "X19": 0.553,
            "X20": 0.628,
            "X21": 2.359,
            "X22": 0.634,
            "X23": 0.482,
            "X24": 1.611,
            "X25": -0.271,
            "X26": -0.319,
            "X27": -1.218,
            "X28": 0.954,
            "X29": -0.65,
            "X30": 0.163,
            "X31": 0.523,
            "X32": -0.924,
            "X33": -0.332,
            "X34": -0.57,
            "X35": 0.953,
            "X36": -0.041,
            "X37": 0.694,
            "X38": -0.529,
            "X39": -0.96,
            "X40": 0.5,
            "X41": 1.754,
            "X42": -0.901,
            "X43": 1.038,
            "X44": -1.022,
            "X45": -0.755,
            "X46": 1.13,
            "X47": -0.14,
            "X48": -0.01,
            "X49": 0.143,
            "X50": 0.684,
            "X51": -0.596,
            "X52": -0.019,
            "X53": -1.112,
            "X54": -0.788,
            "X55": -1.201,
            "X56": 0.347,
            "X57": 0.2,
            "X58": -1.635,
            "X59": 0.142,
            "X60": -0.605,
            "X61": -0.448,
            "X62": -1.317,
            "X63": 1.83,
            "X64": 0.911,
            "X65": -0.08,
            "X66": 0.507,
            "X67": 0.694,
            "X68": 1.993,
            "X69": 1.019,
            "X70": -0.198,
            "X71": 0.253,
            "X72": 1.008,
            "X73": -0.048,
            "X74": -1.221,
            "X75": -0.283,
            "X76": -0.416,
            "X77": 0.544,
            "X78": 2.883,
            "X79": 0.574,
            "X80": -1.789,
            "X81": 0.551,
            "X82": -1.29,
            "X83": 2.008,
            "X84": -1.343,
            "X85": -0.335,
            "X86": -2.265,
            "X87": -0.177,
            "X88": -1.732,
            "X89": -0.075,
            "X90": 0.051,
            "X91": -2.077,
            "X92": 1.01,
            "X93": 1.152,
            "X94": -1.253,
            "X95": -0.52,
            "X96": -0.729,
            "X97": -1.12,
            "X98": -1.305,
            "X99": 0.411,
            "X100": -1.65,
            "X101": -1.159,
            "X102": 0.2,
            "X103": -0.61,
            "X104": -0.73,
            "X105": -0.018,
            "X106": -0.254,
            "X107": 1.075,
            "X108": 0.07,
            "X109": -0.789,
            "X110": -0.784,
            "X111": -1.231,
            "X112": -1.162,
            "X113": -0.618,
            "X114": 0.285,
            "X115": 0.427,
            "X116": 0.91,
            "X117": -0.618,
            "X118": -1.364,
            "X119": 0.11,
            "X120": 1.706,
            "X121": 0.671,
            "X122": -0.957,
            "X123": -0.781,
            "X124": -1.137,
            "X125": 1.422,
            "X126": 1.193,
            "X127": -0.758,
            "X128": 1.189,
            "X129": -1.028,
            "X130": 0.954,
            "X131": 0.317,
            "X132": -0.015,
            "X133": 0.047,
            "X134": 0.11,
            "X135": 0.239,
            "X136": -0.508,
            "X137": 0.389,
            "X138": 0.176,
            "X139": 1.998,
            "X140": 0.104,
            "X141": 2.721,
            "X142": -0.053,
            "X143": -0.856,
            "X144": 0.137,
            "X145": -0.97,
            "X146": 0.543,
            "X147": -0.431,
            "X148": -0.496,
            "X149": 2.037,
            "X150": -0.263,
            "X151": 0.207,
            "X152": -0.892,
            "X153": -0.442,
            "X154": 1.774,
            "X155": -0.678,
            "X156": -2.44,
            "X157": -0.689,
            "X158": -0.884,
            "X159": 1.923,
            "X160": 0.047,
            "X161": 2.01,
            "X162": -0.2,
            "X163": -1.269,
            "X164": 0.745,
            "X165": -1.252,
            "X166": -0.804,
            "X167": -0.641,
            "X168": -1.456,
            "X169": 0.528,
            "X170": -0.045,
            "X171": 0.76,
            "X172": 0.224,
            "X173": 1.278,
            "X174": -0.313,
            "X175": -1.256,
            "X176": 0.489,
            "X177": -0.556,
            "X178": -0.097,
            "X179": 1.811,
            "X180": 1.022,
            "X181": 0.618,
            "X182": -0.426,
            "X183": 0.489,
            "X184": -0.03,
            "X185": 1.688,
            "X186": -0.42,
            "X187": 0.635,
            "X188": -0.309,
            "X189": 0.161,
            "X190": -0.544,
            "X191": 1.097,
            "X192": 0.855,
            "X193": -0.766,
            "X194": 2.72,
            "X195": -0.42,
            "X196": -0.463,
            "X197": -0.494,
            "X198": -0.312,
            "X199": 0.75,
            "X200": 0.438,
            "X201": -0.763,
            "X202": 1.689,
            "X203": 1.893,
            "X204": 0.673,
            "X205": 1.166,
            "X206": -0.207,
            "X207": -0.317,
            "X208": 0.191,
            "X209": -0.275,
            "X210": -0.732,
            "X211": -1.949,
            "X212": 0.858,
            "X213": -0.477,
            "X214": 0.725,
            "X215": 0.192,
            "X216": -1.198,
            "X217": 0.358,
            "X218": 0.129,
            "X219": -0.189,
            "X220": 0.486,
            "X221": -0.93,
            "X222": 1.599,
            "X223": -2.361,
            "X224": -0.216,
            "X225": 0.403,
            "X226": 1.087,
            "X227": -1.597,
            "X228": -0.379,
            "X229": 0.929,
            "X230": -0.865,
            "X231": -1.064,
            "X232": 0.987,
            "X233": -0.761,
            "X234": 0.012,
            "X235": 0.551,
            "X236": 1.706,
            "X237": 0.161,
            "X238": 1.64,
            "X239": -1.176,
            "X240": 0.849,
            "X241": -0.059,
            "X242": 0.178,
            "X243": -0.155,
            "X244": -0.572,
            "X245": 0.16,
            "X246": 0.451,
            "X247": 0.511,
            "X248": -2.118,
            "X249": 0.268,
            "X250": 0.381,
            "X251": -0.338,
            "X252": -1.217,
            "X253": -0.161,
            "X254": 0.068,
            "X255": 1.829,
            "X256": 0.431,
            "X257": 1.208,
            "X258": -0.987,
            "X259": -0.467,
            "X260": -1.523,
            "X261": 1.207,
            "X262": 1.646,
            "X263": 1.645,
            "X264": -0.753,
            "X265": -0.287,
            "X266": -0.082,
            "X267": 0.436,
            "X268": -0.234,
            "X269": 0.823,
            "X270": -0.388,
            "X271": -0.694,
            "X272": 0.024,
            "X273": 0.038,
            "X274": -0.901,
            "X275": -0.416,
            "X276": -1.245,
            "X277": 1.265,
            "X278": -0.563,
            "X279": -1.066,
            "X280": -0.803,
            "X281": -1.798,
            "X282": -0.361,
            "X283": 2.025,
            "X284": -0.772,
            "X285": -2.534,
            "X286": 0.405,
            "X287": 2.351,
            "X288": -1.793,
            "X289": 0.609,
            "X290": 0.923,
            "X291": 0.239,
            "X292": 0.234,
            "X293": 1.318,
            "X294": -0.101,
            "X295": 0.68,
            "X296": 1.545,
            "X297": 0.868,
            "X298": 0.039,
            "X299": 0.14
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6109,
            ".pred_1": 0.3891,
            "id": 219,
            "X0": -1.336,
            "X1": -0.033,
            "X2": 0.139,
            "X3": -0.366,
            "X4": -0.888,
            "X5": -0.179,
            "X6": 0.839,
            "X7": -0.768,
            "X8": -0.425,
            "X9": 1.058,
            "X10": -1.781,
            "X11": -2.168,
            "X12": -0.149,
            "X13": 0.33,
            "X14": 1.78,
            "X15": -0.827,
            "X16": -0.737,
            "X17": 0.468,
            "X18": 0.283,
            "X19": -0.546,
            "X20": -0.301,
            "X21": 0.592,
            "X22": -0.84,
            "X23": 0.441,
            "X24": -0.834,
            "X25": 0.651,
            "X26": -0.523,
            "X27": -1.036,
            "X28": -1.662,
            "X29": 1.044,
            "X30": -1.319,
            "X31": 0.244,
            "X32": -0.509,
            "X33": 0.029,
            "X34": 0.453,
            "X35": 1.009,
            "X36": 0.535,
            "X37": 0.896,
            "X38": -0.081,
            "X39": 0.787,
            "X40": -2.51,
            "X41": 0.703,
            "X42": -0.789,
            "X43": -0.421,
            "X44": -0.388,
            "X45": 0.366,
            "X46": -1.23,
            "X47": -0.119,
            "X48": 0.843,
            "X49": 0.482,
            "X50": 0.961,
            "X51": 1.043,
            "X52": 0.181,
            "X53": -0.137,
            "X54": 0.638,
            "X55": 1.611,
            "X56": 0.16,
            "X57": 0.501,
            "X58": -0.501,
            "X59": 0.559,
            "X60": 0.333,
            "X61": 0.585,
            "X62": 0.829,
            "X63": 0.514,
            "X64": -0.914,
            "X65": -0.41,
            "X66": -2.088,
            "X67": -1.389,
            "X68": 0.033,
            "X69": -1.143,
            "X70": -1.047,
            "X71": 1.08,
            "X72": 0.755,
            "X73": 0.844,
            "X74": -0.139,
            "X75": -0.667,
            "X76": -1.002,
            "X77": -0.513,
            "X78": -0.33,
            "X79": -0.485,
            "X80": 0.003,
            "X81": -0.473,
            "X82": -0.549,
            "X83": -1.099,
            "X84": -0.955,
            "X85": 0.383,
            "X86": -0.623,
            "X87": -0.793,
            "X88": -1.554,
            "X89": -0.327,
            "X90": -0.713,
            "X91": 0.837,
            "X92": -0.744,
            "X93": 0.084,
            "X94": -0.546,
            "X95": -0.253,
            "X96": 1.117,
            "X97": 0.828,
            "X98": 0.246,
            "X99": -0.372,
            "X100": -0.281,
            "X101": 1.447,
            "X102": 1.22,
            "X103": -0.007,
            "X104": 1.83,
            "X105": 1.488,
            "X106": -0.783,
            "X107": 1.919,
            "X108": 0.975,
            "X109": -1.707,
            "X110": 0.693,
            "X111": -0.616,
            "X112": 0.422,
            "X113": 1.277,
            "X114": 0.561,
            "X115": -0.566,
            "X116": 0.377,
            "X117": 0.905,
            "X118": -0.572,
            "X119": -0.017,
            "X120": 2.094,
            "X121": 0.355,
            "X122": -0.512,
            "X123": -0.097,
            "X124": 0.588,
            "X125": 0.853,
            "X126": 0.568,
            "X127": -0.253,
            "X128": -1.197,
            "X129": -0.532,
            "X130": -0.059,
            "X131": 0.886,
            "X132": 0.611,
            "X133": -0.499,
            "X134": 0.864,
            "X135": -0.382,
            "X136": 0.46,
            "X137": -0.428,
            "X138": -0.584,
            "X139": -0.718,
            "X140": 1.531,
            "X141": -1.039,
            "X142": 1.914,
            "X143": -0.661,
            "X144": -0.307,
            "X145": 0.37,
            "X146": -0.361,
            "X147": 0.685,
            "X148": -0.424,
            "X149": -0.937,
            "X150": -0.346,
            "X151": -0.501,
            "X152": -1.247,
            "X153": 0.745,
            "X154": -0.314,
            "X155": -0.682,
            "X156": -1.069,
            "X157": 0.691,
            "X158": 0.333,
            "X159": -1.597,
            "X160": 1.566,
            "X161": -0.981,
            "X162": -0.563,
            "X163": 0.052,
            "X164": -0.639,
            "X165": -0.542,
            "X166": -1.774,
            "X167": 0.428,
            "X168": 0.504,
            "X169": 1.208,
            "X170": 1.119,
            "X171": -1.328,
            "X172": 1.204,
            "X173": 0.122,
            "X174": -1.303,
            "X175": 0.428,
            "X176": -0.368,
            "X177": 0.057,
            "X178": -0.327,
            "X179": -1.202,
            "X180": -0.867,
            "X181": -1.624,
            "X182": 2.155,
            "X183": 0.134,
            "X184": 0.612,
            "X185": 0.843,
            "X186": 0.194,
            "X187": -1.147,
            "X188": -2.021,
            "X189": -0.224,
            "X190": 0.596,
            "X191": 0.275,
            "X192": -1.892,
            "X193": 1.251,
            "X194": 2.293,
            "X195": 0.621,
            "X196": 0.277,
            "X197": 0.171,
            "X198": 1.028,
            "X199": 0.008,
            "X200": -0.617,
            "X201": -1.662,
            "X202": -1.576,
            "X203": 1.213,
            "X204": -0.333,
            "X205": 0.297,
            "X206": 0.312,
            "X207": -0.308,
            "X208": 0.494,
            "X209": 0.396,
            "X210": 0.611,
            "X211": 0.146,
            "X212": -1.394,
            "X213": -0.386,
            "X214": 0.838,
            "X215": -0.679,
            "X216": 0.085,
            "X217": 0.004,
            "X218": -0.471,
            "X219": -0.071,
            "X220": -0.912,
            "X221": 1.162,
            "X222": -0.689,
            "X223": -0.188,
            "X224": 1.303,
            "X225": -0.212,
            "X226": 1.746,
            "X227": -0.325,
            "X228": -0.541,
            "X229": -0.417,
            "X230": -0.844,
            "X231": 2.038,
            "X232": -2.729,
            "X233": -0.235,
            "X234": 1.782,
            "X235": 0.674,
            "X236": 2.113,
            "X237": 0.046,
            "X238": 1.098,
            "X239": -0.452,
            "X240": 1.668,
            "X241": 0.257,
            "X242": 0.237,
            "X243": 0.048,
            "X244": 1.01,
            "X245": 0.12,
            "X246": 1.379,
            "X247": -0.78,
            "X248": -0.051,
            "X249": 0.7,
            "X250": 0.806,
            "X251": -0.767,
            "X252": 0.937,
            "X253": -0.488,
            "X254": 0.983,
            "X255": 1.139,
            "X256": -0.131,
            "X257": 0.209,
            "X258": 1.271,
            "X259": 1.083,
            "X260": -0.981,
            "X261": 0.645,
            "X262": -0.212,
            "X263": 0.232,
            "X264": 0.408,
            "X265": -0.721,
            "X266": -0.729,
            "X267": 0.878,
            "X268": 0.592,
            "X269": -1.164,
            "X270": -1.887,
            "X271": 1.05,
            "X272": -1.408,
            "X273": 1.474,
            "X274": 0.172,
            "X275": 0.325,
            "X276": -0.565,
            "X277": 2.379,
            "X278": -0.2,
            "X279": -0.49,
            "X280": 0.415,
            "X281": 1.28,
            "X282": 0.741,
            "X283": 0.954,
            "X284": 0.82,
            "X285": 0.597,
            "X286": -2.422,
            "X287": -0.142,
            "X288": -1.379,
            "X289": 1.632,
            "X290": -2.133,
            "X291": 1.302,
            "X292": -1.778,
            "X293": -0.647,
            "X294": 1.11,
            "X295": 0.811,
            "X296": -0.823,
            "X297": 1.009,
            "X298": -1.127,
            "X299": -0.819
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7287,
            ".pred_1": 0.2713,
            "id": 25,
            "X0": -0.617,
            "X1": -0.282,
            "X2": 0.693,
            "X3": 0.179,
            "X4": 0.791,
            "X5": -0.19,
            "X6": 0.685,
            "X7": 0.472,
            "X8": 0.678,
            "X9": -0.876,
            "X10": -0.719,
            "X11": 0.632,
            "X12": 0.79,
            "X13": 0.206,
            "X14": 1.307,
            "X15": 0.747,
            "X16": 0.562,
            "X17": 0.488,
            "X18": -0.796,
            "X19": -0.146,
            "X20": -1.172,
            "X21": -0.018,
            "X22": -0.397,
            "X23": -1.029,
            "X24": 0.187,
            "X25": -0.628,
            "X26": -0.021,
            "X27": -1.408,
            "X28": 0.551,
            "X29": 1.123,
            "X30": -0.44,
            "X31": 1.164,
            "X32": -0.935,
            "X33": 0.943,
            "X34": 0.197,
            "X35": 0.105,
            "X36": 1.778,
            "X37": 0.151,
            "X38": -0.251,
            "X39": 0.253,
            "X40": -0.924,
            "X41": -0.805,
            "X42": 2.143,
            "X43": 0.421,
            "X44": -0.413,
            "X45": -1.072,
            "X46": 0.057,
            "X47": 0.672,
            "X48": 0.153,
            "X49": -1.153,
            "X50": -1.546,
            "X51": -0.464,
            "X52": 0.352,
            "X53": -0.091,
            "X54": 0.981,
            "X55": 0.14,
            "X56": -0.775,
            "X57": -1.675,
            "X58": -0.449,
            "X59": 0.59,
            "X60": 1.013,
            "X61": -2.035,
            "X62": -1.012,
            "X63": 0.785,
            "X64": -0.964,
            "X65": 1.122,
            "X66": 0.59,
            "X67": 1.296,
            "X68": 0.477,
            "X69": 0.205,
            "X70": -0.016,
            "X71": -1.052,
            "X72": 0.418,
            "X73": 0.445,
            "X74": 0.598,
            "X75": -0.58,
            "X76": -1.011,
            "X77": 0.734,
            "X78": -1.353,
            "X79": 1.841,
            "X80": -1.346,
            "X81": -0.167,
            "X82": 0.364,
            "X83": -0.211,
            "X84": -0.504,
            "X85": -1.313,
            "X86": 0.639,
            "X87": 0.616,
            "X88": 0.3,
            "X89": 0.069,
            "X90": -0.702,
            "X91": -0.009,
            "X92": 1.386,
            "X93": 0.81,
            "X94": 0.468,
            "X95": -1,
            "X96": -0.805,
            "X97": -0.06,
            "X98": -0.365,
            "X99": 0.933,
            "X100": 0.711,
            "X101": -1.329,
            "X102": -1.327,
            "X103": -0.419,
            "X104": 1.445,
            "X105": 0.064,
            "X106": 1.788,
            "X107": 0.356,
            "X108": -2.656,
            "X109": 0.833,
            "X110": 0.345,
            "X111": -0.237,
            "X112": 0.03,
            "X113": -1.484,
            "X114": -2.302,
            "X115": 0.166,
            "X116": -0.644,
            "X117": -0.148,
            "X118": -1.044,
            "X119": -0.804,
            "X120": -0.834,
            "X121": -0.005,
            "X122": -0.045,
            "X123": 0.616,
            "X124": -1,
            "X125": -0.426,
            "X126": -0.016,
            "X127": 0.258,
            "X128": 1.162,
            "X129": -0.092,
            "X130": -1.377,
            "X131": -1.225,
            "X132": 0.308,
            "X133": 0.851,
            "X134": -0.843,
            "X135": -0.823,
            "X136": -1.106,
            "X137": 0.859,
            "X138": 1.059,
            "X139": -2.195,
            "X140": 1.981,
            "X141": -0.696,
            "X142": -1.208,
            "X143": 1.245,
            "X144": -1.381,
            "X145": -0.865,
            "X146": -0.201,
            "X147": 0.661,
            "X148": 0.276,
            "X149": 1.377,
            "X150": 1.489,
            "X151": 0.184,
            "X152": -0.072,
            "X153": -0.896,
            "X154": -0.56,
            "X155": -0.511,
            "X156": -1.242,
            "X157": 0.64,
            "X158": -0.36,
            "X159": -0.923,
            "X160": 0.097,
            "X161": 1.293,
            "X162": -0.695,
            "X163": -1.685,
            "X164": -0.137,
            "X165": 0.375,
            "X166": 0.531,
            "X167": 1.864,
            "X168": 0.134,
            "X169": -0.598,
            "X170": -0.456,
            "X171": -1.15,
            "X172": 0.717,
            "X173": 0.128,
            "X174": -0.362,
            "X175": -0.116,
            "X176": -0.583,
            "X177": 0.739,
            "X178": 0.086,
            "X179": -0.063,
            "X180": -0.592,
            "X181": -0.805,
            "X182": -1.107,
            "X183": 0.835,
            "X184": -1.418,
            "X185": 0.169,
            "X186": 0.082,
            "X187": 1.029,
            "X188": 0.207,
            "X189": 0.866,
            "X190": 0.699,
            "X191": 0.598,
            "X192": 1.418,
            "X193": -1.687,
            "X194": 0.766,
            "X195": -0.847,
            "X196": 1.526,
            "X197": 0.067,
            "X198": -0.761,
            "X199": -1.131,
            "X200": 0.695,
            "X201": -1.355,
            "X202": 0.386,
            "X203": 0.14,
            "X204": 0.89,
            "X205": -0.21,
            "X206": 1.288,
            "X207": -0.038,
            "X208": -0.084,
            "X209": 0.07,
            "X210": -1.34,
            "X211": -0.298,
            "X212": -0.204,
            "X213": -0.974,
            "X214": -0.056,
            "X215": -1.579,
            "X216": -0.325,
            "X217": -0.033,
            "X218": -1.393,
            "X219": 0.572,
            "X220": -0.167,
            "X221": -0.08,
            "X222": 0.109,
            "X223": 0.574,
            "X224": 0.013,
            "X225": -0.314,
            "X226": 0.103,
            "X227": -0.555,
            "X228": -0.467,
            "X229": -0.153,
            "X230": 1.503,
            "X231": -0.817,
            "X232": -0.945,
            "X233": -0.491,
            "X234": -0.471,
            "X235": -0.732,
            "X236": 0.29,
            "X237": -0.06,
            "X238": 1.224,
            "X239": -0.138,
            "X240": 0.564,
            "X241": 0.409,
            "X242": -0.465,
            "X243": 0.278,
            "X244": 1.361,
            "X245": -2.792,
            "X246": -0.187,
            "X247": -0.665,
            "X248": 1.245,
            "X249": 0.987,
            "X250": -0.717,
            "X251": -0.289,
            "X252": -0.503,
            "X253": -0.493,
            "X254": 0.529,
            "X255": -1.694,
            "X256": -1.313,
            "X257": -0.7,
            "X258": -1.382,
            "X259": 0.294,
            "X260": -0.295,
            "X261": -0.828,
            "X262": -0.572,
            "X263": -1.32,
            "X264": 0.589,
            "X265": -0.62,
            "X266": -0.145,
            "X267": 1.023,
            "X268": -0.839,
            "X269": -0.097,
            "X270": -1.737,
            "X271": 0.594,
            "X272": -0.441,
            "X273": -0.658,
            "X274": 0.201,
            "X275": -1.255,
            "X276": -1.176,
            "X277": 0.425,
            "X278": 0.098,
            "X279": 0.575,
            "X280": 0.463,
            "X281": -1.679,
            "X282": -0.064,
            "X283": 1.155,
            "X284": 1.091,
            "X285": -1.497,
            "X286": -0.058,
            "X287": -0.183,
            "X288": 1.264,
            "X289": -0.475,
            "X290": -0.249,
            "X291": 0.189,
            "X292": 0.072,
            "X293": -0.414,
            "X294": 1.041,
            "X295": -1.362,
            "X296": 1.29,
            "X297": -1.482,
            "X298": -0.778,
            "X299": 1.751
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6843,
            ".pred_1": 0.3157,
            "id": 111,
            "X0": -0.403,
            "X1": 0.694,
            "X2": 2.186,
            "X3": -0.083,
            "X4": -0.679,
            "X5": -0.092,
            "X6": 0.984,
            "X7": 0.045,
            "X8": -1.185,
            "X9": -0.306,
            "X10": 2.957,
            "X11": -0.322,
            "X12": -0.28,
            "X13": 1.026,
            "X14": 1.4,
            "X15": -2.604,
            "X16": -0.872,
            "X17": 0.006,
            "X18": 0.375,
            "X19": 1.36,
            "X20": 0.233,
            "X21": 0.092,
            "X22": 0.33,
            "X23": -0.371,
            "X24": -0.842,
            "X25": -0.652,
            "X26": 1.339,
            "X27": -0.744,
            "X28": 1.016,
            "X29": -0.615,
            "X30": -2.803,
            "X31": 2.021,
            "X32": 1.072,
            "X33": -0.604,
            "X34": 0.693,
            "X35": -0.402,
            "X36": 0.412,
            "X37": -0.912,
            "X38": -1.079,
            "X39": -1.982,
            "X40": 0.621,
            "X41": -0.308,
            "X42": -0.204,
            "X43": -0.04,
            "X44": 1.064,
            "X45": 0.545,
            "X46": -0.685,
            "X47": -0.594,
            "X48": 0.067,
            "X49": 0.887,
            "X50": 1.022,
            "X51": -0.588,
            "X52": 0.894,
            "X53": -0.12,
            "X54": 0.933,
            "X55": -0.994,
            "X56": -0.21,
            "X57": -0.39,
            "X58": 0.722,
            "X59": 0.59,
            "X60": -1.905,
            "X61": -1.29,
            "X62": 0.923,
            "X63": -0.25,
            "X64": -1.228,
            "X65": 1.707,
            "X66": 0.317,
            "X67": -0.328,
            "X68": -0.221,
            "X69": 0.476,
            "X70": 0.189,
            "X71": 0.831,
            "X72": -0.615,
            "X73": -0.709,
            "X74": -1.296,
            "X75": -0.06,
            "X76": 0.623,
            "X77": 0.526,
            "X78": -1.105,
            "X79": 0.56,
            "X80": 0.495,
            "X81": -0.951,
            "X82": 0.783,
            "X83": -0.522,
            "X84": 1.212,
            "X85": 0.484,
            "X86": 0.655,
            "X87": 0.216,
            "X88": 0.839,
            "X89": 2.363,
            "X90": 1.186,
            "X91": -1.575,
            "X92": -0.665,
            "X93": 0.48,
            "X94": -0.973,
            "X95": 0.061,
            "X96": -0.748,
            "X97": -0.772,
            "X98": 0.178,
            "X99": 1.89,
            "X100": -0.278,
            "X101": 0.612,
            "X102": 0.138,
            "X103": -1.187,
            "X104": -0.345,
            "X105": 0.921,
            "X106": 0.353,
            "X107": -1.711,
            "X108": -2.117,
            "X109": -0.275,
            "X110": 1.22,
            "X111": 1.003,
            "X112": 1.325,
            "X113": 0.46,
            "X114": 0.359,
            "X115": -0.456,
            "X116": 0.099,
            "X117": 0.303,
            "X118": -1.133,
            "X119": -2.415,
            "X120": 0,
            "X121": 1.212,
            "X122": -0.054,
            "X123": -1.11,
            "X124": -0.435,
            "X125": 0.713,
            "X126": 0.949,
            "X127": 0.27,
            "X128": -1.752,
            "X129": -0.268,
            "X130": -1.179,
            "X131": -0.059,
            "X132": 0.452,
            "X133": 0.519,
            "X134": -0.486,
            "X135": -0.382,
            "X136": -0.75,
            "X137": -0.024,
            "X138": -0.557,
            "X139": 0.432,
            "X140": 1.04,
            "X141": 0.214,
            "X142": -0.502,
            "X143": 0.476,
            "X144": 0.585,
            "X145": -0.916,
            "X146": -0.998,
            "X147": 0.484,
            "X148": -0.794,
            "X149": 0.222,
            "X150": -1.763,
            "X151": 3.132,
            "X152": -0.07,
            "X153": -1.251,
            "X154": 0.278,
            "X155": 0.374,
            "X156": -0.724,
            "X157": -0.163,
            "X158": 0.719,
            "X159": -1.32,
            "X160": 0.514,
            "X161": 1.006,
            "X162": 0.654,
            "X163": -0.987,
            "X164": -1.664,
            "X165": -1.237,
            "X166": -0.31,
            "X167": 0.989,
            "X168": 1.887,
            "X169": -0.834,
            "X170": 0.805,
            "X171": 0.041,
            "X172": 0.124,
            "X173": -0.28,
            "X174": 0.552,
            "X175": 0.183,
            "X176": 0.451,
            "X177": 0.673,
            "X178": -0.821,
            "X179": -0.111,
            "X180": -1.676,
            "X181": -1.249,
            "X182": -0.745,
            "X183": -2.132,
            "X184": -0.122,
            "X185": -0.122,
            "X186": -2.322,
            "X187": -1.696,
            "X188": 0.829,
            "X189": 1.572,
            "X190": -1.24,
            "X191": 1.757,
            "X192": -0.435,
            "X193": 0.396,
            "X194": -0.287,
            "X195": -0.761,
            "X196": -0.418,
            "X197": 0.657,
            "X198": -0.65,
            "X199": -2.469,
            "X200": 0.954,
            "X201": -0.044,
            "X202": 0.965,
            "X203": 0.563,
            "X204": 0.336,
            "X205": -1.579,
            "X206": -0.853,
            "X207": 1.747,
            "X208": -0.827,
            "X209": -1.331,
            "X210": 0.995,
            "X211": 1.198,
            "X212": -0.028,
            "X213": 0.247,
            "X214": 1.978,
            "X215": -0.854,
            "X216": -0.145,
            "X217": 1.967,
            "X218": -1.611,
            "X219": -1.504,
            "X220": -0.63,
            "X221": 0.813,
            "X222": 0.682,
            "X223": 0.164,
            "X224": 1.049,
            "X225": -1.405,
            "X226": -0.029,
            "X227": -0.981,
            "X228": 0.608,
            "X229": 1.592,
            "X230": -2.001,
            "X231": -0.636,
            "X232": -0.978,
            "X233": -2.03,
            "X234": 1.104,
            "X235": -0.981,
            "X236": 1.832,
            "X237": 2.009,
            "X238": -0.735,
            "X239": -0.279,
            "X240": 0.468,
            "X241": -0.546,
            "X242": 0.67,
            "X243": 1.177,
            "X244": -0.316,
            "X245": -1.803,
            "X246": -0.7,
            "X247": -0.132,
            "X248": -0.469,
            "X249": -0.03,
            "X250": -0.096,
            "X251": -0.071,
            "X252": -1.45,
            "X253": 1.993,
            "X254": -1.032,
            "X255": 0.269,
            "X256": -0.047,
            "X257": -0.357,
            "X258": 0.095,
            "X259": 0.714,
            "X260": -0.104,
            "X261": -1.206,
            "X262": 0.211,
            "X263": 0.291,
            "X264": 0.214,
            "X265": 0.881,
            "X266": 0.169,
            "X267": 1.231,
            "X268": -1.479,
            "X269": -0.974,
            "X270": 1.055,
            "X271": 0.266,
            "X272": 1.63,
            "X273": -0.439,
            "X274": 0.367,
            "X275": 0.425,
            "X276": -0.969,
            "X277": 2.193,
            "X278": -0.383,
            "X279": -0.265,
            "X280": 0.37,
            "X281": 0.548,
            "X282": 1.816,
            "X283": -0.672,
            "X284": 0.101,
            "X285": -1.535,
            "X286": -0.348,
            "X287": 0.183,
            "X288": 0.393,
            "X289": -1.153,
            "X290": 1.806,
            "X291": 0.839,
            "X292": 0.875,
            "X293": -1.064,
            "X294": 0.288,
            "X295": -0.076,
            "X296": 0.212,
            "X297": -0.159,
            "X298": 0.05,
            "X299": -0.122
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8473,
            ".pred_1": 0.1527,
            "id": 18,
            "X0": -0.106,
            "X1": 0.325,
            "X2": 1.442,
            "X3": -0.623,
            "X4": 1.933,
            "X5": -1.864,
            "X6": 0.78,
            "X7": -0.269,
            "X8": -0.093,
            "X9": 1.199,
            "X10": 0.849,
            "X11": 0.143,
            "X12": -0.521,
            "X13": -0.485,
            "X14": 0.871,
            "X15": -0.423,
            "X16": -0.984,
            "X17": -1.179,
            "X18": 1.384,
            "X19": -2.169,
            "X20": 0.727,
            "X21": 0.387,
            "X22": -1.522,
            "X23": 2.447,
            "X24": 1.36,
            "X25": 0.769,
            "X26": -0.688,
            "X27": -0.333,
            "X28": 0.56,
            "X29": -0.325,
            "X30": 1.533,
            "X31": 1.086,
            "X32": 0.401,
            "X33": 0.252,
            "X34": 0.931,
            "X35": -0.032,
            "X36": -1.546,
            "X37": -1.412,
            "X38": 0.061,
            "X39": -0.849,
            "X40": 0.372,
            "X41": -0.556,
            "X42": 1.35,
            "X43": -0.43,
            "X44": -1.016,
            "X45": -0.906,
            "X46": -0.149,
            "X47": 0.319,
            "X48": 0.264,
            "X49": -0.309,
            "X50": -0.91,
            "X51": -0.305,
            "X52": -0.822,
            "X53": 0.731,
            "X54": -0.263,
            "X55": -0.131,
            "X56": -0.048,
            "X57": 0.205,
            "X58": -1.969,
            "X59": 1.4,
            "X60": 0.076,
            "X61": -0.94,
            "X62": 0.672,
            "X63": 2.033,
            "X64": 1.594,
            "X65": 2.327,
            "X66": -2.938,
            "X67": -1.922,
            "X68": -1.596,
            "X69": 1.431,
            "X70": 1.362,
            "X71": -2.614,
            "X72": 0.417,
            "X73": 0.536,
            "X74": 0.057,
            "X75": -0.049,
            "X76": -0.039,
            "X77": 0.203,
            "X78": -1.165,
            "X79": -0.773,
            "X80": 0.727,
            "X81": -0.688,
            "X82": -0.961,
            "X83": -0.601,
            "X84": 0.681,
            "X85": 0.631,
            "X86": -0.688,
            "X87": 0.424,
            "X88": 0.615,
            "X89": 1.157,
            "X90": 0.7,
            "X91": -1.186,
            "X92": 0.674,
            "X93": -2.592,
            "X94": 0.713,
            "X95": -0.235,
            "X96": -0.104,
            "X97": -0.128,
            "X98": -0.357,
            "X99": 1.191,
            "X100": 0.936,
            "X101": -0.446,
            "X102": -0.648,
            "X103": -1.122,
            "X104": -0.165,
            "X105": -0.398,
            "X106": 1.098,
            "X107": 0.821,
            "X108": 0.038,
            "X109": 1.249,
            "X110": 0.783,
            "X111": 0.679,
            "X112": 0.772,
            "X113": -0.819,
            "X114": -0.265,
            "X115": -1.439,
            "X116": -0.81,
            "X117": 0.395,
            "X118": -1.104,
            "X119": 1.443,
            "X120": 0.705,
            "X121": 0.048,
            "X122": 1.416,
            "X123": -2.149,
            "X124": -1.656,
            "X125": 0.584,
            "X126": 2.303,
            "X127": -0.589,
            "X128": 0.527,
            "X129": -0.144,
            "X130": -0.57,
            "X131": -0.236,
            "X132": 0.337,
            "X133": 0.86,
            "X134": -1.859,
            "X135": -0.481,
            "X136": 1.711,
            "X137": -1.435,
            "X138": -0.812,
            "X139": 0.685,
            "X140": -1.007,
            "X141": 0.747,
            "X142": 1.097,
            "X143": -0.398,
            "X144": -1.436,
            "X145": -1.077,
            "X146": 0.31,
            "X147": 1.324,
            "X148": 0.443,
            "X149": -0.862,
            "X150": -0.558,
            "X151": 0.066,
            "X152": -0.937,
            "X153": 0.146,
            "X154": -1.179,
            "X155": 1.918,
            "X156": 0.318,
            "X157": -0.633,
            "X158": -0.404,
            "X159": -1.612,
            "X160": 1.595,
            "X161": 0.038,
            "X162": -0.531,
            "X163": 1.591,
            "X164": 0.659,
            "X165": -0.551,
            "X166": -0.868,
            "X167": 0.144,
            "X168": -1.908,
            "X169": -1.891,
            "X170": -0.845,
            "X171": 0.957,
            "X172": -0.84,
            "X173": 1.76,
            "X174": -0.354,
            "X175": 1.746,
            "X176": 0.812,
            "X177": 0.381,
            "X178": -0.879,
            "X179": 0.549,
            "X180": 0.181,
            "X181": -0.276,
            "X182": 1.529,
            "X183": 0.349,
            "X184": -0.449,
            "X185": 0.224,
            "X186": 0.954,
            "X187": -0.125,
            "X188": -1.653,
            "X189": -1.902,
            "X190": -1.399,
            "X191": -0.022,
            "X192": 0.461,
            "X193": -1.04,
            "X194": 1.105,
            "X195": -0.702,
            "X196": 0.326,
            "X197": -1.044,
            "X198": 1.06,
            "X199": -1.11,
            "X200": -0.349,
            "X201": -0.858,
            "X202": -0.511,
            "X203": 0.369,
            "X204": -0.755,
            "X205": 1.897,
            "X206": -0.635,
            "X207": -0.615,
            "X208": 0.54,
            "X209": -0.157,
            "X210": 1.176,
            "X211": -0.18,
            "X212": -0.107,
            "X213": -0.063,
            "X214": 0.252,
            "X215": -1.487,
            "X216": -0.465,
            "X217": -0.122,
            "X218": 0.299,
            "X219": 0.278,
            "X220": 0.524,
            "X221": -0.625,
            "X222": -1.55,
            "X223": -0.276,
            "X224": -0.445,
            "X225": -0.481,
            "X226": 0.574,
            "X227": -1.22,
            "X228": -0.083,
            "X229": -0.754,
            "X230": -0.416,
            "X231": -0.151,
            "X232": 1.353,
            "X233": 1.61,
            "X234": -1.226,
            "X235": 0.482,
            "X236": -1.097,
            "X237": -0.417,
            "X238": -0.808,
            "X239": 1.608,
            "X240": 1.036,
            "X241": 0.999,
            "X242": 0.801,
            "X243": 0.402,
            "X244": -0.186,
            "X245": 0.77,
            "X246": 1.432,
            "X247": -1.035,
            "X248": -2.022,
            "X249": 0.463,
            "X250": -1.37,
            "X251": -0.73,
            "X252": -0.048,
            "X253": -1.128,
            "X254": 1.158,
            "X255": -1.355,
            "X256": -0.637,
            "X257": -0.159,
            "X258": 0.452,
            "X259": -1.866,
            "X260": 0.89,
            "X261": -2.814,
            "X262": 1.056,
            "X263": 0.701,
            "X264": -0.39,
            "X265": -0.753,
            "X266": 1.558,
            "X267": -0.156,
            "X268": -0.671,
            "X269": 1.129,
            "X270": 1.011,
            "X271": -0.128,
            "X272": 0.592,
            "X273": 1.484,
            "X274": 1,
            "X275": -2.011,
            "X276": 0.225,
            "X277": -0.406,
            "X278": -0.987,
            "X279": 1.458,
            "X280": 1.661,
            "X281": -0.743,
            "X282": -0.946,
            "X283": -0.165,
            "X284": -1.203,
            "X285": 1.039,
            "X286": -1.136,
            "X287": 1.464,
            "X288": -0.238,
            "X289": -0.937,
            "X290": -0.062,
            "X291": 0.585,
            "X292": 0.482,
            "X293": 0.676,
            "X294": 0.847,
            "X295": 1.252,
            "X296": -0.245,
            "X297": 0.839,
            "X298": 0.145,
            "X299": -0.63
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7471,
            ".pred_1": 0.2529,
            "id": 196,
            "X0": 1.836,
            "X1": -0.099,
            "X2": 1.661,
            "X3": 1.539,
            "X4": 0.979,
            "X5": -0.72,
            "X6": -0.207,
            "X7": 0.343,
            "X8": 0.117,
            "X9": -0.403,
            "X10": -0.251,
            "X11": -0.564,
            "X12": 0.701,
            "X13": -0.543,
            "X14": 0.303,
            "X15": 1.601,
            "X16": 0.224,
            "X17": 0.226,
            "X18": 0.506,
            "X19": -0.321,
            "X20": 1.106,
            "X21": -0.182,
            "X22": -0.852,
            "X23": -0.091,
            "X24": -2.55,
            "X25": 0.672,
            "X26": -0.03,
            "X27": 0.212,
            "X28": -1.206,
            "X29": -0.198,
            "X30": -1.65,
            "X31": -0.836,
            "X32": 0.335,
            "X33": -1.644,
            "X34": 0.151,
            "X35": 2.348,
            "X36": 1.289,
            "X37": 0.541,
            "X38": 1.192,
            "X39": 0.157,
            "X40": 0.612,
            "X41": -0.071,
            "X42": -1.294,
            "X43": 0.137,
            "X44": 1.166,
            "X45": -0.475,
            "X46": 2.024,
            "X47": 0.663,
            "X48": 1.275,
            "X49": -0.205,
            "X50": 0.742,
            "X51": 0.635,
            "X52": 0.585,
            "X53": 0.213,
            "X54": 1.939,
            "X55": -0.696,
            "X56": 0.124,
            "X57": -0.042,
            "X58": 0.332,
            "X59": -0.573,
            "X60": 0.598,
            "X61": -0.185,
            "X62": -0.898,
            "X63": -0.419,
            "X64": 0.324,
            "X65": 0.579,
            "X66": 0.512,
            "X67": -1.163,
            "X68": -1.807,
            "X69": -1.875,
            "X70": 0.03,
            "X71": -1.409,
            "X72": -1.041,
            "X73": -0.239,
            "X74": -0.491,
            "X75": -1.445,
            "X76": 0.6,
            "X77": 1.91,
            "X78": -0.101,
            "X79": 0.032,
            "X80": 0.572,
            "X81": 0.982,
            "X82": -1.535,
            "X83": 0.401,
            "X84": 0.626,
            "X85": 0.35,
            "X86": 0.792,
            "X87": 0.128,
            "X88": 1.285,
            "X89": -0.436,
            "X90": 0.125,
            "X91": 1.596,
            "X92": -0.922,
            "X93": 0.511,
            "X94": -0.304,
            "X95": 0.408,
            "X96": 0.763,
            "X97": 0.133,
            "X98": -0.282,
            "X99": 1.164,
            "X100": 0.185,
            "X101": -0.11,
            "X102": -0.422,
            "X103": 0.621,
            "X104": -1.267,
            "X105": -0.109,
            "X106": 0.679,
            "X107": 0.59,
            "X108": -0.945,
            "X109": 1.111,
            "X110": 0.505,
            "X111": 0.204,
            "X112": 0.296,
            "X113": -1.607,
            "X114": -1.129,
            "X115": -0.094,
            "X116": 0.248,
            "X117": 0.056,
            "X118": 0.144,
            "X119": 2.903,
            "X120": -0.17,
            "X121": 0.687,
            "X122": -0.489,
            "X123": -2.18,
            "X124": 0.119,
            "X125": -0.729,
            "X126": -0.672,
            "X127": 0.993,
            "X128": -0.568,
            "X129": 1.462,
            "X130": 0.642,
            "X131": -2.271,
            "X132": 0.406,
            "X133": -0.46,
            "X134": 0.958,
            "X135": 0.253,
            "X136": 1.239,
            "X137": -2.503,
            "X138": 0.084,
            "X139": 0.777,
            "X140": -0.697,
            "X141": -0.49,
            "X142": 0.319,
            "X143": -0.486,
            "X144": -0.923,
            "X145": 0.646,
            "X146": -1.029,
            "X147": -0.522,
            "X148": -0.352,
            "X149": -0.295,
            "X150": -1.108,
            "X151": 2.267,
            "X152": -1.061,
            "X153": -1.025,
            "X154": 0.114,
            "X155": -0.763,
            "X156": -1.211,
            "X157": 0.908,
            "X158": -0.768,
            "X159": 0.458,
            "X160": -1.987,
            "X161": -0.335,
            "X162": -1.175,
            "X163": 0.509,
            "X164": -0.673,
            "X165": 0.627,
            "X166": -0.511,
            "X167": 1.81,
            "X168": -1.191,
            "X169": -0.948,
            "X170": 1.949,
            "X171": -1.162,
            "X172": 0.291,
            "X173": -1.389,
            "X174": -0.436,
            "X175": 1.377,
            "X176": 3.263,
            "X177": 0.887,
            "X178": 1.87,
            "X179": -1.337,
            "X180": -0.673,
            "X181": 0.055,
            "X182": 0.205,
            "X183": -1.427,
            "X184": -0.972,
            "X185": -0.104,
            "X186": 0.332,
            "X187": -1.302,
            "X188": 0.792,
            "X189": 0.019,
            "X190": -1.588,
            "X191": -0.517,
            "X192": -0.741,
            "X193": -0.908,
            "X194": 0.293,
            "X195": 1.017,
            "X196": -0.749,
            "X197": 1.375,
            "X198": -1.403,
            "X199": -0.498,
            "X200": 1.842,
            "X201": 1.495,
            "X202": 1.041,
            "X203": 0.038,
            "X204": 0.01,
            "X205": -1.812,
            "X206": -0.018,
            "X207": 0.403,
            "X208": 1.957,
            "X209": -0.933,
            "X210": 0.799,
            "X211": -0.062,
            "X212": 0.785,
            "X213": -1.923,
            "X214": 0.75,
            "X215": 1.145,
            "X216": -0.43,
            "X217": 0.474,
            "X218": -0.947,
            "X219": 0.479,
            "X220": 1.39,
            "X221": 1.251,
            "X222": -1.383,
            "X223": 0.355,
            "X224": -0.47,
            "X225": -1.104,
            "X226": 1.096,
            "X227": 0.493,
            "X228": 1.122,
            "X229": -0.168,
            "X230": -0.893,
            "X231": 1.375,
            "X232": 1.684,
            "X233": 0.18,
            "X234": -0.355,
            "X235": -0.616,
            "X236": -1.47,
            "X237": 0.095,
            "X238": 1.012,
            "X239": -1.994,
            "X240": -0.657,
            "X241": 1.325,
            "X242": 0.332,
            "X243": -0.154,
            "X244": -1.082,
            "X245": -1.879,
            "X246": -0.584,
            "X247": -0.368,
            "X248": -0.086,
            "X249": 0.862,
            "X250": 2.064,
            "X251": -0.93,
            "X252": 0.771,
            "X253": -1.903,
            "X254": -1.861,
            "X255": -0.948,
            "X256": 1.828,
            "X257": 2.327,
            "X258": -0.658,
            "X259": 1.252,
            "X260": -0.635,
            "X261": -0.786,
            "X262": -1.302,
            "X263": 0.871,
            "X264": -2.264,
            "X265": -1.15,
            "X266": 0.147,
            "X267": -0.679,
            "X268": 0.765,
            "X269": -0.314,
            "X270": 1.34,
            "X271": -0.112,
            "X272": 2.325,
            "X273": -0.216,
            "X274": -0.174,
            "X275": -1.863,
            "X276": 0.31,
            "X277": 1.372,
            "X278": 0.949,
            "X279": 1.042,
            "X280": -0.452,
            "X281": 0.351,
            "X282": 0.18,
            "X283": 0.003,
            "X284": 1.274,
            "X285": 1.207,
            "X286": 1.995,
            "X287": -0.695,
            "X288": -1.424,
            "X289": -1.523,
            "X290": 1.85,
            "X291": -2.105,
            "X292": -0.703,
            "X293": 0.144,
            "X294": -0.986,
            "X295": 0.429,
            "X296": -0.072,
            "X297": 0.352,
            "X298": -1.021,
            "X299": 0.657
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.5136,
            ".pred_1": 0.4864,
            "id": 180,
            "X0": 0.868,
            "X1": 0.288,
            "X2": 0.379,
            "X3": -0.726,
            "X4": -0.383,
            "X5": -0.458,
            "X6": -0.441,
            "X7": 0.325,
            "X8": 0.043,
            "X9": 1.112,
            "X10": -0.602,
            "X11": 1.104,
            "X12": 0.507,
            "X13": -0.137,
            "X14": 0.978,
            "X15": 0.892,
            "X16": -0.422,
            "X17": 0.437,
            "X18": 0.144,
            "X19": -0.949,
            "X20": 0.2,
            "X21": -0.309,
            "X22": 0.229,
            "X23": 1.084,
            "X24": 0.026,
            "X25": 1.625,
            "X26": -1.14,
            "X27": 0.804,
            "X28": -0.234,
            "X29": 0.397,
            "X30": 0.729,
            "X31": 0.223,
            "X32": 0.031,
            "X33": 0.65,
            "X34": -0.916,
            "X35": -1.897,
            "X36": -1.037,
            "X37": -1.749,
            "X38": -0.169,
            "X39": 0.246,
            "X40": -0.26,
            "X41": 0.34,
            "X42": -1.513,
            "X43": -1.294,
            "X44": 0.115,
            "X45": -0.982,
            "X46": 2.612,
            "X47": -0.237,
            "X48": 0.819,
            "X49": -2.156,
            "X50": -0.546,
            "X51": -0.382,
            "X52": 1.837,
            "X53": 0.064,
            "X54": -0.134,
            "X55": -0.872,
            "X56": -1.029,
            "X57": 0.049,
            "X58": -0.629,
            "X59": 0.368,
            "X60": 0.891,
            "X61": 1.07,
            "X62": -0.571,
            "X63": 0.578,
            "X64": 1.501,
            "X65": -0.202,
            "X66": 0.186,
            "X67": 0.088,
            "X68": 0.646,
            "X69": 0.973,
            "X70": 0.39,
            "X71": 2.046,
            "X72": -2.218,
            "X73": 0.539,
            "X74": 0.57,
            "X75": -0.18,
            "X76": 0.311,
            "X77": -0.164,
            "X78": 0.586,
            "X79": -0.282,
            "X80": -0.191,
            "X81": -0.405,
            "X82": 0.175,
            "X83": -0.374,
            "X84": -0.241,
            "X85": -2.1,
            "X86": 0.22,
            "X87": 0.536,
            "X88": 1.497,
            "X89": -0.344,
            "X90": 0.418,
            "X91": -0.591,
            "X92": 1.127,
            "X93": 2.211,
            "X94": -1.165,
            "X95": 0.506,
            "X96": 0.179,
            "X97": -0.275,
            "X98": -0.827,
            "X99": 0.477,
            "X100": 0.177,
            "X101": -0.847,
            "X102": -1.152,
            "X103": -0.171,
            "X104": -0.514,
            "X105": -1.213,
            "X106": -0.647,
            "X107": 0.785,
            "X108": 0.168,
            "X109": -0.365,
            "X110": -0.146,
            "X111": 0.15,
            "X112": -1.599,
            "X113": -1.643,
            "X114": 0.082,
            "X115": -0.253,
            "X116": 0.767,
            "X117": -0.044,
            "X118": -0.417,
            "X119": 0.824,
            "X120": -1.21,
            "X121": 1.126,
            "X122": -1.901,
            "X123": -0.606,
            "X124": 0.3,
            "X125": 0.624,
            "X126": 1.056,
            "X127": 0.837,
            "X128": -1.052,
            "X129": 0.058,
            "X130": -0.199,
            "X131": 0.563,
            "X132": 0.977,
            "X133": 0.526,
            "X134": 1.551,
            "X135": 0.206,
            "X136": 1.405,
            "X137": 0.359,
            "X138": 0.093,
            "X139": -0.818,
            "X140": 0.213,
            "X141": -0.218,
            "X142": 2.116,
            "X143": -1.547,
            "X144": 0.025,
            "X145": -0.142,
            "X146": 1.054,
            "X147": -0.127,
            "X148": -1.276,
            "X149": -0.077,
            "X150": 0.175,
            "X151": -0.374,
            "X152": -0.646,
            "X153": -0.996,
            "X154": -0.43,
            "X155": 0.03,
            "X156": -1.006,
            "X157": 0.393,
            "X158": -0.511,
            "X159": 0.606,
            "X160": 2.365,
            "X161": -1.363,
            "X162": -1.611,
            "X163": -2.465,
            "X164": 1.511,
            "X165": -1.877,
            "X166": 1.347,
            "X167": 0.238,
            "X168": 0.83,
            "X169": -0.19,
            "X170": 1.937,
            "X171": -1.432,
            "X172": -1.296,
            "X173": -2.094,
            "X174": 0.954,
            "X175": 0.048,
            "X176": -1.098,
            "X177": -1.307,
            "X178": 1.162,
            "X179": 0.205,
            "X180": 0.333,
            "X181": -1.456,
            "X182": 0.017,
            "X183": -1.174,
            "X184": 0.378,
            "X185": -0.154,
            "X186": -1.618,
            "X187": 0.315,
            "X188": 0.616,
            "X189": 0.301,
            "X190": 0.194,
            "X191": 1.035,
            "X192": -1.305,
            "X193": 0.122,
            "X194": 1.534,
            "X195": 0.425,
            "X196": 0.993,
            "X197": -0.083,
            "X198": -0.789,
            "X199": 1.243,
            "X200": 1.078,
            "X201": -0.127,
            "X202": -0.179,
            "X203": -1.749,
            "X204": -1.857,
            "X205": -0.038,
            "X206": -1.129,
            "X207": -1.56,
            "X208": -1.477,
            "X209": 0.75,
            "X210": 0.843,
            "X211": 0.49,
            "X212": -2.276,
            "X213": 0.989,
            "X214": -0.142,
            "X215": -1.466,
            "X216": 0.593,
            "X217": -0.033,
            "X218": 1.114,
            "X219": -0.941,
            "X220": 1.763,
            "X221": -0.489,
            "X222": -0.116,
            "X223": -0.603,
            "X224": -0.518,
            "X225": 0.882,
            "X226": 0.387,
            "X227": 1.547,
            "X228": -0.338,
            "X229": -2.105,
            "X230": -1.811,
            "X231": 0.491,
            "X232": 0.389,
            "X233": -0.627,
            "X234": 1.076,
            "X235": -1.516,
            "X236": -1.061,
            "X237": -1.229,
            "X238": -0.626,
            "X239": 0.253,
            "X240": 0.568,
            "X241": 0.017,
            "X242": 0.679,
            "X243": 0.808,
            "X244": -0.269,
            "X245": -1.411,
            "X246": -1.732,
            "X247": -0.86,
            "X248": -1.356,
            "X249": -0.496,
            "X250": -0.594,
            "X251": 1.41,
            "X252": -0.599,
            "X253": -0.019,
            "X254": 0.12,
            "X255": -0.484,
            "X256": 0.808,
            "X257": -0.208,
            "X258": -0.476,
            "X259": -0.4,
            "X260": -0.345,
            "X261": 1.486,
            "X262": 0.737,
            "X263": 0.814,
            "X264": 0.044,
            "X265": -1.015,
            "X266": -0.131,
            "X267": -0.208,
            "X268": -0.238,
            "X269": 0.703,
            "X270": 0.218,
            "X271": 1.406,
            "X272": 0.502,
            "X273": -1.065,
            "X274": 0.414,
            "X275": -1.049,
            "X276": -1.216,
            "X277": 0.748,
            "X278": -0.25,
            "X279": -1.53,
            "X280": -0.257,
            "X281": -0.337,
            "X282": 2.84,
            "X283": 0.724,
            "X284": 0.07,
            "X285": -0.408,
            "X286": -1.07,
            "X287": -0.137,
            "X288": -0.394,
            "X289": -0.987,
            "X290": 1.318,
            "X291": -1.535,
            "X292": 0.46,
            "X293": 0.978,
            "X294": -2.015,
            "X295": 2.338,
            "X296": 0.48,
            "X297": -0.617,
            "X298": -0.674,
            "X299": 1.072
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8418,
            ".pred_1": 0.1582,
            "id": 159,
            "X0": 0.788,
            "X1": 2.407,
            "X2": 0.23,
            "X3": -0.855,
            "X4": 0.689,
            "X5": 0.147,
            "X6": -0.232,
            "X7": -1.706,
            "X8": 1.26,
            "X9": 1.095,
            "X10": 0.128,
            "X11": -0.066,
            "X12": -0.303,
            "X13": 0.502,
            "X14": -1.55,
            "X15": 1.166,
            "X16": -0.735,
            "X17": -0.34,
            "X18": -1.019,
            "X19": -1.097,
            "X20": -0.398,
            "X21": -2.777,
            "X22": -0.598,
            "X23": -0.623,
            "X24": -1.002,
            "X25": -0.424,
            "X26": -1.18,
            "X27": -1.111,
            "X28": 1.97,
            "X29": -0.138,
            "X30": 1.626,
            "X31": -0.996,
            "X32": 1.669,
            "X33": -0.317,
            "X34": -0.618,
            "X35": 0.396,
            "X36": -0.646,
            "X37": 0.204,
            "X38": 0.296,
            "X39": -0.742,
            "X40": -0.902,
            "X41": -0.195,
            "X42": 1.476,
            "X43": 0.685,
            "X44": 0.912,
            "X45": -1.193,
            "X46": 0.073,
            "X47": 2.966,
            "X48": 0.471,
            "X49": -1.345,
            "X50": -0.191,
            "X51": -0.765,
            "X52": 0.591,
            "X53": 0.598,
            "X54": 1.005,
            "X55": -0.425,
            "X56": 2.129,
            "X57": 0.229,
            "X58": -0.167,
            "X59": 0.066,
            "X60": -2.296,
            "X61": 0.432,
            "X62": -1.311,
            "X63": 2.514,
            "X64": 0.491,
            "X65": -0.301,
            "X66": -0.436,
            "X67": 0.19,
            "X68": -0.724,
            "X69": 0.378,
            "X70": 0.454,
            "X71": -0.245,
            "X72": -1.006,
            "X73": 0.117,
            "X74": -1.099,
            "X75": -0.823,
            "X76": -0.636,
            "X77": -0.858,
            "X78": -0.246,
            "X79": -0.625,
            "X80": 1.128,
            "X81": -0.288,
            "X82": 0.57,
            "X83": 0.048,
            "X84": -0.301,
            "X85": -0.265,
            "X86": -1.156,
            "X87": 1.158,
            "X88": -1.609,
            "X89": 0.485,
            "X90": 0.525,
            "X91": -1.421,
            "X92": -0.632,
            "X93": 0.324,
            "X94": 0.088,
            "X95": 0.265,
            "X96": -0.655,
            "X97": -0.04,
            "X98": -2.067,
            "X99": -2.064,
            "X100": -0.253,
            "X101": -0.807,
            "X102": -0.416,
            "X103": 0.777,
            "X104": -1.489,
            "X105": 0.265,
            "X106": 0.945,
            "X107": 0.579,
            "X108": 0.833,
            "X109": -0.536,
            "X110": -0.059,
            "X111": 0.958,
            "X112": -0.627,
            "X113": 0.569,
            "X114": -1.064,
            "X115": 0.015,
            "X116": -0.6,
            "X117": 0.262,
            "X118": -1.003,
            "X119": -0.207,
            "X120": 1.691,
            "X121": -0.009,
            "X122": 0.205,
            "X123": 0.206,
            "X124": -2.429,
            "X125": 2.194,
            "X126": -0.928,
            "X127": -0.779,
            "X128": -0.04,
            "X129": -0.015,
            "X130": -0.793,
            "X131": 0.72,
            "X132": 1.244,
            "X133": -0.692,
            "X134": -0.488,
            "X135": 1.093,
            "X136": -1.347,
            "X137": 0.128,
            "X138": 0.348,
            "X139": 1.041,
            "X140": 1.405,
            "X141": 1.918,
            "X142": 0.589,
            "X143": 0.265,
            "X144": 0.72,
            "X145": 0.976,
            "X146": -0.46,
            "X147": -0.294,
            "X148": -0.345,
            "X149": 1.787,
            "X150": -0.729,
            "X151": 0.146,
            "X152": 0.436,
            "X153": -1.138,
            "X154": -0.081,
            "X155": -0.612,
            "X156": -0.836,
            "X157": 0.459,
            "X158": 0.249,
            "X159": 0.453,
            "X160": 0.231,
            "X161": -0.745,
            "X162": -0.702,
            "X163": 0.317,
            "X164": 0.352,
            "X165": 2.642,
            "X166": 1.752,
            "X167": 0.343,
            "X168": -0.189,
            "X169": 0.188,
            "X170": -0.899,
            "X171": -1.434,
            "X172": -0.33,
            "X173": -2.468,
            "X174": 0.53,
            "X175": -1.591,
            "X176": 0.563,
            "X177": 0.568,
            "X178": 0.14,
            "X179": -2.174,
            "X180": 0.995,
            "X181": 0.656,
            "X182": 0.162,
            "X183": 0.37,
            "X184": -1.504,
            "X185": 0.257,
            "X186": 0.589,
            "X187": -1.551,
            "X188": 1.787,
            "X189": 0.274,
            "X190": -0.009,
            "X191": -1.467,
            "X192": 0.486,
            "X193": -0.257,
            "X194": -1.052,
            "X195": 0.342,
            "X196": 0.888,
            "X197": 1.097,
            "X198": 0.512,
            "X199": 0.092,
            "X200": -1.69,
            "X201": -0.075,
            "X202": 0.953,
            "X203": 0.165,
            "X204": -0.303,
            "X205": 0.948,
            "X206": 1.74,
            "X207": 0.284,
            "X208": -1.083,
            "X209": 0.359,
            "X210": 0.484,
            "X211": -0.339,
            "X212": 1.609,
            "X213": -0.717,
            "X214": -0.22,
            "X215": -0.907,
            "X216": 0.805,
            "X217": -1.896,
            "X218": 2.211,
            "X219": 1.001,
            "X220": -0.865,
            "X221": -1.04,
            "X222": 0.136,
            "X223": 1.07,
            "X224": 0.318,
            "X225": 0.45,
            "X226": -1.424,
            "X227": 0.342,
            "X228": -0.718,
            "X229": -1.751,
            "X230": -0.334,
            "X231": -0.637,
            "X232": 0.323,
            "X233": -0.586,
            "X234": 0.294,
            "X235": 1.175,
            "X236": 0.032,
            "X237": 1.356,
            "X238": -0.663,
            "X239": 0.751,
            "X240": 1.027,
            "X241": 0.609,
            "X242": -0.6,
            "X243": 1.118,
            "X244": -0.798,
            "X245": -0.627,
            "X246": -0.77,
            "X247": -0.414,
            "X248": 0.905,
            "X249": -1.484,
            "X250": -1.059,
            "X251": -0.655,
            "X252": -0.717,
            "X253": 0.215,
            "X254": 1.498,
            "X255": 0.293,
            "X256": 2.146,
            "X257": 0.398,
            "X258": -0.92,
            "X259": 1.57,
            "X260": -0.855,
            "X261": -1.334,
            "X262": 1.594,
            "X263": 0.573,
            "X264": 0.674,
            "X265": -2.349,
            "X266": 1.235,
            "X267": 0.373,
            "X268": -0.331,
            "X269": -0.176,
            "X270": -0.592,
            "X271": 0.808,
            "X272": 0.1,
            "X273": -0.747,
            "X274": -1.097,
            "X275": 0.316,
            "X276": 0.571,
            "X277": -0.888,
            "X278": -0.71,
            "X279": 0.073,
            "X280": 1.606,
            "X281": -0.268,
            "X282": 0.105,
            "X283": -0.52,
            "X284": 1.47,
            "X285": 0.737,
            "X286": 1.04,
            "X287": 0.063,
            "X288": -0.742,
            "X289": -0.552,
            "X290": 1.73,
            "X291": 1.775,
            "X292": 0.997,
            "X293": -1.458,
            "X294": 0.401,
            "X295": -1.738,
            "X296": 0.217,
            "X297": 0.335,
            "X298": -1.158,
            "X299": -0.995
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7223,
            ".pred_1": 0.2777,
            "id": 118,
            "X0": -0.005,
            "X1": -1.489,
            "X2": -0.636,
            "X3": 1.002,
            "X4": 1.139,
            "X5": 0.692,
            "X6": -1.04,
            "X7": -0.511,
            "X8": -0.704,
            "X9": -0.716,
            "X10": 0.365,
            "X11": -1.409,
            "X12": -0.294,
            "X13": 1.589,
            "X14": -1.077,
            "X15": 0.711,
            "X16": -2.523,
            "X17": -2.571,
            "X18": -0.368,
            "X19": 0.084,
            "X20": 1.743,
            "X21": 0.035,
            "X22": 0.045,
            "X23": -0.378,
            "X24": 0.635,
            "X25": 1.049,
            "X26": 1.521,
            "X27": -0.863,
            "X28": -1.371,
            "X29": 0.817,
            "X30": -0.536,
            "X31": 0.852,
            "X32": 0.339,
            "X33": -0.389,
            "X34": 0.536,
            "X35": 0.04,
            "X36": -0.504,
            "X37": 0.392,
            "X38": 1.515,
            "X39": 0.689,
            "X40": -3.129,
            "X41": 0.278,
            "X42": -1.091,
            "X43": -0.777,
            "X44": -0.472,
            "X45": 2.951,
            "X46": 1.072,
            "X47": 0.012,
            "X48": -1.203,
            "X49": -0.214,
            "X50": -0.394,
            "X51": 2.028,
            "X52": -1.509,
            "X53": -0.537,
            "X54": -1.992,
            "X55": 0.107,
            "X56": -0.251,
            "X57": -0.122,
            "X58": 1.373,
            "X59": -2.347,
            "X60": 0.453,
            "X61": 0.888,
            "X62": -0.629,
            "X63": 0.156,
            "X64": -0.09,
            "X65": -2.152,
            "X66": -0.184,
            "X67": -0.268,
            "X68": 0.593,
            "X69": 0.468,
            "X70": -1.425,
            "X71": -0.584,
            "X72": 0.376,
            "X73": 1.192,
            "X74": -1.776,
            "X75": -0.325,
            "X76": 1.101,
            "X77": -0.227,
            "X78": 0.177,
            "X79": -0.085,
            "X80": 0.608,
            "X81": -0.731,
            "X82": -1.865,
            "X83": -0.818,
            "X84": 0.728,
            "X85": -0.668,
            "X86": 0.416,
            "X87": -1.235,
            "X88": 0.522,
            "X89": -0.426,
            "X90": 0.963,
            "X91": 0.148,
            "X92": 0.809,
            "X93": 0.441,
            "X94": 0.928,
            "X95": -0.798,
            "X96": 1.239,
            "X97": 0.755,
            "X98": -2.829,
            "X99": -1.491,
            "X100": 0.145,
            "X101": 1.937,
            "X102": 1.384,
            "X103": 0.926,
            "X104": 0.234,
            "X105": -1.732,
            "X106": -0.159,
            "X107": -0.396,
            "X108": 0.544,
            "X109": -0.997,
            "X110": -0.096,
            "X111": 0.444,
            "X112": -0.263,
            "X113": 2.365,
            "X114": -0.356,
            "X115": 0.267,
            "X116": -1.112,
            "X117": -1.238,
            "X118": -1.066,
            "X119": -1.339,
            "X120": -0.136,
            "X121": -1.915,
            "X122": 1.048,
            "X123": -1.259,
            "X124": -0.026,
            "X125": 2.043,
            "X126": 0.447,
            "X127": -1.954,
            "X128": -1.615,
            "X129": -0.127,
            "X130": 0.258,
            "X131": 2.17,
            "X132": -0.32,
            "X133": -0.561,
            "X134": 0.187,
            "X135": 0.162,
            "X136": -0.466,
            "X137": 0.767,
            "X138": 1.229,
            "X139": 1.808,
            "X140": -0.016,
            "X141": 0.649,
            "X142": -1.591,
            "X143": -0.326,
            "X144": 0.429,
            "X145": -1.297,
            "X146": -1.817,
            "X147": -1.542,
            "X148": -1.774,
            "X149": 0.708,
            "X150": 0.2,
            "X151": -0.966,
            "X152": 1.016,
            "X153": -0.087,
            "X154": 1.784,
            "X155": 1.498,
            "X156": -0.579,
            "X157": -0.891,
            "X158": 1.719,
            "X159": -0.061,
            "X160": 1.722,
            "X161": -0.059,
            "X162": 0.801,
            "X163": -1.238,
            "X164": -0.554,
            "X165": -0.209,
            "X166": 1.418,
            "X167": 0.645,
            "X168": -0.475,
            "X169": -0.05,
            "X170": -1.458,
            "X171": -1.647,
            "X172": 0.759,
            "X173": -0.807,
            "X174": -1.543,
            "X175": -0.032,
            "X176": 0.668,
            "X177": 0.158,
            "X178": 0.372,
            "X179": 0.019,
            "X180": -0.156,
            "X181": 0.257,
            "X182": -2.014,
            "X183": 0.139,
            "X184": 2.279,
            "X185": -1.934,
            "X186": 0.442,
            "X187": -2.155,
            "X188": 0.153,
            "X189": 1.106,
            "X190": 0.09,
            "X191": 1.019,
            "X192": -0.041,
            "X193": 1.947,
            "X194": 0.663,
            "X195": -1.25,
            "X196": 0.506,
            "X197": 0.834,
            "X198": -0.346,
            "X199": -1.433,
            "X200": -0.403,
            "X201": 0.232,
            "X202": -0.444,
            "X203": -1.244,
            "X204": -0.821,
            "X205": -1.472,
            "X206": 0.351,
            "X207": 1.765,
            "X208": -1.16,
            "X209": -0.009,
            "X210": -0.017,
            "X211": -1.733,
            "X212": -2.011,
            "X213": 0.578,
            "X214": -1.749,
            "X215": -0.371,
            "X216": 0.998,
            "X217": -0.161,
            "X218": -0.115,
            "X219": 0.956,
            "X220": 0.886,
            "X221": 0.083,
            "X222": -0.163,
            "X223": 1.51,
            "X224": 1.1,
            "X225": 1.478,
            "X226": 0.216,
            "X227": 0.283,
            "X228": -0.818,
            "X229": 1.011,
            "X230": 0.403,
            "X231": 0.602,
            "X232": -0.694,
            "X233": 0.88,
            "X234": 1.046,
            "X235": -0.128,
            "X236": -0.535,
            "X237": -0.847,
            "X238": -0.345,
            "X239": -0.065,
            "X240": 0.76,
            "X241": -0.775,
            "X242": 1.277,
            "X243": -0.624,
            "X244": -1.271,
            "X245": -0.376,
            "X246": -1.21,
            "X247": 1.597,
            "X248": 0.915,
            "X249": 0.518,
            "X250": 0.105,
            "X251": -0.497,
            "X252": -2.219,
            "X253": 0.915,
            "X254": -0.586,
            "X255": 0.99,
            "X256": 0.545,
            "X257": 2.086,
            "X258": -0.434,
            "X259": -1.808,
            "X260": -0.1,
            "X261": -2.044,
            "X262": 1.707,
            "X263": -0.394,
            "X264": 0.546,
            "X265": -1.294,
            "X266": 0.313,
            "X267": 2.312,
            "X268": -1.471,
            "X269": 2.481,
            "X270": -1.753,
            "X271": -0.333,
            "X272": 0.05,
            "X273": 1.503,
            "X274": 0.424,
            "X275": 1.589,
            "X276": -0.692,
            "X277": -1.203,
            "X278": -0.135,
            "X279": 0.142,
            "X280": -0.054,
            "X281": -0.621,
            "X282": 0.73,
            "X283": -0.337,
            "X284": -0.445,
            "X285": 1.831,
            "X286": -0.684,
            "X287": 0.061,
            "X288": -0.171,
            "X289": -2.57,
            "X290": -0.909,
            "X291": 0.004,
            "X292": 0.435,
            "X293": -0.523,
            "X294": -0.229,
            "X295": 0.403,
            "X296": 0.668,
            "X297": 0.923,
            "X298": 0.358,
            "X299": 0.686
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6154,
            ".pred_1": 0.3846,
            "id": 96,
            "X0": -0.825,
            "X1": 0.135,
            "X2": -1.307,
            "X3": -0.139,
            "X4": 0.728,
            "X5": -1.061,
            "X6": 0.459,
            "X7": -0.761,
            "X8": 0.129,
            "X9": 0.149,
            "X10": 0.371,
            "X11": -0.551,
            "X12": -0.035,
            "X13": -0.799,
            "X14": -0.739,
            "X15": 0.176,
            "X16": -1.818,
            "X17": 0.572,
            "X18": 0.445,
            "X19": 0.566,
            "X20": -1.603,
            "X21": 0.082,
            "X22": 1.303,
            "X23": 0.4,
            "X24": 0.403,
            "X25": -1.326,
            "X26": 0.957,
            "X27": 1.071,
            "X28": -0.006,
            "X29": 0.7,
            "X30": -1.888,
            "X31": 0.42,
            "X32": 1.041,
            "X33": 1.336,
            "X34": -0.525,
            "X35": -0.81,
            "X36": -0.295,
            "X37": 1.04,
            "X38": -2.274,
            "X39": -0.04,
            "X40": 0.552,
            "X41": -1.241,
            "X42": 0.437,
            "X43": -0.939,
            "X44": -1.956,
            "X45": 0.804,
            "X46": 0.135,
            "X47": -0.614,
            "X48": 0.659,
            "X49": -0.597,
            "X50": 1.503,
            "X51": 1.215,
            "X52": 0.492,
            "X53": 1.17,
            "X54": -0.394,
            "X55": 0.085,
            "X56": -1.847,
            "X57": 0.528,
            "X58": 0.668,
            "X59": 1.787,
            "X60": 1.096,
            "X61": -0.674,
            "X62": -0.57,
            "X63": 0.863,
            "X64": -0.281,
            "X65": 1.704,
            "X66": 0.053,
            "X67": -0.104,
            "X68": -0.821,
            "X69": -1.026,
            "X70": -0.455,
            "X71": 0.18,
            "X72": -1.22,
            "X73": 0.541,
            "X74": -0.183,
            "X75": 0.983,
            "X76": -0.578,
            "X77": -1.147,
            "X78": -0.414,
            "X79": -2.798,
            "X80": -1.579,
            "X81": 0.686,
            "X82": 0.285,
            "X83": 0.665,
            "X84": 1.018,
            "X85": 0.719,
            "X86": -0.182,
            "X87": 2.56,
            "X88": -0.647,
            "X89": -0.666,
            "X90": 0.598,
            "X91": 0.224,
            "X92": -0.675,
            "X93": -0.47,
            "X94": -1.436,
            "X95": 0.758,
            "X96": -1.048,
            "X97": 0.687,
            "X98": 0.846,
            "X99": 0.962,
            "X100": -0.528,
            "X101": -0.028,
            "X102": -0.801,
            "X103": 0.801,
            "X104": -0.19,
            "X105": -0.155,
            "X106": -0.968,
            "X107": -0.072,
            "X108": -1.449,
            "X109": 0.218,
            "X110": 0.338,
            "X111": -0.303,
            "X112": -0.602,
            "X113": 0.078,
            "X114": 1.653,
            "X115": -0.555,
            "X116": -0.761,
            "X117": -1.158,
            "X118": -0.95,
            "X119": 0.003,
            "X120": -0.738,
            "X121": 0.115,
            "X122": 1.109,
            "X123": -0.878,
            "X124": -0.277,
            "X125": -0.271,
            "X126": -0.616,
            "X127": -0.255,
            "X128": 1.33,
            "X129": -2.32,
            "X130": 0.816,
            "X131": 1.281,
            "X132": 1.716,
            "X133": 1.593,
            "X134": -0.565,
            "X135": 0.137,
            "X136": -0.641,
            "X137": 0.432,
            "X138": -0.354,
            "X139": -0.514,
            "X140": -0.032,
            "X141": -0.051,
            "X142": 0.523,
            "X143": -0.136,
            "X144": -1.463,
            "X145": 1.418,
            "X146": -0.83,
            "X147": 1.042,
            "X148": -1.4,
            "X149": 1.238,
            "X150": -0.239,
            "X151": -3.113,
            "X152": 0.907,
            "X153": 0.724,
            "X154": 0.059,
            "X155": -0.14,
            "X156": 3.009,
            "X157": -0.149,
            "X158": -0.634,
            "X159": 0.067,
            "X160": 0.277,
            "X161": -0.654,
            "X162": -0.836,
            "X163": -1.717,
            "X164": -0.072,
            "X165": -0.016,
            "X166": -0.924,
            "X167": 0.545,
            "X168": -0.161,
            "X169": -1.816,
            "X170": -0.305,
            "X171": -0.463,
            "X172": 0.754,
            "X173": 1.018,
            "X174": -0.605,
            "X175": 0.464,
            "X176": -1.054,
            "X177": -1.733,
            "X178": 0.377,
            "X179": 1.121,
            "X180": 0.044,
            "X181": -1.243,
            "X182": -0.56,
            "X183": -0.245,
            "X184": 1.962,
            "X185": -0.128,
            "X186": 1.41,
            "X187": 0.816,
            "X188": -0.652,
            "X189": -0.145,
            "X190": 0.328,
            "X191": -0.165,
            "X192": 0.634,
            "X193": -1.354,
            "X194": 1.215,
            "X195": 0.029,
            "X196": 0.31,
            "X197": 0.749,
            "X198": -1.419,
            "X199": 1.662,
            "X200": 0.587,
            "X201": 0.788,
            "X202": 1.487,
            "X203": -0.604,
            "X204": 0.078,
            "X205": 0.195,
            "X206": -0.066,
            "X207": -0.079,
            "X208": -1.28,
            "X209": -0.591,
            "X210": -0.676,
            "X211": -0.012,
            "X212": -0.557,
            "X213": -1.031,
            "X214": -1.602,
            "X215": -0.24,
            "X216": 1.403,
            "X217": 1.089,
            "X218": -0.911,
            "X219": 0.421,
            "X220": 0.121,
            "X221": -0.046,
            "X222": -0.021,
            "X223": -2.629,
            "X224": -1.126,
            "X225": 0.453,
            "X226": -0.52,
            "X227": 0.629,
            "X228": 1.894,
            "X229": -1.66,
            "X230": -1.339,
            "X231": 0.539,
            "X232": -0.043,
            "X233": -0.264,
            "X234": -0.133,
            "X235": -0.903,
            "X236": -0.042,
            "X237": -0.154,
            "X238": -0.145,
            "X239": 0.18,
            "X240": -0.203,
            "X241": -0.351,
            "X242": -1.22,
            "X243": 0.634,
            "X244": 2.28,
            "X245": 0.406,
            "X246": 1.825,
            "X247": 0.81,
            "X248": 0.124,
            "X249": -0.648,
            "X250": 1.147,
            "X251": 0.759,
            "X252": 1.442,
            "X253": -0.087,
            "X254": 1.357,
            "X255": 0.46,
            "X256": -0.833,
            "X257": -0.347,
            "X258": -0.034,
            "X259": 1.1,
            "X260": 0.85,
            "X261": -0.059,
            "X262": -0.892,
            "X263": 1.264,
            "X264": 0.941,
            "X265": -0.146,
            "X266": 0.235,
            "X267": -0.228,
            "X268": 1.061,
            "X269": -0.035,
            "X270": -0.463,
            "X271": -0.346,
            "X272": -0.788,
            "X273": 0.305,
            "X274": -1.091,
            "X275": 0.735,
            "X276": -0.858,
            "X277": 0.137,
            "X278": -1.786,
            "X279": 1.379,
            "X280": -0.015,
            "X281": -1.872,
            "X282": 1.271,
            "X283": -0.85,
            "X284": 0.157,
            "X285": 0.694,
            "X286": -0.492,
            "X287": 1.43,
            "X288": 1.717,
            "X289": 0.532,
            "X290": 0.112,
            "X291": -0.867,
            "X292": 1.364,
            "X293": -0.753,
            "X294": -2.351,
            "X295": -1.989,
            "X296": -0.114,
            "X297": -0.744,
            "X298": 0.82,
            "X299": 0.759
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.9283,
            ".pred_1": 0.0717,
            "id": 108,
            "X0": 1.133,
            "X1": 1.1,
            "X2": -0.118,
            "X3": -1.375,
            "X4": 0.42,
            "X5": -0.629,
            "X6": -0.744,
            "X7": 0.371,
            "X8": -0.583,
            "X9": -2.753,
            "X10": 0.146,
            "X11": -1.446,
            "X12": -0.275,
            "X13": 0.634,
            "X14": 1.816,
            "X15": 0.73,
            "X16": 1.087,
            "X17": -1.251,
            "X18": -2.275,
            "X19": 1.513,
            "X20": -2.023,
            "X21": -0.815,
            "X22": 0.664,
            "X23": 0.936,
            "X24": -0.449,
            "X25": -0.22,
            "X26": 0.069,
            "X27": -0.645,
            "X28": 0.343,
            "X29": -0.266,
            "X30": -0.712,
            "X31": -0.645,
            "X32": 0.33,
            "X33": -0.693,
            "X34": 2.034,
            "X35": -0.34,
            "X36": 0.018,
            "X37": -0.553,
            "X38": -0.048,
            "X39": -1.37,
            "X40": 0.807,
            "X41": -1.252,
            "X42": -0.773,
            "X43": -0.953,
            "X44": -0.393,
            "X45": 0.018,
            "X46": 0.932,
            "X47": 0.256,
            "X48": -1.501,
            "X49": 0.242,
            "X50": -0.528,
            "X51": -2.53,
            "X52": 0.224,
            "X53": -2.285,
            "X54": -0.382,
            "X55": -0.41,
            "X56": -0.788,
            "X57": 0.802,
            "X58": -0.296,
            "X59": -0.905,
            "X60": -1.618,
            "X61": -1.523,
            "X62": -1.017,
            "X63": -0.901,
            "X64": -0.975,
            "X65": -1.596,
            "X66": 0.916,
            "X67": 0.54,
            "X68": -0.756,
            "X69": 0.459,
            "X70": -0.294,
            "X71": -0.201,
            "X72": -0.304,
            "X73": -0.721,
            "X74": 0.165,
            "X75": -0.24,
            "X76": 0.424,
            "X77": -0.471,
            "X78": 0.072,
            "X79": -0.918,
            "X80": 0.872,
            "X81": -1.087,
            "X82": -0.274,
            "X83": -0.804,
            "X84": -1.902,
            "X85": -1.503,
            "X86": 0.249,
            "X87": -1.159,
            "X88": 0.833,
            "X89": -0.395,
            "X90": -1.118,
            "X91": -0.52,
            "X92": 0.659,
            "X93": -0.453,
            "X94": 0.668,
            "X95": 0.422,
            "X96": -2.731,
            "X97": 0.24,
            "X98": 0.347,
            "X99": -0.481,
            "X100": -1.592,
            "X101": -0.646,
            "X102": 2.048,
            "X103": -0.517,
            "X104": -0.659,
            "X105": 0.013,
            "X106": -0.223,
            "X107": 0.23,
            "X108": 0.495,
            "X109": 1.438,
            "X110": 1.705,
            "X111": -0.353,
            "X112": 0.718,
            "X113": 0.805,
            "X114": 0.127,
            "X115": 2.436,
            "X116": -0.393,
            "X117": 1.289,
            "X118": 0.825,
            "X119": 1.48,
            "X120": -1.006,
            "X121": -1.063,
            "X122": -2.705,
            "X123": 0.141,
            "X124": -1.542,
            "X125": 1.501,
            "X126": -0.423,
            "X127": -2.064,
            "X128": 0.79,
            "X129": 0.367,
            "X130": -0.565,
            "X131": -0.149,
            "X132": -0.371,
            "X133": -1.489,
            "X134": 0.766,
            "X135": -0.952,
            "X136": -0.286,
            "X137": 1.569,
            "X138": 0.606,
            "X139": -0.518,
            "X140": 0.412,
            "X141": 1.526,
            "X142": -1.613,
            "X143": -0.256,
            "X144": -0.058,
            "X145": 0.635,
            "X146": 0.021,
            "X147": -0.636,
            "X148": -1.421,
            "X149": -0.6,
            "X150": -0.719,
            "X151": -2.465,
            "X152": -1.036,
            "X153": -1.221,
            "X154": -0.732,
            "X155": 0.284,
            "X156": 1.064,
            "X157": -1.072,
            "X158": 0.793,
            "X159": 0.395,
            "X160": -1.547,
            "X161": -2.083,
            "X162": 0.556,
            "X163": 1.282,
            "X164": 0.849,
            "X165": 0.656,
            "X166": -0.574,
            "X167": 0.497,
            "X168": -0.61,
            "X169": 0.62,
            "X170": -1.896,
            "X171": 0.134,
            "X172": 0.195,
            "X173": 0.215,
            "X174": -1.216,
            "X175": -1.216,
            "X176": 0.785,
            "X177": -1.436,
            "X178": -0.824,
            "X179": -0.185,
            "X180": -0.35,
            "X181": 0.143,
            "X182": -0.082,
            "X183": 0.118,
            "X184": -0.657,
            "X185": -0.761,
            "X186": -0.129,
            "X187": 0.355,
            "X188": 0.758,
            "X189": -0.562,
            "X190": 0.296,
            "X191": 1.21,
            "X192": -1.465,
            "X193": 2.903,
            "X194": -0.57,
            "X195": -0.292,
            "X196": 0.378,
            "X197": -0.346,
            "X198": -0.786,
            "X199": -0.811,
            "X200": 1.603,
            "X201": 1.573,
            "X202": -0.05,
            "X203": 1.465,
            "X204": -0.078,
            "X205": 0.637,
            "X206": 1.594,
            "X207": 0.324,
            "X208": 0.952,
            "X209": -0.547,
            "X210": 0.529,
            "X211": -0.175,
            "X212": -0.754,
            "X213": -0.317,
            "X214": 1.222,
            "X215": -1.325,
            "X216": 0.741,
            "X217": 1.193,
            "X218": -0.825,
            "X219": -0.278,
            "X220": -0.733,
            "X221": 1.581,
            "X222": -0.684,
            "X223": -0.941,
            "X224": 0.98,
            "X225": -0.752,
            "X226": -0.41,
            "X227": -0.161,
            "X228": 0.84,
            "X229": 0.129,
            "X230": -0.563,
            "X231": 0.843,
            "X232": 0.648,
            "X233": -0.627,
            "X234": -0.025,
            "X235": 1.266,
            "X236": -0.044,
            "X237": -1.138,
            "X238": 2.466,
            "X239": 0.58,
            "X240": 1.063,
            "X241": 0.475,
            "X242": 0.165,
            "X243": 0.557,
            "X244": -0.436,
            "X245": 1.31,
            "X246": 0.91,
            "X247": -0.637,
            "X248": 1.29,
            "X249": -0.348,
            "X250": -0.661,
            "X251": 0.632,
            "X252": -0.09,
            "X253": 1.391,
            "X254": 0.849,
            "X255": -0.119,
            "X256": -1.104,
            "X257": -0.46,
            "X258": -0.581,
            "X259": -0.442,
            "X260": 1.638,
            "X261": -1.527,
            "X262": 0.037,
            "X263": -2.365,
            "X264": -0.206,
            "X265": -0.151,
            "X266": -1.151,
            "X267": 0.915,
            "X268": -0.309,
            "X269": -2.029,
            "X270": -0.201,
            "X271": 0.463,
            "X272": -0.74,
            "X273": -0.2,
            "X274": -0.32,
            "X275": -0.216,
            "X276": 0.147,
            "X277": -0.032,
            "X278": -0.69,
            "X279": -1.809,
            "X280": -0.215,
            "X281": -0.049,
            "X282": 0.312,
            "X283": -0.272,
            "X284": -0.475,
            "X285": 0.02,
            "X286": -0.03,
            "X287": -0.345,
            "X288": 0.297,
            "X289": 0.646,
            "X290": 0.398,
            "X291": -0.742,
            "X292": 1.021,
            "X293": -0.377,
            "X294": -0.648,
            "X295": -1.645,
            "X296": -0.435,
            "X297": 1.065,
            "X298": -0.205,
            "X299": -0.49
          },
          {
            ".pred_class": 1,
            ".pred_0": 0.47,
            ".pred_1": 0.53,
            "id": 185,
            "X0": 0.053,
            "X1": -0.516,
            "X2": 0.49,
            "X3": -1.743,
            "X4": -1.521,
            "X5": -0.892,
            "X6": -0.911,
            "X7": -0.99,
            "X8": -0.358,
            "X9": 1.753,
            "X10": 1.266,
            "X11": 0.66,
            "X12": -0.276,
            "X13": -0.217,
            "X14": -1.474,
            "X15": 0.134,
            "X16": -0.008,
            "X17": 1.443,
            "X18": -1.523,
            "X19": 1.749,
            "X20": -2.031,
            "X21": -0.804,
            "X22": -0.723,
            "X23": -0.964,
            "X24": 0.048,
            "X25": 0.958,
            "X26": 0.74,
            "X27": 0.277,
            "X28": 2.048,
            "X29": -0.54,
            "X30": 0.9,
            "X31": -1.137,
            "X32": -0.261,
            "X33": -1.679,
            "X34": 0.505,
            "X35": -0.536,
            "X36": 0.26,
            "X37": -2.322,
            "X38": 0.681,
            "X39": -0.11,
            "X40": 1.675,
            "X41": 1.099,
            "X42": -0.752,
            "X43": 1.331,
            "X44": 0.558,
            "X45": -0.626,
            "X46": 1.389,
            "X47": -1.313,
            "X48": 2.504,
            "X49": -0.86,
            "X50": 0.181,
            "X51": -0.228,
            "X52": 1.807,
            "X53": -0.624,
            "X54": 1.069,
            "X55": -1.651,
            "X56": -1.95,
            "X57": 0.866,
            "X58": 0.863,
            "X59": -0.312,
            "X60": 1.446,
            "X61": 0.166,
            "X62": 0.185,
            "X63": -0.282,
            "X64": -0.524,
            "X65": -0.605,
            "X66": -0.404,
            "X67": -0.529,
            "X68": -0.549,
            "X69": -1.641,
            "X70": 0.24,
            "X71": 0.574,
            "X72": 0.717,
            "X73": 0.403,
            "X74": 0.649,
            "X75": 1.018,
            "X76": -0.435,
            "X77": 0.161,
            "X78": -0.287,
            "X79": -1.246,
            "X80": -0.041,
            "X81": 0.706,
            "X82": 1.706,
            "X83": -0.887,
            "X84": -1.573,
            "X85": -1.376,
            "X86": -1.183,
            "X87": -2.28,
            "X88": 0.926,
            "X89": 0.333,
            "X90": -0.563,
            "X91": 0.794,
            "X92": -2.296,
            "X93": 0.027,
            "X94": -0.038,
            "X95": 0.207,
            "X96": -0.439,
            "X97": -0.477,
            "X98": 1.104,
            "X99": -0.046,
            "X100": -1.492,
            "X101": -0.369,
            "X102": 0.288,
            "X103": 0.206,
            "X104": 0.738,
            "X105": -0.756,
            "X106": -0.88,
            "X107": -0.102,
            "X108": -0.013,
            "X109": -1.392,
            "X110": -0.289,
            "X111": 0.709,
            "X112": 0.095,
            "X113": -2.036,
            "X114": 1.494,
            "X115": 0.378,
            "X116": 0.129,
            "X117": -0.012,
            "X118": -1.196,
            "X119": 0.314,
            "X120": 0.34,
            "X121": 0.698,
            "X122": -1.171,
            "X123": 1.366,
            "X124": -1.446,
            "X125": -1.56,
            "X126": 0.677,
            "X127": 0.963,
            "X128": -0.514,
            "X129": 0.353,
            "X130": -0.301,
            "X131": -0.164,
            "X132": 1.631,
            "X133": -0.216,
            "X134": 1.669,
            "X135": -2.007,
            "X136": -1.633,
            "X137": 0.758,
            "X138": 0.918,
            "X139": 1.215,
            "X140": -0.898,
            "X141": 1.159,
            "X142": 0.217,
            "X143": -0.638,
            "X144": -0.288,
            "X145": -0.494,
            "X146": -0.551,
            "X147": -0.674,
            "X148": 0.771,
            "X149": -1.066,
            "X150": 0.316,
            "X151": -0.258,
            "X152": -0.695,
            "X153": -0.157,
            "X154": -0.249,
            "X155": -2.102,
            "X156": -1.148,
            "X157": -0.123,
            "X158": -2.412,
            "X159": -1.7,
            "X160": 0.456,
            "X161": 2.266,
            "X162": 0.451,
            "X163": -1.317,
            "X164": -0.697,
            "X165": 1.819,
            "X166": -0.907,
            "X167": 0.645,
            "X168": 0.371,
            "X169": -0.459,
            "X170": 0.675,
            "X171": -1.107,
            "X172": -1.497,
            "X173": -0.6,
            "X174": 1.506,
            "X175": -1.53,
            "X176": -1.923,
            "X177": -0.635,
            "X178": 0.91,
            "X179": 0.055,
            "X180": -0.025,
            "X181": 1.384,
            "X182": 2.001,
            "X183": 1.603,
            "X184": 0.985,
            "X185": -0.596,
            "X186": -0.694,
            "X187": 0.343,
            "X188": -0.754,
            "X189": 0.378,
            "X190": -0.201,
            "X191": -1.731,
            "X192": 0.238,
            "X193": -0.557,
            "X194": 1.156,
            "X195": -0.223,
            "X196": 0.212,
            "X197": -0.337,
            "X198": -0.658,
            "X199": 0.281,
            "X200": -0.675,
            "X201": -0.209,
            "X202": -0.821,
            "X203": -0.71,
            "X204": 1.318,
            "X205": -0.117,
            "X206": 0.684,
            "X207": -2.24,
            "X208": -0.796,
            "X209": -0.354,
            "X210": -1.799,
            "X211": 0.896,
            "X212": -0.048,
            "X213": -0.171,
            "X214": -0.663,
            "X215": 0.871,
            "X216": 0.686,
            "X217": -1.405,
            "X218": 1.381,
            "X219": -1.769,
            "X220": 0.256,
            "X221": 0.562,
            "X222": 1.168,
            "X223": 0.45,
            "X224": -1.901,
            "X225": -1.286,
            "X226": -0.78,
            "X227": 0.169,
            "X228": 1.868,
            "X229": -0.132,
            "X230": -0.324,
            "X231": 0.655,
            "X232": 0.575,
            "X233": -1.193,
            "X234": 0.21,
            "X235": -1.003,
            "X236": 1.101,
            "X237": 1.001,
            "X238": -0.777,
            "X239": -1.377,
            "X240": 1.223,
            "X241": -0.687,
            "X242": -0.547,
            "X243": -0.41,
            "X244": -1.286,
            "X245": 0.36,
            "X246": 0.142,
            "X247": 1.533,
            "X248": 0.053,
            "X249": -0.099,
            "X250": -0.195,
            "X251": -1.616,
            "X252": -1.312,
            "X253": 0.49,
            "X254": 0.735,
            "X255": 0.149,
            "X256": 1.741,
            "X257": 0.212,
            "X258": -1.483,
            "X259": 0.737,
            "X260": 2.195,
            "X261": -1.292,
            "X262": 1.27,
            "X263": -2.159,
            "X264": 1.098,
            "X265": 0.061,
            "X266": -1.117,
            "X267": 0.048,
            "X268": 1.002,
            "X269": 0.929,
            "X270": -0.076,
            "X271": -1.003,
            "X272": -1.169,
            "X273": -0.277,
            "X274": -0.957,
            "X275": -0.291,
            "X276": -1.289,
            "X277": 0.33,
            "X278": 0.258,
            "X279": 0.659,
            "X280": -0.604,
            "X281": -0.281,
            "X282": -0.951,
            "X283": 0.18,
            "X284": 2.792,
            "X285": 0.237,
            "X286": -0.659,
            "X287": -0.524,
            "X288": 0,
            "X289": 0.889,
            "X290": 0.704,
            "X291": 0.443,
            "X292": -1.04,
            "X293": -1.77,
            "X294": -1.255,
            "X295": -0.389,
            "X296": -2.416,
            "X297": -0.656,
            "X298": 0.152,
            "X299": 0.286
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7448,
            ".pred_1": 0.2552,
            "id": 222,
            "X0": 0.738,
            "X1": -0.058,
            "X2": -1.083,
            "X3": -1.839,
            "X4": -1.26,
            "X5": -2.529,
            "X6": -0.337,
            "X7": -1.481,
            "X8": -0.832,
            "X9": -0.389,
            "X10": -2.134,
            "X11": -0.081,
            "X12": -0.885,
            "X13": -0.256,
            "X14": 1.873,
            "X15": -0.802,
            "X16": -0.656,
            "X17": -0.376,
            "X18": -1.193,
            "X19": 0.436,
            "X20": -0.828,
            "X21": -1.402,
            "X22": 0.271,
            "X23": -0.423,
            "X24": -1.247,
            "X25": -2.601,
            "X26": 1.591,
            "X27": -0.345,
            "X28": -0.633,
            "X29": -0.615,
            "X30": -1.29,
            "X31": -0.957,
            "X32": 0.422,
            "X33": 0.076,
            "X34": -0.458,
            "X35": 1.103,
            "X36": 0.445,
            "X37": -0.555,
            "X38": -0.065,
            "X39": -0.965,
            "X40": -1.393,
            "X41": 1.054,
            "X42": -1.545,
            "X43": 0.954,
            "X44": -0.994,
            "X45": 0.446,
            "X46": 2.193,
            "X47": -0.374,
            "X48": -0.08,
            "X49": -0.115,
            "X50": -0.973,
            "X51": 0.165,
            "X52": 0.45,
            "X53": 1.476,
            "X54": -2.178,
            "X55": 0.659,
            "X56": 0.971,
            "X57": -0.257,
            "X58": -0.76,
            "X59": -0.259,
            "X60": 1.802,
            "X61": 0.492,
            "X62": -1.595,
            "X63": -0.138,
            "X64": 0.518,
            "X65": 0.292,
            "X66": -1.078,
            "X67": 0.141,
            "X68": -0.22,
            "X69": -1.826,
            "X70": 1.768,
            "X71": -0.426,
            "X72": -0.413,
            "X73": 0.977,
            "X74": -1.487,
            "X75": 0.372,
            "X76": 0.229,
            "X77": 0.359,
            "X78": -1.586,
            "X79": -0.9,
            "X80": -0.707,
            "X81": -0.834,
            "X82": 1.387,
            "X83": 0.447,
            "X84": -1.824,
            "X85": -0.159,
            "X86": 1.522,
            "X87": -1.751,
            "X88": -0.092,
            "X89": 1.307,
            "X90": 1.342,
            "X91": -0.266,
            "X92": -0.381,
            "X93": 1.194,
            "X94": 0.078,
            "X95": -0.961,
            "X96": -0.032,
            "X97": -1.002,
            "X98": -2.112,
            "X99": -0.515,
            "X100": 0.206,
            "X101": -2.036,
            "X102": 1.191,
            "X103": -1.062,
            "X104": 0.271,
            "X105": -0.421,
            "X106": -0.321,
            "X107": -0.593,
            "X108": 1.083,
            "X109": -0.256,
            "X110": 2.014,
            "X111": -1.108,
            "X112": 0.842,
            "X113": 0.323,
            "X114": -0.592,
            "X115": -0.452,
            "X116": 0.761,
            "X117": 0.614,
            "X118": 0.348,
            "X119": -0.482,
            "X120": 0.619,
            "X121": 1.532,
            "X122": 0.296,
            "X123": -0.744,
            "X124": 1.664,
            "X125": 2.164,
            "X126": 1.262,
            "X127": -0.536,
            "X128": -0.875,
            "X129": 0.764,
            "X130": -0.106,
            "X131": 1.447,
            "X132": -0.571,
            "X133": 0.345,
            "X134": -1.73,
            "X135": 0.949,
            "X136": -1.803,
            "X137": 0.071,
            "X138": -0.614,
            "X139": 1.612,
            "X140": 1.485,
            "X141": -0.381,
            "X142": 0.189,
            "X143": -1.418,
            "X144": 0.28,
            "X145": 2.53,
            "X146": -1.177,
            "X147": -0.321,
            "X148": 1.879,
            "X149": -0.166,
            "X150": -1.015,
            "X151": -1.257,
            "X152": 0.716,
            "X153": -1.453,
            "X154": 0.902,
            "X155": -0.457,
            "X156": 1.099,
            "X157": -0.996,
            "X158": 1.264,
            "X159": 0.522,
            "X160": -0.456,
            "X161": 1.475,
            "X162": 0.513,
            "X163": -0.658,
            "X164": 0.634,
            "X165": -1.049,
            "X166": 0.826,
            "X167": -0.37,
            "X168": -1.077,
            "X169": -0.095,
            "X170": -0.782,
            "X171": -0.824,
            "X172": 0.934,
            "X173": -0.567,
            "X174": 1.362,
            "X175": 2.22,
            "X176": 0.408,
            "X177": -0.447,
            "X178": 0.34,
            "X179": 1.68,
            "X180": -0.476,
            "X181": 0.259,
            "X182": -0.769,
            "X183": -1.967,
            "X184": -1.192,
            "X185": 0.347,
            "X186": -0.29,
            "X187": -0.666,
            "X188": -0.603,
            "X189": -1.834,
            "X190": 1.038,
            "X191": 0.648,
            "X192": -0.128,
            "X193": -1.424,
            "X194": 1.628,
            "X195": -0.989,
            "X196": -0.091,
            "X197": 0.356,
            "X198": -1.365,
            "X199": -0.361,
            "X200": 0.465,
            "X201": -0.064,
            "X202": -0.923,
            "X203": 0.628,
            "X204": -1.266,
            "X205": -0.191,
            "X206": 0.081,
            "X207": 0.719,
            "X208": 0.765,
            "X209": 0.392,
            "X210": 1.663,
            "X211": 0.467,
            "X212": 1.379,
            "X213": -0.579,
            "X214": -0.011,
            "X215": -2.094,
            "X216": -1.884,
            "X217": 0.619,
            "X218": -0.723,
            "X219": 0.36,
            "X220": 1.138,
            "X221": -1.235,
            "X222": -0.676,
            "X223": -1.136,
            "X224": -0.6,
            "X225": -0.173,
            "X226": 0.88,
            "X227": 0.4,
            "X228": 1.495,
            "X229": -0.532,
            "X230": 0.781,
            "X231": 0.561,
            "X232": 1.043,
            "X233": -0.102,
            "X234": 1.392,
            "X235": 1.902,
            "X236": 0.101,
            "X237": -1.235,
            "X238": 1.479,
            "X239": 1.28,
            "X240": -0.81,
            "X241": 0.728,
            "X242": 0.475,
            "X243": -0.953,
            "X244": 0.056,
            "X245": 1.736,
            "X246": 0.301,
            "X247": -0.399,
            "X248": -1.588,
            "X249": -0.7,
            "X250": -0.064,
            "X251": 0.305,
            "X252": -0.852,
            "X253": -1.462,
            "X254": -0.349,
            "X255": 0.534,
            "X256": 0.501,
            "X257": 0.821,
            "X258": -0.885,
            "X259": -0.883,
            "X260": 0.675,
            "X261": -0.376,
            "X262": 0.423,
            "X263": -0.153,
            "X264": 0.17,
            "X265": 0.926,
            "X266": -0.161,
            "X267": 1.858,
            "X268": -0.834,
            "X269": -0.702,
            "X270": 1.171,
            "X271": 0.739,
            "X272": -0.634,
            "X273": 0.314,
            "X274": 0.286,
            "X275": 0.964,
            "X276": 0.32,
            "X277": -0.629,
            "X278": -0.583,
            "X279": -0.389,
            "X280": -1.027,
            "X281": -0.01,
            "X282": -0.293,
            "X283": -0.267,
            "X284": 0.643,
            "X285": -0.699,
            "X286": -0.562,
            "X287": -0.853,
            "X288": -0.553,
            "X289": -0.056,
            "X290": 1.142,
            "X291": 0.354,
            "X292": 0.885,
            "X293": 0.276,
            "X294": -0.931,
            "X295": 0.78,
            "X296": 0.168,
            "X297": 1.083,
            "X298": 0.083,
            "X299": 0.066
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6953,
            ".pred_1": 0.3047,
            "id": 16,
            "X0": -0.014,
            "X1": -2.094,
            "X2": 1.406,
            "X3": -0.187,
            "X4": -1.027,
            "X5": -0.906,
            "X6": -0.906,
            "X7": 0.824,
            "X8": -0.272,
            "X9": -0.785,
            "X10": 0.288,
            "X11": 1.468,
            "X12": -2.358,
            "X13": -0.026,
            "X14": 0.377,
            "X15": 0.344,
            "X16": -0.586,
            "X17": -0.47,
            "X18": 0.993,
            "X19": 0.105,
            "X20": 1.379,
            "X21": 0.878,
            "X22": -0.485,
            "X23": -0.132,
            "X24": -0.303,
            "X25": -0.761,
            "X26": 0.222,
            "X27": 0.901,
            "X28": -0.597,
            "X29": -0.146,
            "X30": 0.834,
            "X31": 1.171,
            "X32": -0.483,
            "X33": 0.76,
            "X34": -0.235,
            "X35": 0.602,
            "X36": -0.955,
            "X37": 1.695,
            "X38": 1.123,
            "X39": 0.529,
            "X40": 0.101,
            "X41": -1.078,
            "X42": -2.575,
            "X43": 0.24,
            "X44": -0.85,
            "X45": 0.302,
            "X46": -1.09,
            "X47": 0.194,
            "X48": 1.648,
            "X49": 2.425,
            "X50": 1.939,
            "X51": -2.001,
            "X52": 0.647,
            "X53": 0.93,
            "X54": -1.431,
            "X55": 0.789,
            "X56": 0.517,
            "X57": -0.081,
            "X58": 0.216,
            "X59": -0.594,
            "X60": -0.694,
            "X61": -0.118,
            "X62": -0.567,
            "X63": -0.106,
            "X64": 0.142,
            "X65": 0.966,
            "X66": -0.504,
            "X67": -0.71,
            "X68": -1.721,
            "X69": -0.105,
            "X70": -0.175,
            "X71": -1.269,
            "X72": 0.371,
            "X73": 0.164,
            "X74": 0.067,
            "X75": -0.275,
            "X76": 2.385,
            "X77": -1.2,
            "X78": 0.34,
            "X79": 0.533,
            "X80": -0.233,
            "X81": 0.183,
            "X82": -0.628,
            "X83": 0.161,
            "X84": 0.101,
            "X85": 0.583,
            "X86": 0.281,
            "X87": -1.051,
            "X88": 0.095,
            "X89": 0.125,
            "X90": 0.065,
            "X91": -0.598,
            "X92": 0.678,
            "X93": 0.505,
            "X94": -0.849,
            "X95": -0.614,
            "X96": -0.409,
            "X97": -0.11,
            "X98": -0.314,
            "X99": -0.157,
            "X100": 0.275,
            "X101": 0.228,
            "X102": -0.123,
            "X103": 0.117,
            "X104": -0.261,
            "X105": 0.255,
            "X106": 0.433,
            "X107": -0.66,
            "X108": -1.674,
            "X109": 0.19,
            "X110": 0.212,
            "X111": 1.52,
            "X112": -0.316,
            "X113": 1.35,
            "X114": -0.804,
            "X115": 0.973,
            "X116": -0.578,
            "X117": -1.294,
            "X118": -1.466,
            "X119": 0.179,
            "X120": -0.188,
            "X121": -0.003,
            "X122": 0.22,
            "X123": -0.384,
            "X124": -2.442,
            "X125": -1.324,
            "X126": 0.572,
            "X127": 0.654,
            "X128": 2.039,
            "X129": 0.919,
            "X130": 1.03,
            "X131": 0.636,
            "X132": -0.226,
            "X133": 0.053,
            "X134": 0.55,
            "X135": 0.149,
            "X136": 0.171,
            "X137": -0.445,
            "X138": 0.703,
            "X139": 0.381,
            "X140": -0.896,
            "X141": 1.157,
            "X142": -1.262,
            "X143": -1.205,
            "X144": 0.28,
            "X145": -1.28,
            "X146": -1.405,
            "X147": -1.608,
            "X148": 0.195,
            "X149": -0.35,
            "X150": -1.289,
            "X151": 0.451,
            "X152": 1.438,
            "X153": 1.264,
            "X154": -1.278,
            "X155": -0.037,
            "X156": -0.838,
            "X157": -0.099,
            "X158": -0.362,
            "X159": 0.444,
            "X160": -0.419,
            "X161": -0.601,
            "X162": -0.492,
            "X163": 0.793,
            "X164": 1.194,
            "X165": -1.199,
            "X166": 0.885,
            "X167": 0.115,
            "X168": -1.817,
            "X169": -1.082,
            "X170": -0.686,
            "X171": 0.041,
            "X172": -1.571,
            "X173": 1.147,
            "X174": 1.629,
            "X175": -0.48,
            "X176": 0.858,
            "X177": -1.034,
            "X178": 0.78,
            "X179": 0.444,
            "X180": -0.01,
            "X181": -0.27,
            "X182": -0.072,
            "X183": -1.324,
            "X184": -2.681,
            "X185": -0.514,
            "X186": 0.342,
            "X187": -0.843,
            "X188": -0.79,
            "X189": -0.404,
            "X190": 0.552,
            "X191": 0.179,
            "X192": 0.65,
            "X193": 0.775,
            "X194": 1.909,
            "X195": 1.036,
            "X196": -0.539,
            "X197": -1.035,
            "X198": 0.485,
            "X199": -0.192,
            "X200": 0.585,
            "X201": -0.167,
            "X202": 1.497,
            "X203": 1.679,
            "X204": -1.573,
            "X205": -0.897,
            "X206": 0.501,
            "X207": -0.556,
            "X208": 1.016,
            "X209": -1.26,
            "X210": -0.065,
            "X211": -1.512,
            "X212": -1.012,
            "X213": -0.389,
            "X214": 0.286,
            "X215": -1.453,
            "X216": -0.855,
            "X217": -0.075,
            "X218": 0.935,
            "X219": -0.262,
            "X220": -1.17,
            "X221": -0.014,
            "X222": 0.582,
            "X223": 0.295,
            "X224": -0.728,
            "X225": 0.812,
            "X226": -0.325,
            "X227": -0.092,
            "X228": -0.345,
            "X229": 1.858,
            "X230": 0.165,
            "X231": -0.306,
            "X232": -0.723,
            "X233": 0.321,
            "X234": -1.037,
            "X235": -0.801,
            "X236": -0.226,
            "X237": 0.738,
            "X238": -1.638,
            "X239": -1.192,
            "X240": -0.317,
            "X241": 0.559,
            "X242": -2.067,
            "X243": -1.121,
            "X244": 0.654,
            "X245": 0.767,
            "X246": -1.646,
            "X247": 0.318,
            "X248": 0.675,
            "X249": 1.82,
            "X250": 0.388,
            "X251": 0.249,
            "X252": 0.776,
            "X253": 0.299,
            "X254": -0.423,
            "X255": -0.4,
            "X256": 0.815,
            "X257": -0.545,
            "X258": 1.543,
            "X259": -1.588,
            "X260": 0.026,
            "X261": -0.41,
            "X262": 0.604,
            "X263": -2.195,
            "X264": 0.552,
            "X265": -0.337,
            "X266": -1.106,
            "X267": 0.793,
            "X268": -1.025,
            "X269": -0.387,
            "X270": 1.005,
            "X271": 1.444,
            "X272": -0.133,
            "X273": 2.148,
            "X274": -1.136,
            "X275": 0.552,
            "X276": 0.288,
            "X277": 0.115,
            "X278": 0.239,
            "X279": 1.195,
            "X280": -1.201,
            "X281": -0.235,
            "X282": 0.221,
            "X283": -0.303,
            "X284": 1.425,
            "X285": 0.155,
            "X286": 0.648,
            "X287": -0.257,
            "X288": 0.346,
            "X289": 1.129,
            "X290": 2.546,
            "X291": -1.121,
            "X292": 0.746,
            "X293": 0.013,
            "X294": -0.828,
            "X295": 1.304,
            "X296": 0.18,
            "X297": -0.274,
            "X298": 1.624,
            "X299": -0.236
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.7021,
            ".pred_1": 0.2979,
            "id": 146,
            "X0": -1.179,
            "X1": -0.636,
            "X2": 0.702,
            "X3": -1.057,
            "X4": 0.647,
            "X5": 0.368,
            "X6": -0.425,
            "X7": -0.138,
            "X8": -0.007,
            "X9": 0.994,
            "X10": -1.018,
            "X11": -1.34,
            "X12": -1.193,
            "X13": -1.285,
            "X14": 0.815,
            "X15": 1.065,
            "X16": 0.37,
            "X17": -1.682,
            "X18": 1.22,
            "X19": 3.192,
            "X20": -2.206,
            "X21": 1.157,
            "X22": -0.436,
            "X23": 0.691,
            "X24": 0.608,
            "X25": 0.302,
            "X26": 0.222,
            "X27": 0.839,
            "X28": 1.146,
            "X29": -0.605,
            "X30": -0.907,
            "X31": -0.356,
            "X32": -0.116,
            "X33": -0.855,
            "X34": -0.333,
            "X35": -1.667,
            "X36": -0.349,
            "X37": 0.756,
            "X38": 1.574,
            "X39": -1.181,
            "X40": -1.21,
            "X41": 0.266,
            "X42": -0.331,
            "X43": -0.438,
            "X44": -0.966,
            "X45": -0.227,
            "X46": 0.306,
            "X47": 1.723,
            "X48": 0.247,
            "X49": 1.037,
            "X50": 0.515,
            "X51": -0.411,
            "X52": -0.445,
            "X53": 0.793,
            "X54": 0.449,
            "X55": -0.541,
            "X56": -0.097,
            "X57": -0.505,
            "X58": -1.874,
            "X59": -0.607,
            "X60": 0.562,
            "X61": -0.822,
            "X62": -0.288,
            "X63": -0.346,
            "X64": -0.864,
            "X65": 0.337,
            "X66": -0.877,
            "X67": 0.446,
            "X68": 1.372,
            "X69": 1.156,
            "X70": -0.778,
            "X71": 0.535,
            "X72": 0.383,
            "X73": 1.463,
            "X74": -2.165,
            "X75": 0.53,
            "X76": 0.823,
            "X77": 0.058,
            "X78": 1.061,
            "X79": 1.047,
            "X80": -0.115,
            "X81": 1.654,
            "X82": 1.485,
            "X83": 2.038,
            "X84": 0.228,
            "X85": -1.198,
            "X86": 0.829,
            "X87": -1.2,
            "X88": 2.418,
            "X89": 0.644,
            "X90": 0.412,
            "X91": -0.76,
            "X92": 0.298,
            "X93": -0.206,
            "X94": 0.891,
            "X95": -1.799,
            "X96": 0.042,
            "X97": -0.057,
            "X98": 0.097,
            "X99": 0.661,
            "X100": -0.007,
            "X101": 1.931,
            "X102": 1.181,
            "X103": 0.298,
            "X104": -0.549,
            "X105": -0.99,
            "X106": -0.737,
            "X107": -0.523,
            "X108": 0.478,
            "X109": 0.35,
            "X110": -0.271,
            "X111": -0.043,
            "X112": -0.242,
            "X113": -1.442,
            "X114": 1.177,
            "X115": 1.402,
            "X116": 0.416,
            "X117": -1.637,
            "X118": 0.967,
            "X119": 0.75,
            "X120": -0.652,
            "X121": 1.472,
            "X122": -0.854,
            "X123": 0.78,
            "X124": -2.125,
            "X125": 0.736,
            "X126": -0.486,
            "X127": -0.165,
            "X128": -1.073,
            "X129": -0.379,
            "X130": -0.534,
            "X131": -0.791,
            "X132": -1.618,
            "X133": -0.585,
            "X134": -0.278,
            "X135": 0.061,
            "X136": -1.511,
            "X137": 0.785,
            "X138": 1.497,
            "X139": -0.097,
            "X140": 0.412,
            "X141": -0.326,
            "X142": 0.697,
            "X143": -1.684,
            "X144": 1.365,
            "X145": 0.155,
            "X146": -0.554,
            "X147": 0.735,
            "X148": 0.494,
            "X149": 0.968,
            "X150": -0.179,
            "X151": -2.687,
            "X152": 0.408,
            "X153": -1.352,
            "X154": -0.556,
            "X155": 0.263,
            "X156": 1.091,
            "X157": 0.268,
            "X158": -1.75,
            "X159": 0.101,
            "X160": -1.901,
            "X161": 0.147,
            "X162": 0.824,
            "X163": -0.234,
            "X164": 1.723,
            "X165": 2.495,
            "X166": -0.048,
            "X167": 1.623,
            "X168": 0.389,
            "X169": 0.099,
            "X170": 1.087,
            "X171": -1.193,
            "X172": 0.141,
            "X173": -3.105,
            "X174": -0.254,
            "X175": 0.053,
            "X176": -0.127,
            "X177": -0.585,
            "X178": -1.065,
            "X179": 0.347,
            "X180": 0.499,
            "X181": -1.125,
            "X182": -0.296,
            "X183": -0.25,
            "X184": -2.34,
            "X185": -1.201,
            "X186": 1.389,
            "X187": 0.572,
            "X188": 1.365,
            "X189": -1.017,
            "X190": 0.309,
            "X191": 0.352,
            "X192": 0.232,
            "X193": -0.096,
            "X194": 0.217,
            "X195": -0.252,
            "X196": 0.218,
            "X197": -0.955,
            "X198": 0.504,
            "X199": 1.368,
            "X200": -0.412,
            "X201": -2.156,
            "X202": 0.4,
            "X203": 0.561,
            "X204": -0.624,
            "X205": 0.005,
            "X206": -0.44,
            "X207": -1.058,
            "X208": -0.913,
            "X209": 0.533,
            "X210": -0.935,
            "X211": -0.047,
            "X212": -1.065,
            "X213": 0.596,
            "X214": 1.604,
            "X215": 0.702,
            "X216": 0.544,
            "X217": -0.153,
            "X218": 1.153,
            "X219": 1.058,
            "X220": 0.489,
            "X221": -0.81,
            "X222": 1.979,
            "X223": 0.777,
            "X224": -0.483,
            "X225": -0.631,
            "X226": 0.383,
            "X227": -1.195,
            "X228": -0.768,
            "X229": -0.331,
            "X230": 1.203,
            "X231": -0.503,
            "X232": 0.283,
            "X233": -0.996,
            "X234": -0.996,
            "X235": -1.087,
            "X236": -1.354,
            "X237": 0.514,
            "X238": -0.241,
            "X239": -0.43,
            "X240": -1.835,
            "X241": -1.229,
            "X242": 0.607,
            "X243": -0.642,
            "X244": -0.759,
            "X245": 0.588,
            "X246": 1.243,
            "X247": 0.851,
            "X248": 2.258,
            "X249": 0.089,
            "X250": -0.19,
            "X251": 0.221,
            "X252": 0.908,
            "X253": -1.984,
            "X254": -0.823,
            "X255": -1.474,
            "X256": -0.478,
            "X257": -0.24,
            "X258": 0.526,
            "X259": 0.323,
            "X260": -0.229,
            "X261": -0.031,
            "X262": -0.44,
            "X263": 0.826,
            "X264": -1.398,
            "X265": -0.648,
            "X266": 0.393,
            "X267": -1.724,
            "X268": -1,
            "X269": -0.33,
            "X270": 1.591,
            "X271": -0.491,
            "X272": -0.056,
            "X273": 1.1,
            "X274": -0.117,
            "X275": -0.433,
            "X276": 0.419,
            "X277": -1,
            "X278": 0.152,
            "X279": 2.093,
            "X280": -1.007,
            "X281": 0.108,
            "X282": 0.945,
            "X283": 0.4,
            "X284": -1.361,
            "X285": 0.539,
            "X286": 1.286,
            "X287": -0.189,
            "X288": -0.732,
            "X289": -0.853,
            "X290": 0.042,
            "X291": 0.928,
            "X292": -1.073,
            "X293": 1.116,
            "X294": 2.194,
            "X295": 0.227,
            "X296": 0.155,
            "X297": 0.675,
            "X298": 0.148,
            "X299": 0.338
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.8595,
            ".pred_1": 0.1405,
            "id": 69,
            "X0": 0.165,
            "X1": 0.086,
            "X2": -0.661,
            "X3": 1.604,
            "X4": 0.057,
            "X5": -0.684,
            "X6": 0.896,
            "X7": -0.625,
            "X8": 0.079,
            "X9": -0.115,
            "X10": -0.319,
            "X11": -0.311,
            "X12": 0.258,
            "X13": -1.274,
            "X14": -0.563,
            "X15": -0.142,
            "X16": 0.133,
            "X17": 0.678,
            "X18": 0.11,
            "X19": -2.048,
            "X20": 1.717,
            "X21": 0.203,
            "X22": 1.182,
            "X23": 0.39,
            "X24": 1.427,
            "X25": 1.449,
            "X26": 0.318,
            "X27": -1.6,
            "X28": -0.379,
            "X29": 1.621,
            "X30": 1.251,
            "X31": 0.391,
            "X32": 2.81,
            "X33": -1.882,
            "X34": 1.142,
            "X35": -0.061,
            "X36": -0.167,
            "X37": -0.688,
            "X38": 1.615,
            "X39": 0.96,
            "X40": 0.446,
            "X41": -0.558,
            "X42": -0.811,
            "X43": -1.152,
            "X44": -0.55,
            "X45": -0.092,
            "X46": 0.04,
            "X47": 0.102,
            "X48": 1.605,
            "X49": 0.575,
            "X50": -0.809,
            "X51": -0.411,
            "X52": -0.622,
            "X53": -0.551,
            "X54": 0.551,
            "X55": -0.837,
            "X56": 0.852,
            "X57": -1.199,
            "X58": -0.536,
            "X59": 0.777,
            "X60": -0.128,
            "X61": 0.296,
            "X62": -2.005,
            "X63": -1.384,
            "X64": -0.182,
            "X65": -0.216,
            "X66": 1.931,
            "X67": -1.792,
            "X68": 1.037,
            "X69": 1.503,
            "X70": -0.289,
            "X71": 0.808,
            "X72": -1.075,
            "X73": -0.377,
            "X74": -0.131,
            "X75": -0.905,
            "X76": 0.985,
            "X77": 1.718,
            "X78": -0.394,
            "X79": -0.397,
            "X80": 1.079,
            "X81": 1.351,
            "X82": -1.055,
            "X83": -0.025,
            "X84": 0.846,
            "X85": 0.546,
            "X86": 0.986,
            "X87": 0.918,
            "X88": -2.68,
            "X89": -0.118,
            "X90": 0.444,
            "X91": 0.275,
            "X92": -1.46,
            "X93": 0.402,
            "X94": -0.31,
            "X95": -0.238,
            "X96": -0.949,
            "X97": -0.858,
            "X98": -0.582,
            "X99": 1.725,
            "X100": -0.385,
            "X101": 0.077,
            "X102": -0.38,
            "X103": 1.733,
            "X104": 1.186,
            "X105": -0.379,
            "X106": -1.062,
            "X107": -1.369,
            "X108": 1.11,
            "X109": -0.109,
            "X110": -0.288,
            "X111": -0.555,
            "X112": -0.548,
            "X113": -1.064,
            "X114": -0.314,
            "X115": -0.188,
            "X116": -2.424,
            "X117": -1.081,
            "X118": -0.568,
            "X119": 0.737,
            "X120": -0.079,
            "X121": 0.655,
            "X122": 1.263,
            "X123": 0.017,
            "X124": -0.897,
            "X125": 1.661,
            "X126": -0.045,
            "X127": -0.684,
            "X128": 0.997,
            "X129": 1.547,
            "X130": 0.318,
            "X131": 1.555,
            "X132": 0.373,
            "X133": 0.939,
            "X134": 0.125,
            "X135": 2.897,
            "X136": -0.613,
            "X137": 0.382,
            "X138": 0.476,
            "X139": 0.027,
            "X140": -0.651,
            "X141": -1.345,
            "X142": -0.438,
            "X143": 0.075,
            "X144": -0.812,
            "X145": -0.114,
            "X146": 0.168,
            "X147": -1.344,
            "X148": 0.806,
            "X149": -1.058,
            "X150": 0.126,
            "X151": 2.277,
            "X152": -0.389,
            "X153": -0.19,
            "X154": 0.962,
            "X155": -1.347,
            "X156": 1.234,
            "X157": -0.506,
            "X158": -0.223,
            "X159": -0.944,
            "X160": -1.008,
            "X161": 0.637,
            "X162": 0.606,
            "X163": -0.67,
            "X164": -1.551,
            "X165": -2.118,
            "X166": 0.55,
            "X167": -0.805,
            "X168": -1.994,
            "X169": 0.226,
            "X170": 1.688,
            "X171": -0.465,
            "X172": 0.093,
            "X173": -0.174,
            "X174": 0.782,
            "X175": -0.058,
            "X176": 2.094,
            "X177": -1.397,
            "X178": 0.046,
            "X179": -1.299,
            "X180": 0.642,
            "X181": -0.989,
            "X182": -0.449,
            "X183": 1.086,
            "X184": 0.115,
            "X185": 0.183,
            "X186": -0.642,
            "X187": -0.623,
            "X188": -0.706,
            "X189": 0.044,
            "X190": -0.057,
            "X191": -0.949,
            "X192": 0.147,
            "X193": 0.061,
            "X194": -2.139,
            "X195": 1.341,
            "X196": -0.069,
            "X197": -0.749,
            "X198": 0.746,
            "X199": -1.284,
            "X200": -1.384,
            "X201": 0.595,
            "X202": -1.815,
            "X203": -0.506,
            "X204": -0.07,
            "X205": 0.603,
            "X206": 1.334,
            "X207": 0.615,
            "X208": 1.31,
            "X209": 0.314,
            "X210": 0.086,
            "X211": 1.126,
            "X212": 0.242,
            "X213": -1.608,
            "X214": 0.246,
            "X215": -0.24,
            "X216": 0.716,
            "X217": 0.228,
            "X218": -0.184,
            "X219": -0.161,
            "X220": 0.339,
            "X221": 0.113,
            "X222": -0.499,
            "X223": 0.508,
            "X224": 0.617,
            "X225": 0.046,
            "X226": -0.335,
            "X227": 0.513,
            "X228": 1.548,
            "X229": 0.87,
            "X230": -0.699,
            "X231": -0.176,
            "X232": 0.874,
            "X233": 0.202,
            "X234": 0.784,
            "X235": -2.277,
            "X236": 0.495,
            "X237": -0.257,
            "X238": 1.508,
            "X239": -0.418,
            "X240": -0.721,
            "X241": 0.015,
            "X242": -0.141,
            "X243": -0.861,
            "X244": -1.552,
            "X245": -0.582,
            "X246": -1.459,
            "X247": -0.069,
            "X248": 0.031,
            "X249": -1.581,
            "X250": 0.028,
            "X251": 0.532,
            "X252": 1.074,
            "X253": -0.485,
            "X254": 0.235,
            "X255": -0.875,
            "X256": 1.005,
            "X257": -1.609,
            "X258": -0.9,
            "X259": 0.465,
            "X260": -0.867,
            "X261": 0.384,
            "X262": 2.275,
            "X263": 0.926,
            "X264": 0.016,
            "X265": -0.136,
            "X266": -1.956,
            "X267": 0.284,
            "X268": -0.595,
            "X269": 0.413,
            "X270": -0.666,
            "X271": -0.842,
            "X272": 0.262,
            "X273": 0.179,
            "X274": 1.031,
            "X275": 0.703,
            "X276": -1.068,
            "X277": 2,
            "X278": -0.69,
            "X279": 0.778,
            "X280": -0.064,
            "X281": -0.258,
            "X282": -0.356,
            "X283": -1.162,
            "X284": 0.183,
            "X285": 1.287,
            "X286": 1.882,
            "X287": 1.1,
            "X288": 2.001,
            "X289": 2.473,
            "X290": -0.729,
            "X291": 1.791,
            "X292": 0.718,
            "X293": 1.837,
            "X294": 0.153,
            "X295": -0.686,
            "X296": 0.819,
            "X297": -0.479,
            "X298": -1.129,
            "X299": 0.928
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6873,
            ".pred_1": 0.3127,
            "id": 173,
            "X0": -0.754,
            "X1": -0.114,
            "X2": 0.719,
            "X3": 1.613,
            "X4": -0.975,
            "X5": 0.556,
            "X6": 0.815,
            "X7": -1.882,
            "X8": -2.226,
            "X9": 0.069,
            "X10": 0.259,
            "X11": -1.002,
            "X12": -0.151,
            "X13": -0.777,
            "X14": 0.19,
            "X15": 0.116,
            "X16": 1.164,
            "X17": -1.031,
            "X18": -0.249,
            "X19": -1.643,
            "X20": -1.03,
            "X21": 1.701,
            "X22": 1.319,
            "X23": -1.416,
            "X24": -1.364,
            "X25": 0.034,
            "X26": -0.405,
            "X27": -1.649,
            "X28": -1.135,
            "X29": -0.227,
            "X30": 0.219,
            "X31": 0.612,
            "X32": -0.396,
            "X33": 0.583,
            "X34": -0.576,
            "X35": 0.012,
            "X36": -0.459,
            "X37": -0.672,
            "X38": 0.644,
            "X39": 0.306,
            "X40": 0.567,
            "X41": 1.24,
            "X42": 0.496,
            "X43": -1.5,
            "X44": 0.727,
            "X45": -1.235,
            "X46": -0.965,
            "X47": 0.542,
            "X48": -0.353,
            "X49": 1.138,
            "X50": 0.225,
            "X51": 0.631,
            "X52": -0.422,
            "X53": 0.029,
            "X54": -0.342,
            "X55": -2.359,
            "X56": 0.886,
            "X57": -0.032,
            "X58": 0.485,
            "X59": 0.544,
            "X60": 0.022,
            "X61": -1.667,
            "X62": -2.048,
            "X63": -0.493,
            "X64": -0.091,
            "X65": -0.686,
            "X66": 0.916,
            "X67": -1.812,
            "X68": -0.276,
            "X69": 0.228,
            "X70": 0.736,
            "X71": -1.283,
            "X72": 0.556,
            "X73": 1.678,
            "X74": 0.149,
            "X75": -0.404,
            "X76": 1.637,
            "X77": -0.291,
            "X78": -0.056,
            "X79": 0.582,
            "X80": -0.558,
            "X81": -1.463,
            "X82": 1.674,
            "X83": 0.058,
            "X84": 0.769,
            "X85": 0.4,
            "X86": 0.346,
            "X87": -0.775,
            "X88": -1.136,
            "X89": 0.39,
            "X90": 1.108,
            "X91": -2.01,
            "X92": -1.175,
            "X93": -0.291,
            "X94": 0.29,
            "X95": 0.171,
            "X96": -0.139,
            "X97": 0.664,
            "X98": 0.75,
            "X99": -1.869,
            "X100": 1.598,
            "X101": -1.317,
            "X102": -0.511,
            "X103": -0.906,
            "X104": -0.635,
            "X105": 0.788,
            "X106": -0.413,
            "X107": 0.67,
            "X108": 1.842,
            "X109": 1.312,
            "X110": -2.579,
            "X111": 0.517,
            "X112": -0.21,
            "X113": -0.522,
            "X114": -0.524,
            "X115": 0.629,
            "X116": -2.306,
            "X117": -0.709,
            "X118": 0.887,
            "X119": -0.967,
            "X120": 0.518,
            "X121": -0.038,
            "X122": 0.138,
            "X123": -0.717,
            "X124": -0.867,
            "X125": 0.647,
            "X126": 0.14,
            "X127": 1.197,
            "X128": -0.282,
            "X129": -1.213,
            "X130": -0.98,
            "X131": -0.109,
            "X132": 0.432,
            "X133": 0.114,
            "X134": 0.299,
            "X135": -1.948,
            "X136": -1.502,
            "X137": -1.078,
            "X138": -0.421,
            "X139": -0.663,
            "X140": 0.437,
            "X141": -0.272,
            "X142": 1.74,
            "X143": -1.287,
            "X144": 2.358,
            "X145": -0.03,
            "X146": -1.562,
            "X147": -0.028,
            "X148": 1.822,
            "X149": 1.237,
            "X150": 0.281,
            "X151": 0.08,
            "X152": 2.832,
            "X153": -2.98,
            "X154": -0.192,
            "X155": -1.382,
            "X156": 1.66,
            "X157": 0.158,
            "X158": 0.446,
            "X159": 1.137,
            "X160": -0.489,
            "X161": 1.157,
            "X162": -1.054,
            "X163": -0.395,
            "X164": -0.485,
            "X165": 1.651,
            "X166": -0.57,
            "X167": -1.05,
            "X168": 1.357,
            "X169": 0.408,
            "X170": -1.3,
            "X171": -0.411,
            "X172": -0.697,
            "X173": -0.097,
            "X174": 0.46,
            "X175": 0.131,
            "X176": 1.113,
            "X177": -0.638,
            "X178": 0.889,
            "X179": 0.8,
            "X180": -0.819,
            "X181": -0.934,
            "X182": 0.523,
            "X183": -1.341,
            "X184": 0.026,
            "X185": -1.425,
            "X186": 0.055,
            "X187": 1.508,
            "X188": -0.657,
            "X189": -0.25,
            "X190": 0.036,
            "X191": -0.116,
            "X192": -0.858,
            "X193": -0.343,
            "X194": -0.13,
            "X195": 1.48,
            "X196": 1.494,
            "X197": 0.2,
            "X198": 0.897,
            "X199": 0.55,
            "X200": -0.253,
            "X201": -0.091,
            "X202": -0.91,
            "X203": -0.321,
            "X204": 0.324,
            "X205": 0.631,
            "X206": 0.447,
            "X207": -0.336,
            "X208": -0.145,
            "X209": -1.208,
            "X210": 1.21,
            "X211": -0.631,
            "X212": 0.362,
            "X213": -0.903,
            "X214": 0.637,
            "X215": -0.009,
            "X216": 0.49,
            "X217": -0.061,
            "X218": 0.394,
            "X219": 1.172,
            "X220": -1.389,
            "X221": 0.602,
            "X222": 0.033,
            "X223": -1.576,
            "X224": 0.074,
            "X225": 0.777,
            "X226": 0.874,
            "X227": -0.058,
            "X228": 1.578,
            "X229": 0.305,
            "X230": -0.199,
            "X231": 1.212,
            "X232": 2.336,
            "X233": 0.256,
            "X234": 1.359,
            "X235": -0.622,
            "X236": -0.391,
            "X237": -0.555,
            "X238": -0.671,
            "X239": -0.132,
            "X240": 0.295,
            "X241": 0.784,
            "X242": -0.026,
            "X243": -0.496,
            "X244": 1.338,
            "X245": -0.824,
            "X246": -0.648,
            "X247": 0.424,
            "X248": 1.816,
            "X249": -0.609,
            "X250": 0.491,
            "X251": 0.277,
            "X252": -1.168,
            "X253": 0.044,
            "X254": -0.789,
            "X255": -0.339,
            "X256": 1.316,
            "X257": -1.709,
            "X258": 0.638,
            "X259": -0.843,
            "X260": -1.974,
            "X261": -0.539,
            "X262": -0.848,
            "X263": -0.27,
            "X264": -1.122,
            "X265": 0.076,
            "X266": 0.368,
            "X267": -0.672,
            "X268": 0.512,
            "X269": 0.925,
            "X270": 1.874,
            "X271": 0.827,
            "X272": -1.096,
            "X273": -0.89,
            "X274": -1.129,
            "X275": 0.331,
            "X276": -0.317,
            "X277": -0.323,
            "X278": 0.223,
            "X279": 1.359,
            "X280": 1.147,
            "X281": -0.627,
            "X282": 2.011,
            "X283": -0.719,
            "X284": -0.129,
            "X285": -0.869,
            "X286": 1.625,
            "X287": -0.313,
            "X288": -1.428,
            "X289": 0.032,
            "X290": -0.504,
            "X291": -0.984,
            "X292": 0.662,
            "X293": -0.135,
            "X294": -0.885,
            "X295": 1.47,
            "X296": -1.554,
            "X297": -1.638,
            "X298": -0.491,
            "X299": -0.592
          },
          {
            ".pred_class": 0,
            ".pred_0": 0.6529,
            ".pred_1": 0.3471,
            "id": 38,
            "X0": 0.024,
            "X1": -1.26,
            "X2": -0.508,
            "X3": -0.068,
            "X4": -0.149,
            "X5": 2.41,
            "X6": 0.963,
            "X7": -0.229,
            "X8": -0.378,
            "X9": -0.878,
            "X10": -2.341,
            "X11": -0.127,
            "X12": 2.189,
            "X13": 0.576,
            "X14": -0.085,
            "X15": -1.744,
            "X16": -1.162,
            "X17": -1.175,
            "X18": 0.505,
            "X19": -0.174,
            "X20": -0.979,
            "X21": -0.624,
            "X22": -0.039,
            "X23": -1.18,
            "X24": -0.158,
            "X25": -0.594,
            "X26": 0.431,
            "X27": 0.56,
            "X28": -0.235,
            "X29": 0.745,
            "X30": 0.435,
            "X31": -0.583,
            "X32": -0.122,
            "X33": 0.534,
            "X34": -0.782,
            "X35": 1.067,
            "X36": 0.465,
            "X37": -0.395,
            "X38": -0.199,
            "X39": 1.217,
            "X40": -1.028,
            "X41": -0.993,
            "X42": 0.514,
            "X43": -1.373,
            "X44": -1.273,
            "X45": 0.75,
            "X46": 1.398,
            "X47": -0.351,
            "X48": -0.453,
            "X49": -0.398,
            "X50": 0.214,
            "X51": 0.714,
            "X52": 1.349,
            "X53": -2.353,
            "X54": 0.799,
            "X55": -1.082,
            "X56": 1.349,
            "X57": 0.43,
            "X58": -2.068,
            "X59": 0.952,
            "X60": 0.06,
            "X61": -0.582,
            "X62": 0.84,
            "X63": 0.933,
            "X64": -0.423,
            "X65": 0.64,
            "X66": 0.746,
            "X67": -0.151,
            "X68": 1,
            "X69": 0.109,
            "X70": 0.277,
            "X71": -0.031,
            "X72": -0.131,
            "X73": 1.173,
            "X74": 0.081,
            "X75": -1.293,
            "X76": 0.507,
            "X77": -2.83,
            "X78": 0.202,
            "X79": 0.108,
            "X80": 0.452,
            "X81": 0.224,
            "X82": 1.156,
            "X83": 0.382,
            "X84": 0.001,
            "X85": 0.172,
            "X86": 0.118,
            "X87": -0.187,
            "X88": -0.071,
            "X89": -0.14,
            "X90": 0.162,
            "X91": 0.74,
            "X92": 1.282,
            "X93": 0.55,
            "X94": -0.233,
            "X95": -0.459,
            "X96": -0.224,
            "X97": -0.76,
            "X98": -1.867,
            "X99": 0.087,
            "X100": -0.713,
            "X101": -0.721,
            "X102": -0.016,
            "X103": 0.179,
            "X104": 0.455,
            "X105": -0.793,
            "X106": 1.739,
            "X107": 2.464,
            "X108": 0.522,
            "X109": -0.412,
            "X110": -0.536,
            "X111": -0.495,
            "X112": 0.975,
            "X113": -0.853,
            "X114": 0.206,
            "X115": 0.791,
            "X116": -1.725,
            "X117": -1.473,
            "X118": 2.478,
            "X119": 0.847,
            "X120": 0.416,
            "X121": -0.105,
            "X122": -0.091,
            "X123": 0.641,
            "X124": 1.19,
            "X125": -0.456,
            "X126": 0.598,
            "X127": -1.394,
            "X128": 1.031,
            "X129": 1.139,
            "X130": -0.336,
            "X131": -0.24,
            "X132": 0.718,
            "X133": -0.555,
            "X134": -0.059,
            "X135": -0.097,
            "X136": -0.476,
            "X137": 1.229,
            "X138": 1.619,
            "X139": -0.509,
            "X140": -1.077,
            "X141": -0.806,
            "X142": -0.814,
            "X143": -0.441,
            "X144": 0.947,
            "X145": -1.204,
            "X146": -0.858,
            "X147": -0.652,
            "X148": -0.637,
            "X149": -2.04,
            "X150": -0.019,
            "X151": -0.175,
            "X152": 0.077,
            "X153": -0.073,
            "X154": -1.213,
            "X155": 1.287,
            "X156": 0.646,
            "X157": 0.734,
            "X158": 0.925,
            "X159": -1.11,
            "X160": 1.531,
            "X161": -1.226,
            "X162": 1.938,
            "X163": 1,
            "X164": 0.362,
            "X165": 1.344,
            "X166": -0.892,
            "X167": -0.285,
            "X168": 0.36,
            "X169": -0.244,
            "X170": 1.574,
            "X171": 0.712,
            "X172": -0.926,
            "X173": 0.561,
            "X174": -1.527,
            "X175": -2.475,
            "X176": 0.213,
            "X177": 1.565,
            "X178": 0.265,
            "X179": 1.526,
            "X180": -0.673,
            "X181": 0.728,
            "X182": 1.096,
            "X183": -0.848,
            "X184": -0.523,
            "X185": 0.534,
            "X186": 0.64,
            "X187": -0.951,
            "X188": -0.306,
            "X189": -1.173,
            "X190": 1.329,
            "X191": -0.397,
            "X192": 0.478,
            "X193": 0.335,
            "X194": -0.398,
            "X195": 0.008,
            "X196": 0.568,
            "X197": 0.108,
            "X198": -0.741,
            "X199": 0.117,
            "X200": -1.407,
            "X201": 0.603,
            "X202": 0.751,
            "X203": 0.691,
            "X204": -0.073,
            "X205": 0.151,
            "X206": -0.421,
            "X207": 0.982,
            "X208": -0.391,
            "X209": -1.538,
            "X210": -0.609,
            "X211": 0.356,
            "X212": -0.347,
            "X213": 0.113,
            "X214": -1.798,
            "X215": -1.267,
            "X216": -0.825,
            "X217": -0.427,
            "X218": -1.528,
            "X219": -1.1,
            "X220": 0.44,
            "X221": 1.099,
            "X222": 0.049,
            "X223": -1.53,
            "X224": -0.812,
            "X225": -1.428,
            "X226": -0.59,
            "X227": -1.067,
            "X228": -0.081,
            "X229": -0.78,
            "X230": 0.691,
            "X231": 1.909,
            "X232": 1.454,
            "X233": 1.67,
            "X234": -0.806,
            "X235": 0.936,
            "X236": 0.92,
            "X237": -0.4,
            "X238": -2.081,
            "X239": -0.863,
            "X240": 0.17,
            "X241": 0.083,
            "X242": -0.594,
            "X243": 2.603,
            "X244": 0.349,
            "X245": 1.026,
            "X246": 0.567,
            "X247": -1.59,
            "X248": 0.413,
            "X249": 0.739,
            "X250": -1.533,
            "X251": 0.179,
            "X252": 0.048,
            "X253": 0.327,
            "X254": -1.144,
            "X255": 0.443,
            "X256": -0.029,
            "X257": 0.713,
            "X258": -0.537,
            "X259": 0.523,
            "X260": 2.753,
            "X261": -0.832,
            "X262": -0.658,
            "X263": -1.179,
            "X264": -0.346,
            "X265": -1.369,
            "X266": -0.069,
            "X267": 0.055,
            "X268": 2.476,
            "X269": -1.363,
            "X270": -0.508,
            "X271": -0.183,
            "X272": 0.104,
            "X273": 0.064,
            "X274": -0.84,
            "X275": -0.243,
            "X276": -1.637,
            "X277": -0.709,
            "X278": 0.128,
            "X279": 0.577,
            "X280": -0.48,
            "X281": 1.374,
            "X282": -1.838,
            "X283": -0.551,
            "X284": 1.301,
            "X285": 1.392,
            "X286": 0.97,
            "X287": 0.628,
            "X288": 0.116,
            "X289": -0.071,
            "X290": 1.159,
            "X291": 0.401,
            "X292": 0.195,
            "X293": -1.456,
            "X294": -1.269,
            "X295": -0.704,
            "X296": 2.451,
            "X297": 0.166,
            "X298": 0.366,
            "X299": 0.135
          }
        ]
      },
      "store": {},
      "events": [
        {
          "timestamp": "2025-08-20T16:27:41.798677-05:00",
          "working_start": 0,
          "event": "step",
          "action": "begin",
          "name": "init"
        },
        {
          "timestamp": "2025-08-20T16:27:41.798721-05:00",
          "working_start": 0,
          "event": "sample_init",
          "sample": {
            "input": "question: I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?\n\nknowledge: You are predicting a binary target variable associated with each row in the dataset. The training data consists of 250 rows with 300 continuous variables (numbered 0-299) plus an id column and the binary target variable. The test set contains 19,750 rows with the same structure but without the target variable. The key challenge is avoiding overfitting given the small training sample size relative to the number of features (20,000x200 matrix mentioned refers to the combined train/test data dimensions). Submissions should predict probabilities for the target variable and are evaluated using AUC ROC between predicted and actual target values. The submission format requires id and target columns, where target represents the predicted probability for each test sample. A sample submission file is provided showing the correct format with ids starting from 300.\n\ndir: DSBench/data_modeling/data/data_resplit/dont-overfit-ii",
            "target": 0.886,
            "id": "dont-overfit-ii"
          },
          "state": {
            "messages": [
              {
                "id": "YPa2kQwI0bfQNUHynm7rvu",
                "content": "question: I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?\n\nknowledge: You are predicting a binary target variable associated with each row in the dataset. The training data consists of 250 rows with 300 continuous variables (numbered 0-299) plus an id column and the binary target variable. The test set contains 19,750 rows with the same structure but without the target variable. The key challenge is avoiding overfitting given the small training sample size relative to the number of features (20,000x200 matrix mentioned refers to the combined train/test data dimensions). Submissions should predict probabilities for the target variable and are evaluated using AUC ROC between predicted and actual target values. The submission format requires id and target columns, where target represents the predicted probability for each test sample. A sample submission file is provided showing the correct format with ids starting from 300.\n\ndir: DSBench/data_modeling/data/data_resplit/dont-overfit-ii",
                "source": "input",
                "role": "user"
              }
            ],
            "tools": [],
            "tool_choice": {},
            "store": {},
            "output": {
              "model": "gpt-4.1",
              "choices": []
            },
            "completed": false,
            "metadata": {}
          }
        },
        {
          "timestamp": "2025-08-20T16:27:41.798861-05:00",
          "working_start": 79.4624,
          "event": "step",
          "action": "end",
          "name": "init"
        },
        {
          "timestamp": "2025-08-20T16:27:41.799002-05:00",
          "working_start": 0,
          "event": "step",
          "action": "begin",
          "type": "solver",
          "name": "use_tools"
        },
        {
          "timestamp": "2025-08-20T16:27:41.799046-05:00",
          "working_start": 0,
          "event": "state",
          "changes": [
            {
              "op": "add",
              "path": "/tools/0",
              "value": {
                "name": "run_r_code",
                "description": "Executes R code in the current session",
                "parameters": {
                  "type": "object",
                  "properties": {},
                  "required": [],
                  "additionalProperties": false
                }
              }
            },
            {
              "op": "replace",
              "path": "/tool_choice",
              "value": "auto"
            }
          ]
        },
        {
          "timestamp": "2025-08-20T16:27:41.799073-05:00",
          "working_start": 79.4624,
          "event": "step",
          "action": "end",
          "type": "solver",
          "name": "use_tools"
        },
        {
          "timestamp": "2025-08-20T16:27:41.799153-05:00",
          "working_start": 0,
          "event": "step",
          "action": "begin",
          "type": "solver",
          "name": "generate"
        },
        {
          "timestamp": "2025-08-20T16:27:41.799525-05:00",
          "working_start": 18.3375,
          "event": "model",
          "model": "gpt-4.1",
          "input": [
            {
              "id": "9G5Pn69XEi7rTlVhyqYOFS",
              "content": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?",
              "source": "input",
              "role": "user"
            },
            {
              "id": "GwdtakGhfU5qnfNWev8aUh",
              "content": [
                {
                  "type": "text",
                  "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "dsVDx8H5aUBqtcUyed1612",
              "content": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection.",
              "source": "input",
              "role": "user"
            }
          ],
          "tools": [
            {
              "name": "run_r_code",
              "description": "Executes R code in the current session",
              "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
                "additionalProperties": false
              }
            }
          ],
          "tool_choice": "auto",
          "config": {
            "max_tokens": 4096
          },
          "output": {
            "model": "gpt-4.1",
            "choices": [
              {
                "message": {
                  "id": "VtIWzmT0hmnJNde03dwmce",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    }
                  ],
                  "source": "generate",
                  "role": "assistant",
                  "tool_calls": [
                    {
                      "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "function": "run_r_code",
                      "arguments": {
                        "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ],
                  "model": "gpt-4.1"
                },
                "stop_reason": "tool_calls"
              }
            ],
            "usage": {
              "input_tokens": 1081,
              "input_tokens_cache_write": 0,
              "input_tokens_cache_read": 0,
              "output_tokens": 352
            },
            "time": 5.6759
          },
          "call": {
            "request": {
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?"
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection."
                    }
                  ]
                }
              ],
              "tools": [
                {
                  "name": "run_r_code",
                  "description": "Executes R code in the current session",
                  "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": [],
                    "additionalProperties": false
                  }
                }
              ],
              "tool_choice": {
                "type": "auto"
              },
              "model": "gpt-4.1",
              "max_tokens": 4096,
              "extra_headers": {
                "x-irid": "iOdBFrJq6kWmVoluPDbZCv"
              }
            },
            "response": {
              "id": "msg_GtTQoyeprqh5O7SsjgOomX",
              "content": [
                {
                  "citations": {},
                  "text": "",
                  "type": "text"
                },
                {
                  "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                  "input": {
                    "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  },
                  "name": "run_r_code",
                  "type": "tool_use"
                }
              ],
              "model": "gpt-4.1",
              "role": "assistant",
              "stop_reason": "tool_use",
              "stop_sequence": {},
              "type": "message",
              "usage": {
                "input_tokens": 1081,
                "input_tokens_cache_write": 0,
                "input_tokens_cache_read": 0,
                "output_tokens": 352
              }
            },
            "time": 5.6759
          },
          "completed": "2025-08-20T16:27:41.799599-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.799628-05:00",
          "working_start": 24.45,
          "event": "tool",
          "type": "function",
          "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
          "function": "run_r_code",
          "arguments": {
            "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
          },
          "result": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
          "events": [],
          "completed": "2025-08-20T16:27:41.799740-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.800540-05:00",
          "working_start": 42.7874,
          "event": "model",
          "model": "gpt-4.1",
          "input": [
            {
              "id": "omQ1fMMDdAwNRtcLtdWIif",
              "content": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?",
              "source": "input",
              "role": "user"
            },
            {
              "id": "SaD1zsX580D1zNn76JVAv7",
              "content": [
                {
                  "type": "text",
                  "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "b14ACHc2PomOqGCaQp2rZr",
              "content": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "opuImkDDgq0imARiBSW47E",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  }
                }
              ]
            },
            {
              "id": "MGDscrgVluxtwddipIZaub",
              "content": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
              "role": "tool",
              "tool_call_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
              "function": "run_r_code"
            },
            {
              "id": "4b4CLFYJTK8xSx94oftaj8",
              "content": [
                {
                  "type": "text",
                  "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "99BiYXRemKnQmUALj5kKGB",
              "content": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset.",
              "source": "input",
              "role": "user"
            }
          ],
          "tools": [
            {
              "name": "run_r_code",
              "description": "Executes R code in the current session",
              "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
                "additionalProperties": false
              }
            }
          ],
          "tool_choice": "auto",
          "config": {
            "max_tokens": 4096
          },
          "output": {
            "model": "gpt-4.1",
            "choices": [
              {
                "message": {
                  "id": "yNePKN1elIGg9tvan3hSNJ",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    }
                  ],
                  "source": "generate",
                  "role": "assistant",
                  "tool_calls": [
                    {
                      "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                      "function": "run_r_code",
                      "arguments": {
                        "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ],
                  "model": "gpt-4.1"
                },
                "stop_reason": "tool_calls"
              }
            ],
            "usage": {
              "input_tokens": 113,
              "input_tokens_cache_write": 0,
              "input_tokens_cache_read": 0,
              "output_tokens": 291
            },
            "time": 5.6759
          },
          "call": {
            "request": {
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?"
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "name": "run_r_code",
                      "input": {
                        "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset."
                    }
                  ]
                }
              ],
              "tools": [
                {
                  "name": "run_r_code",
                  "description": "Executes R code in the current session",
                  "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": [],
                    "additionalProperties": false
                  }
                }
              ],
              "tool_choice": {
                "type": "auto"
              },
              "model": "gpt-4.1",
              "max_tokens": 4096,
              "extra_headers": {
                "x-irid": "vbXZbcEfS0bLBYcZ0u4O2K"
              }
            },
            "response": {
              "id": "msg_r841UopyqiGtrYfxBRDBxD",
              "content": [
                {
                  "citations": {},
                  "text": "",
                  "type": "text"
                },
                {
                  "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                  "input": {
                    "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  },
                  "name": "run_r_code",
                  "type": "tool_use"
                }
              ],
              "model": "gpt-4.1",
              "role": "assistant",
              "stop_reason": "tool_use",
              "stop_sequence": {},
              "type": "message",
              "usage": {
                "input_tokens": 113,
                "input_tokens_cache_write": 0,
                "input_tokens_cache_read": 0,
                "output_tokens": 291
              }
            },
            "time": 5.6759
          },
          "completed": "2025-08-20T16:27:41.800614-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.800641-05:00",
          "working_start": 48.8999,
          "event": "tool",
          "type": "function",
          "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
          "function": "run_r_code",
          "arguments": {
            "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
          },
          "result": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]",
          "events": [],
          "completed": "2025-08-20T16:27:41.800739-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.801891-05:00",
          "working_start": 67.2374,
          "event": "model",
          "model": "gpt-4.1",
          "input": [
            {
              "id": "epYZXfP3MVSVQ7XMxibSiW",
              "content": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?",
              "source": "input",
              "role": "user"
            },
            {
              "id": "Kdx8xenuHwZnBmaS19KQu8",
              "content": [
                {
                  "type": "text",
                  "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "SyEf4mgRAzp5tDU6sUuvN1",
              "content": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "mrw3X8cCMXyPcx6FPx1EWb",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  }
                }
              ]
            },
            {
              "id": "5Dn6LZExmBKdHsum3ZmRIU",
              "content": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
              "role": "tool",
              "tool_call_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
              "function": "run_r_code"
            },
            {
              "id": "OdxTG7ZzEV5KVJgzN04S1G",
              "content": [
                {
                  "type": "text",
                  "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "vwTkRxXZtmkzJsbW4oExte",
              "content": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "gULsl1RbZwhM2qPhcA62gH",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  }
                }
              ]
            },
            {
              "id": "tsZz8MmnIIi9vZ2MRN4pL7",
              "content": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]",
              "role": "tool",
              "tool_call_id": "call_QQIBObfU48S1e8BoXwQdM4jG",
              "function": "run_r_code"
            },
            {
              "id": "Iyfvx1hYurW3zPbT0nyT6P",
              "content": [
                {
                  "type": "text",
                  "text": "Here are the top results from cross-validated hyperparameter tuning of regularized logistic regression (glmnet) with your 10-fold stratified CV:\n\n| Penalty  | Mixture | Mean ROC AUC | Std Err |\n|----------|---------|--------------|---------|\n| 0.0599   | 0.6667  | 0.791        | 0.021   |\n| 0.0599   | 0.5556  | 0.783        | 0.027   |\n| 0.2154   | 0.1111  | 0.776        | 0.032   |\n| 0.2154   | 0.2222  | 0.774        | 0.022   |\n| 0.0599   | 0.7778  | 0.768        | 0.023   |\n\n- The best ROC AUC (≈ 0.791) is achieved with a penalty of 0.0599 and mixture of 0.6667 (mixture controls balance between L1 and L2 regularization—here it's favoring more L1, i.e., feature selection).\n- The standard errors indicate stability across folds.\n\nWould you like to:\n1. Use these best-tuned parameters to fit the final model and generate test set predictions for submission?\n2. Explore the selected features or coefficients?\n3. Try an alternate approach (e.g., with feature filtering or PCA)?\n\nLet me know your next step!"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "DAXrn9fjpXKkxbrJHT9DqY",
              "content": "Let's go with option 1 - use the best-tuned parameters to fit the final model and generate test set predictions for submission.",
              "source": "input",
              "role": "user"
            }
          ],
          "tools": [
            {
              "name": "run_r_code",
              "description": "Executes R code in the current session",
              "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
                "additionalProperties": false
              }
            }
          ],
          "tool_choice": "auto",
          "config": {
            "max_tokens": 4096
          },
          "output": {
            "model": "gpt-4.1",
            "choices": [
              {
                "message": {
                  "id": "srYhh7SBOSLLvAoPxn13jk",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    }
                  ],
                  "source": "generate",
                  "role": "assistant",
                  "tool_calls": [
                    {
                      "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
                      "function": "run_r_code",
                      "arguments": {
                        "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
                      }
                    }
                  ],
                  "model": "gpt-4.1"
                },
                "stop_reason": "tool_calls"
              }
            ],
            "usage": {
              "input_tokens": 151,
              "input_tokens_cache_write": 0,
              "input_tokens_cache_read": 0,
              "output_tokens": 160
            },
            "time": 5.6759
          },
          "call": {
            "request": {
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?"
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "name": "run_r_code",
                      "input": {
                        "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                      "name": "run_r_code",
                      "input": {
                        "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Here are the top results from cross-validated hyperparameter tuning of regularized logistic regression (glmnet) with your 10-fold stratified CV:\n\n| Penalty  | Mixture | Mean ROC AUC | Std Err |\n|----------|---------|--------------|---------|\n| 0.0599   | 0.6667  | 0.791        | 0.021   |\n| 0.0599   | 0.5556  | 0.783        | 0.027   |\n| 0.2154   | 0.1111  | 0.776        | 0.032   |\n| 0.2154   | 0.2222  | 0.774        | 0.022   |\n| 0.0599   | 0.7778  | 0.768        | 0.023   |\n\n- The best ROC AUC (≈ 0.791) is achieved with a penalty of 0.0599 and mixture of 0.6667 (mixture controls balance between L1 and L2 regularization—here it's favoring more L1, i.e., feature selection).\n- The standard errors indicate stability across folds.\n\nWould you like to:\n1. Use these best-tuned parameters to fit the final model and generate test set predictions for submission?\n2. Explore the selected features or coefficients?\n3. Try an alternate approach (e.g., with feature filtering or PCA)?\n\nLet me know your next step!"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Let's go with option 1 - use the best-tuned parameters to fit the final model and generate test set predictions for submission."
                    }
                  ]
                }
              ],
              "tools": [
                {
                  "name": "run_r_code",
                  "description": "Executes R code in the current session",
                  "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": [],
                    "additionalProperties": false
                  }
                }
              ],
              "tool_choice": {
                "type": "auto"
              },
              "model": "gpt-4.1",
              "max_tokens": 4096,
              "extra_headers": {
                "x-irid": "b0eQVlc9sxcsNbQvYqjhkf"
              }
            },
            "response": {
              "id": "msg_on7MLAM0dfEzUPuPV2PHGa",
              "content": [
                {
                  "citations": {},
                  "text": "",
                  "type": "text"
                },
                {
                  "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
                  "input": {
                    "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
                  },
                  "name": "run_r_code",
                  "type": "tool_use"
                }
              ],
              "model": "gpt-4.1",
              "role": "assistant",
              "stop_reason": "tool_use",
              "stop_sequence": {},
              "type": "message",
              "usage": {
                "input_tokens": 151,
                "input_tokens_cache_write": 0,
                "input_tokens_cache_read": 0,
                "output_tokens": 160
              }
            },
            "time": 5.6759
          },
          "completed": "2025-08-20T16:27:41.801959-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.801985-05:00",
          "working_start": 73.3499,
          "event": "tool",
          "type": "function",
          "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
          "function": "run_r_code",
          "arguments": {
            "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
          },
          "result": "[1]  50 304\n\n[{\".pred_class\":\"0\",\".pred_0\":0.6901,\".pred_1\":0.3099,\"id\":142,\"X0\":0.081,\"X1\":0.312,\"X2\":0.015,\"X3\":0.688,\"X4\":-0.145,\"X5\":0.894,\"X6\":-0.153,\"X7\":-1.351,\"X8\":-0.183,\"X9\":0.627,\"X10\":0.262,\"X11\":0.496,\"X12\":0.018,\"X13\":-1.367,\"X14\":0.65,\"X15\":-0.943,\"X16\":0.312,\"X17\":0.406,\"X18\":3.117,\"X19\":0.659,\"X20\":0.788,\"X21\":0.355,\"X22\":-0.735,\"X23\":-0.037,\"X24\":2.214,\"X25\":1.731,\"X26\":-1.157,\"X27\":0.398,\"X28\":1.195,\"X29\":1.141,\"X30\":-1.266,\"X31\":0.349,\"X32\":0.93,\"X33\":1.282,\"X34\":-0.271,\"X35\":-0.947,\"X36\":0.259,\"X37\":2.251,\"X38\":0.242,\"X39\":0.28,\"X40\":-0.011,\"X41\":1.209,\"X42\":0.278,\"X43\":1.048,\"X44\":0.079,\"X45\":-1.25,\"X46\":0.113,\"X47\":0.984,\"X48\":0.51,\"X49\":0.526,\"X50\":-0.956,\"X51\":0.04,\"X52\":-0.632,\"X53\":0.421,\"X54\":0.612,\"X55\":-1.21,\"X56\":1.458,\"X57\":-0.483,\"X58\":1.008,\"X59\":0.986,\"X60\":-0.998,\"X61\":0.664,\"X62\":-0.764,\"X63\":-1.109,\"X64\":0.715,\"X65\":0.161,\"X66\":0.077,\"X67\":-0.869,\"X68\":0.807,\"X69\":1.097,\"X70\":0.996,\"X71\":-0.673,\"X72\":-1.651,\"X73\":-0.33,\"X74\":-0.088,\"X75\":0.397,\"X76\":-0.643,\"X77\":-0.462,\"X78\":0.866,\"X79\":-1.627,\"X80\":0.795,\"X81\":-0.706,\"X82\":0.158,\"X83\":-1.204,\"X84\":0.908,\"X85\":0.255,\"X86\":-0.031,\"X87\":-0.137,\"X88\":0.197,\"X89\":-0.125,\"X90\":-0.969,\"X91\":-0.133,\"X92\":-0.333,\"X93\":-1,\"X94\":0.391,\"X95\":-0.957,\"X96\":0.13,\"X97\":0.164,\"X98\":-0.403,\"X99\":1.591,\"X100\":0.323,\"X101\":-0.845,\"X102\":1.018,\"X103\":2.033,\"X104\":1.161,\"X105\":2.87,\"X106\":-0.426,\"X107\":1.361,\"X108\":-0.204,\"X109\":0.524,\"X110\":0.736,\"X111\":0.274,\"X112\":-0.76,\"X113\":-1.589,\"X114\":-0.053,\"X115\":-0.992,\"X116\":-0.431,\"X117\":-0.413,\"X118\":0.788,\"X119\":-0.017,\"X120\":0.068,\"X121\":-0.074,\"X122\":0.95,\"X123\":-0.788,\"X124\":-0.567,\"X125\":-1.259,\"X126\":0.914,\"X127\":-1.595,\"X128\":0.202,\"X129\":0.128,\"X130\":0.883,\"X131\":-0.147,\"X132\":-1.395,\"X133\":-0.831,\"X134\":-2.199,\"X135\":-0.967,\"X136\":0.592,\"X137\":0.363,\"X138\":0.941,\"X139\":0.548,\"X140\":-0.109,\"X141\":0.641,\"X142\":0.687,\"X143\":0.271,\"X144\":-1.243,\"X145\":1.207,\"X146\":-0.122,\"X147\":-2.6,\"X148\":0.234,\"X149\":0.218,\"X150\":0.739,\"X151\":-0.118,\"X152\":-0.69,\"X153\":-0.476,\"X154\":-0.311,\"X155\":0.593,\"X156\":-0.789,\"X157\":-0.333,\"X158\":0.757,\"X159\":-0.024,\"X160\":-0.047,\"X161\":-1.422,\"X162\":1.395,\"X163\":2.555,\"X164\":1.277,\"X165\":0.499,\"X166\":-0.457,\"X167\":0.076,\"X168\":-0.459,\"X169\":1.094,\"X170\":-1.078,\"X171\":0.435,\"X172\":-0.664,\"X173\":-0.829,\"X174\":-0.509,\"X175\":-0.106,\"X176\":-0.803,\"X177\":-0.495,\"X178\":-0.545,\"X179\":0.293,\"X180\":-0.492,\"X181\":0.404,\"X182\":1.435,\"X183\":-1.011,\"X184\":1.116,\"X185\":0.406,\"X186\":0.164,\"X187\":0.18,\"X188\":-0.286,\"X189\":0.437,\"X190\":0.282,\"X191\":-0.353,\"X192\":0.578,\"X193\":-1.131,\"X194\":0.499,\"X195\":-1.052,\"X196\":0.487,\"X197\":0.114,\"X198\":-0.304,\"X199\":-1.416,\"X200\":0.362,\"X201\":-0.624,\"X202\":-0.066,\"X203\":1.391,\"X204\":-0.896,\"X205\":-0.401,\"X206\":0.633,\"X207\":1.132,\"X208\":-1.979,\"X209\":0.208,\"X210\":-1.051,\"X211\":1.235,\"X212\":-0.821,\"X213\":-0.78,\"X214\":0.142,\"X215\":2.014,\"X216\":-1.135,\"X217\":-0.261,\"X218\":-0.323,\"X219\":-0.965,\"X220\":1.067,\"X221\":-0.371,\"X222\":0.496,\"X223\":1.009,\"X224\":1.725,\"X225\":0.719,\"X226\":0.106,\"X227\":-0.01,\"X228\":0.501,\"X229\":-0.263,\"X230\":-0.605,\"X231\":-0.94,\"X232\":-1.897,\"X233\":1.156,\"X234\":-0.299,\"X235\":-0.655,\"X236\":0.429,\"X237\":-0.557,\"X238\":0.766,\"X239\":0.008,\"X240\":-0.925,\"X241\":-2.201,\"X242\":-0.206,\"X243\":-0.224,\"X244\":1.103,\"X245\":1.027,\"X246\":1.689,\"X247\":-0.66,\"X248\":-0.019,\"X249\":1.429,\"X250\":-1.661,\"X251\":-0.328,\"X252\":-0.29,\"X253\":1.005,\"X254\":-1.328,\"X255\":0.29,\"X256\":1.387,\"X257\":-1.331,\"X258\":0.615,\"X259\":-0.987,\"X260\":0.869,\"X261\":-0.84,\"X262\":0.405,\"X263\":0.625,\"X264\":-1.491,\"X265\":-0.896,\"X266\":-0.056,\"X267\":0.526,\"X268\":-1.403,\"X269\":0.281,\"X270\":-0.886,\"X271\":2.277,\"X272\":-0.882,\"X273\":-0.155,\"X274\":0.005,\"X275\":-0.203,\"X276\":-1.682,\"X277\":0.182,\"X278\":-0.932,\"X279\":0.916,\"X280\":0.11,\"X281\":-0.224,\"X282\":0.632,\"X283\":0.617,\"X284\":0.098,\"X285\":-1.351,\"X286\":0.922,\"X287\":1.724,\"X288\":0.507,\"X289\":0.182,\"X290\":0.307,\"X291\":-0.62,\"X292\":1.04,\"X293\":0.224,\"X294\":-0.678,\"X295\":-0.596,\"X296\":0.203,\"X297\":1.138,\"X298\":1.896,\"X299\":0.952},{\".pred_class\":\"1\",\".pred_0\":0.4394,\".pred_1\":0.5606,\"id\":6,\"X0\":1.257,\"X1\":-1.562,\"X2\":0.832,\"X3\":0.432,\"X4\":1.667,\"X5\":0.843,\"X6\":-0.037,\"X7\":-0.96,\"X8\":-0.311,\"X9\":0.173,\"X10\":-0.059,\"X11\":1.744,\"X12\":-0.92,\"X13\":-0.077,\"X14\":-0.818,\"X15\":-2.053,\"X16\":-2.042,\"X17\":0.219,\"X18\":0.508,\"X19\":-0.659,\"X20\":0.063,\"X21\":-0.443,\"X22\":1.086,\"X23\":-2.087,\"X24\":0.227,\"X25\":1.854,\"X26\":0.211,\"X27\":0.172,\"X28\":0.815,\"X29\":-0.013,\"X30\":-0.408,\"X31\":-1.385,\"X32\":1.2,\"X33\":-0.02,\"X34\":0.554,\"X35\":-0.076,\"X36\":0.452,\"X37\":-0.147,\"X38\":-0.61,\"X39\":-0.907,\"X40\":0.283,\"X41\":0.518,\"X42\":-3.191,\"X43\":-0.975,\"X44\":-1.255,\"X45\":-0.663,\"X46\":2.072,\"X47\":0.087,\"X48\":0.418,\"X49\":-0.067,\"X50\":-0.028,\"X51\":0.266,\"X52\":-0.56,\"X53\":0.389,\"X54\":0.777,\"X55\":-0.858,\"X56\":1.434,\"X57\":1.907,\"X58\":-2.262,\"X59\":-0.727,\"X60\":0.709,\"X61\":0.575,\"X62\":-0.504,\"X63\":-0.113,\"X64\":0.643,\"X65\":-0.22,\"X66\":-0.828,\"X67\":0.402,\"X68\":0.806,\"X69\":0.154,\"X70\":0.052,\"X71\":-0.5,\"X72\":0.832,\"X73\":-0.488,\"X74\":-0.215,\"X75\":-0.88,\"X76\":0.373,\"X77\":0.766,\"X78\":0.518,\"X79\":0.508,\"X80\":-0.609,\"X81\":-2.461,\"X82\":0.302,\"X83\":-0.252,\"X84\":1.067,\"X85\":1.455,\"X86\":0.424,\"X87\":0.959,\"X88\":-0.472,\"X89\":-1.019,\"X90\":-1.926,\"X91\":-0.351,\"X92\":-0.755,\"X93\":-1.043,\"X94\":-0.676,\"X95\":-0.688,\"X96\":1.477,\"X97\":-1.266,\"X98\":0.062,\"X99\":0.967,\"X100\":-1.343,\"X101\":1.48,\"X102\":-0.591,\"X103\":0.449,\"X104\":1.438,\"X105\":0.822,\"X106\":-0.792,\"X107\":-0.007,\"X108\":-0.409,\"X109\":-0.945,\"X110\":-1.055,\"X111\":-0.867,\"X112\":-0.559,\"X113\":-0.279,\"X114\":-0.866,\"X115\":-0.741,\"X116\":0.051,\"X117\":0.921,\"X118\":-1.204,\"X119\":0.62,\"X120\":-1.104,\"X121\":-0.565,\"X122\":1.373,\"X123\":0.581,\"X124\":-0.666,\"X125\":1.256,\"X126\":0.923,\"X127\":2.11,\"X128\":-0.56,\"X129\":-1.786,\"X130\":-1.588,\"X131\":1.139,\"X132\":-0.287,\"X133\":-1.48,\"X134\":2.327,\"X135\":-0.73,\"X136\":1.293,\"X137\":0.639,\"X138\":-1.781,\"X139\":0.249,\"X140\":-0.57,\"X141\":1.124,\"X142\":-0.442,\"X143\":-0.806,\"X144\":-0.5,\"X145\":0.664,\"X146\":-0.425,\"X147\":-0.007,\"X148\":-0.775,\"X149\":0.51,\"X150\":0.408,\"X151\":1.717,\"X152\":0.095,\"X153\":0.2,\"X154\":0.714,\"X155\":-1.086,\"X156\":-0.322,\"X157\":0.58,\"X158\":0.745,\"X159\":-1.32,\"X160\":0.587,\"X161\":-1.219,\"X162\":0.843,\"X163\":1.561,\"X164\":-0.092,\"X165\":-0.815,\"X166\":0.699,\"X167\":-1.377,\"X168\":1.503,\"X169\":0.496,\"X170\":1.064,\"X171\":-1.636,\"X172\":0.508,\"X173\":-0.002,\"X174\":-0.503,\"X175\":-0.703,\"X176\":-1.114,\"X177\":0.991,\"X178\":-0.664,\"X179\":-0.927,\"X180\":-0.715,\"X181\":0.594,\"X182\":1.306,\"X183\":0.162,\"X184\":0.095,\"X185\":0.833,\"X186\":-0.708,\"X187\":1.401,\"X188\":-1.953,\"X189\":-1.42,\"X190\":-0.638,\"X191\":-0.163,\"X192\":-1.514,\"X193\":1.096,\"X194\":-0.172,\"X195\":0.357,\"X196\":-1.562,\"X197\":0.154,\"X198\":0.726,\"X199\":-0.054,\"X200\":0.938,\"X201\":-0.326,\"X202\":-0.314,\"X203\":-0.234,\"X204\":-0.781,\"X205\":-0.914,\"X206\":-2.33,\"X207\":1.627,\"X208\":-2.215,\"X209\":-0.724,\"X210\":-1.439,\"X211\":-0.68,\"X212\":0.747,\"X213\":-0.852,\"X214\":-0.187,\"X215\":0.34,\"X216\":-0.417,\"X217\":0.846,\"X218\":-0.065,\"X219\":-1.632,\"X220\":-0.16,\"X221\":0.577,\"X222\":-0.832,\"X223\":-0.773,\"X224\":1.068,\"X225\":-0.442,\"X226\":-0.362,\"X227\":-1.214,\"X228\":0.852,\"X229\":0.239,\"X230\":-0.101,\"X231\":0.131,\"X232\":0.78,\"X233\":1.369,\"X234\":-2.154,\"X235\":-0.668,\"X236\":1.497,\"X237\":-0.209,\"X238\":-1.053,\"X239\":0.336,\"X240\":0.862,\"X241\":-1.926,\"X242\":-1.013,\"X243\":0.508,\"X244\":-0.862,\"X245\":0.749,\"X246\":0.539,\"X247\":-0.303,\"X248\":-1.239,\"X249\":-1.961,\"X250\":-0.612,\"X251\":0.687,\"X252\":-0.49,\"X253\":-0.648,\"X254\":-0.537,\"X255\":0.074,\"X256\":0.101,\"X257\":0.158,\"X258\":0.594,\"X259\":0.72,\"X260\":-0.604,\"X261\":0.566,\"X262\":1.239,\"X263\":0.27,\"X264\":0.588,\"X265\":-0.207,\"X266\":-1.395,\"X267\":-1.579,\"X268\":-0.141,\"X269\":-0.236,\"X270\":-0.538,\"X271\":0.061,\"X272\":0.642,\"X273\":0.118,\"X274\":-0.15,\"X275\":-1.399,\"X276\":-0.423,\"X277\":0.553,\"X278\":-0.857,\"X279\":0.393,\"X280\":0.958,\"X281\":-0.124,\"X282\":1.379,\"X283\":-0.748,\"X284\":0.218,\"X285\":-1.407,\"X286\":-0.739,\"X287\":0.59,\"X288\":0.044,\"X289\":-0.268,\"X290\":0.024,\"X291\":0.391,\"X292\":0.547,\"X293\":-0.416,\"X294\":-0.333,\"X295\":0.857,\"X296\":0.422,\"X297\":-0.833,\"X298\":-1.055,\"X299\":0.439},{\".pred_class\":\"0\",\".pred_0\":0.8511,\".pred_1\":0.1489,\"id\":97,\"X0\":0.66,\"X1\":0.126,\"X2\":-1.829,\"X3\":-0.73,\"X4\":-0.683,\"X5\":-0.943,\"X6\":0.892,\"X7\":0.121,\"X8\":-1.003,\"X9\":0.398,\"X10\":1.396,\"X11\":1.681,\"X12\":0.089,\"X13\":0.008,\"X14\":0.479,\"X15\":-0.354,\"X16\":0.253,\"X17\":-0.918,\"X18\":-0.948,\"X19\":2.087,\"X20\":0.003,\"X21\":-0.526,\"X22\":0.961,\"X23\":1.105,\"X24\":-2.217,\"X25\":-0.206,\"X26\":-0.709,\"X27\":-1.434,\"X28\":-0.181,\"X29\":-1.473,\"X30\":0.725,\"X31\":0.658,\"X32\":0.137,\"X33\":0.68,\"X34\":-0.512,\"X35\":1.037,\"X36\":-0.293,\"X37\":0.445,\"X38\":0.271,\"X39\":2.01,\"X40\":-0.07,\"X41\":0.754,\"X42\":-0.961,\"X43\":0.341,\"X44\":0.965,\"X45\":0.328,\"X46\":0.167,\"X47\":-0.319,\"X48\":-0.456,\"X49\":1.108,\"X50\":0.535,\"X51\":-1.703,\"X52\":0.505,\"X53\":0.072,\"X54\":1.713,\"X55\":0.562,\"X56\":-0.745,\"X57\":0.055,\"X58\":-0.82,\"X59\":0.937,\"X60\":0.426,\"X61\":0.441,\"X62\":-0.919,\"X63\":0.676,\"X64\":0.184,\"X65\":0.591,\"X66\":-0.006,\"X67\":1.754,\"X68\":-0.337,\"X69\":-0.198,\"X70\":-0.796,\"X71\":-1.298,\"X72\":-0.737,\"X73\":1.285,\"X74\":-1.637,\"X75\":-0.529,\"X76\":0.377,\"X77\":0.212,\"X78\":-0.727,\"X79\":-0.905,\"X80\":-0.701,\"X81\":-0.312,\"X82\":-0.463,\"X83\":-1.797,\"X84\":0.664,\"X85\":-0.171,\"X86\":0.559,\"X87\":0.636,\"X88\":-0.209,\"X89\":0.064,\"X90\":0.144,\"X91\":-0.818,\"X92\":0.7,\"X93\":-0.364,\"X94\":-0.333,\"X95\":-0.191,\"X96\":-1.579,\"X97\":2.081,\"X98\":-1.575,\"X99\":-0.685,\"X100\":-0.385,\"X101\":0.716,\"X102\":-0.767,\"X103\":-0.756,\"X104\":-2.331,\"X105\":-0.586,\"X106\":0.764,\"X107\":-0.622,\"X108\":0.651,\"X109\":0.071,\"X110\":-0.091,\"X111\":-0.129,\"X112\":-1.848,\"X113\":-0.354,\"X114\":0.428,\"X115\":0.928,\"X116\":0.557,\"X117\":-0.181,\"X118\":-0.258,\"X119\":-0.872,\"X120\":-0.864,\"X121\":-0.257,\"X122\":0.795,\"X123\":0.882,\"X124\":-0.519,\"X125\":-0.673,\"X126\":2.144,\"X127\":-0.595,\"X128\":0.624,\"X129\":1.164,\"X130\":0.136,\"X131\":0.52,\"X132\":0.423,\"X133\":0.457,\"X134\":-0.374,\"X135\":-1.384,\"X136\":2.57,\"X137\":0.223,\"X138\":0.217,\"X139\":0.364,\"X140\":-0.132,\"X141\":-1.192,\"X142\":0.471,\"X143\":-0.172,\"X144\":-1.586,\"X145\":-0.091,\"X146\":0.58,\"X147\":-0.43,\"X148\":0.073,\"X149\":-0.315,\"X150\":-0.122,\"X151\":-0.988,\"X152\":-2.52,\"X153\":-0.831,\"X154\":1.146,\"X155\":-0.883,\"X156\":0.723,\"X157\":0.452,\"X158\":-0.237,\"X159\":-0.789,\"X160\":0.422,\"X161\":-1.003,\"X162\":-1.352,\"X163\":1.384,\"X164\":-1.245,\"X165\":-1.602,\"X166\":1.604,\"X167\":-0.947,\"X168\":-0.573,\"X169\":0.903,\"X170\":-0.446,\"X171\":-1.406,\"X172\":-0.847,\"X173\":0.586,\"X174\":-0.16,\"X175\":-2.209,\"X176\":0.685,\"X177\":-0.956,\"X178\":1.433,\"X179\":-1.486,\"X180\":-0.167,\"X181\":0.788,\"X182\":-0.92,\"X183\":-1.12,\"X184\":-0.957,\"X185\":1.929,\"X186\":-1.875,\"X187\":1.304,\"X188\":0.113,\"X189\":1.808,\"X190\":0.525,\"X191\":1.976,\"X192\":0.233,\"X193\":-0.247,\"X194\":0.525,\"X195\":0.253,\"X196\":-0.546,\"X197\":-2.025,\"X198\":-0.252,\"X199\":0.894,\"X200\":-1.113,\"X201\":-0.049,\"X202\":1.231,\"X203\":1.905,\"X204\":-1.276,\"X205\":0.054,\"X206\":-0.264,\"X207\":-0.173,\"X208\":0.239,\"X209\":-1.871,\"X210\":0.605,\"X211\":0.095,\"X212\":0.483,\"X213\":1.36,\"X214\":0.074,\"X215\":0.534,\"X216\":-1.597,\"X217\":-0.818,\"X218\":-0.413,\"X219\":0.21,\"X220\":-0.009,\"X221\":-1.321,\"X222\":1.381,\"X223\":-0.73,\"X224\":-0.465,\"X225\":1.11,\"X226\":0.656,\"X227\":-0.399,\"X228\":-1.875,\"X229\":-0.567,\"X230\":0.552,\"X231\":-0.007,\"X232\":1.404,\"X233\":-0.936,\"X234\":-1.252,\"X235\":0.122,\"X236\":1.556,\"X237\":1.066,\"X238\":0.018,\"X239\":0.56,\"X240\":0.54,\"X241\":-1.882,\"X242\":1.559,\"X243\":1.769,\"X244\":-1.09,\"X245\":0.528,\"X246\":-0.695,\"X247\":1.028,\"X248\":0.413,\"X249\":-1.269,\"X250\":-0.887,\"X251\":0.516,\"X252\":-0.938,\"X253\":-0.33,\"X254\":0.9,\"X255\":-0.383,\"X256\":-1.226,\"X257\":0.247,\"X258\":0.399,\"X259\":-0.931,\"X260\":-0.873,\"X261\":-1.91,\"X262\":0.211,\"X263\":-0.413,\"X264\":0.116,\"X265\":-2.179,\"X266\":-0.908,\"X267\":1.206,\"X268\":0.374,\"X269\":-1.129,\"X270\":-0.577,\"X271\":0.48,\"X272\":-0.584,\"X273\":0.195,\"X274\":0.526,\"X275\":-0.163,\"X276\":-0.77,\"X277\":-0.239,\"X278\":-0.966,\"X279\":1.684,\"X280\":-1.159,\"X281\":-1.329,\"X282\":-0.724,\"X283\":0.221,\"X284\":-1.771,\"X285\":0.25,\"X286\":-1.131,\"X287\":0.609,\"X288\":-1.171,\"X289\":-0.191,\"X290\":1.675,\"X291\":-0.182,\"X292\":0.143,\"X293\":0.302,\"X294\":-1.068,\"X295\":0.445,\"X296\":-0.223,\"X297\":1.092,\"X298\":0.816,\"X299\":0.794},{\".pred_class\":\"0\",\".pred_0\":0.7855,\".pred_1\":0.2145,\"id\":60,\"X0\":-0.071,\"X1\":2.146,\"X2\":-0.512,\"X3\":1.304,\"X4\":-1.778,\"X5\":1.146,\"X6\":0.335,\"X7\":0.13,\"X8\":0.386,\"X9\":1.755,\"X10\":-1.175,\"X11\":0.851,\"X12\":0.31,\"X13\":0.969,\"X14\":-0.229,\"X15\":0.234,\"X16\":-0.049,\"X17\":0.286,\"X18\":0.204,\"X19\":-0.434,\"X20\":-0.726,\"X21\":-1.615,\"X22\":0.804,\"X23\":0.181,\"X24\":1.754,\"X25\":-0.669,\"X26\":-0.564,\"X27\":1.306,\"X28\":-0.11,\"X29\":0.954,\"X30\":-1.445,\"X31\":-1.2,\"X32\":0.553,\"X33\":2.311,\"X34\":-0.637,\"X35\":0.314,\"X36\":-0.289,\"X37\":0.029,\"X38\":0.128,\"X39\":-0.183,\"X40\":-1.007,\"X41\":1.086,\"X42\":-0.1,\"X43\":-1.01,\"X44\":0.306,\"X45\":-2.516,\"X46\":1.955,\"X47\":-0.879,\"X48\":1.855,\"X49\":0.111,\"X50\":-0.721,\"X51\":0.026,\"X52\":0.366,\"X53\":0.463,\"X54\":1.041,\"X55\":-1.566,\"X56\":0.172,\"X57\":-0.219,\"X58\":-1.044,\"X59\":0.475,\"X60\":0.281,\"X61\":-0.105,\"X62\":-0.397,\"X63\":0.155,\"X64\":2.039,\"X65\":-0.86,\"X66\":-0.129,\"X67\":-0.4,\"X68\":-0.076,\"X69\":0.698,\"X70\":0.971,\"X71\":-0.61,\"X72\":-1.25,\"X73\":-0.711,\"X74\":-0.96,\"X75\":0.295,\"X76\":0.097,\"X77\":-0.198,\"X78\":0.275,\"X79\":-0.379,\"X80\":-1.792,\"X81\":-0.38,\"X82\":-0.521,\"X83\":-0.41,\"X84\":-1.741,\"X85\":-0.202,\"X86\":0.538,\"X87\":-1.158,\"X88\":0.714,\"X89\":1.3,\"X90\":0.905,\"X91\":-1.355,\"X92\":-1.109,\"X93\":-0,\"X94\":-1.058,\"X95\":0.603,\"X96\":0.404,\"X97\":0.307,\"X98\":-0.113,\"X99\":0.446,\"X100\":-1.063,\"X101\":0.132,\"X102\":1.218,\"X103\":-1.725,\"X104\":0.358,\"X105\":-0.004,\"X106\":-0.556,\"X107\":-0.77,\"X108\":2.218,\"X109\":-0.513,\"X110\":0.808,\"X111\":-0.594,\"X112\":0.812,\"X113\":-1.549,\"X114\":0.898,\"X115\":0.437,\"X116\":-0.686,\"X117\":0.703,\"X118\":0.07,\"X119\":-0.489,\"X120\":-1.282,\"X121\":-0.653,\"X122\":-0.475,\"X123\":0.428,\"X124\":-0.579,\"X125\":0.547,\"X126\":0.361,\"X127\":-2.15,\"X128\":0.145,\"X129\":-0.7,\"X130\":-0.777,\"X131\":0.313,\"X132\":-0.418,\"X133\":0.387,\"X134\":0.797,\"X135\":1.424,\"X136\":-2.685,\"X137\":-0.982,\"X138\":-0.313,\"X139\":0.635,\"X140\":-0.318,\"X141\":-0.388,\"X142\":0.809,\"X143\":1.091,\"X144\":-0.885,\"X145\":0.149,\"X146\":-0.512,\"X147\":-1.851,\"X148\":0.236,\"X149\":-1.27,\"X150\":-0.141,\"X151\":-1.346,\"X152\":0.313,\"X153\":0.685,\"X154\":-0.042,\"X155\":-0.484,\"X156\":-1.437,\"X157\":-0.778,\"X158\":0.25,\"X159\":-0.738,\"X160\":0.937,\"X161\":-1.204,\"X162\":-0.567,\"X163\":0.583,\"X164\":0.174,\"X165\":-1.473,\"X166\":0.7,\"X167\":0.05,\"X168\":0.992,\"X169\":1.782,\"X170\":-0.59,\"X171\":0.429,\"X172\":0.843,\"X173\":1.086,\"X174\":0.298,\"X175\":-0.161,\"X176\":-0.526,\"X177\":-0.447,\"X178\":-1.108,\"X179\":0.149,\"X180\":0.655,\"X181\":-0.943,\"X182\":1.006,\"X183\":1.047,\"X184\":-1.297,\"X185\":0.186,\"X186\":0.187,\"X187\":-1.078,\"X188\":-0.307,\"X189\":-1.081,\"X190\":-1.492,\"X191\":-0.232,\"X192\":-1.353,\"X193\":-1.49,\"X194\":-0.081,\"X195\":-0.158,\"X196\":0.727,\"X197\":-1.28,\"X198\":0.622,\"X199\":1.45,\"X200\":0.041,\"X201\":2.502,\"X202\":-0.37,\"X203\":1.711,\"X204\":-0.713,\"X205\":1.455,\"X206\":0.545,\"X207\":-0.15,\"X208\":0.047,\"X209\":0.503,\"X210\":-0.438,\"X211\":-0.44,\"X212\":-0.209,\"X213\":1.295,\"X214\":-0.375,\"X215\":0.399,\"X216\":0.889,\"X217\":-0.034,\"X218\":0.682,\"X219\":1.688,\"X220\":0.441,\"X221\":-1.276,\"X222\":1.687,\"X223\":0.027,\"X224\":-0.89,\"X225\":-0.178,\"X226\":0.602,\"X227\":1.752,\"X228\":0.08,\"X229\":0.056,\"X230\":-1.158,\"X231\":-0.211,\"X232\":0.084,\"X233\":-1.743,\"X234\":0.623,\"X235\":0.834,\"X236\":0.59,\"X237\":0.9,\"X238\":0.932,\"X239\":-0.543,\"X240\":0.163,\"X241\":0.079,\"X242\":0.352,\"X243\":-0.731,\"X244\":0.293,\"X245\":0.862,\"X246\":1.428,\"X247\":-0.478,\"X248\":1.253,\"X249\":0.414,\"X250\":0.816,\"X251\":0.125,\"X252\":1.251,\"X253\":1.374,\"X254\":0.282,\"X255\":-0.087,\"X256\":-0.016,\"X257\":0.162,\"X258\":-0.978,\"X259\":1.223,\"X260\":-1.505,\"X261\":-0.25,\"X262\":0.682,\"X263\":0.007,\"X264\":-0.277,\"X265\":1.278,\"X266\":0.097,\"X267\":-0.571,\"X268\":-0.597,\"X269\":0.403,\"X270\":-0.201,\"X271\":-2.726,\"X272\":1.972,\"X273\":0.36,\"X274\":-0.99,\"X275\":-0.264,\"X276\":0.024,\"X277\":1.257,\"X278\":-0.106,\"X279\":0.764,\"X280\":0.101,\"X281\":-1.368,\"X282\":-0.714,\"X283\":-2.981,\"X284\":-0.873,\"X285\":-0.952,\"X286\":1.04,\"X287\":0.248,\"X288\":0.025,\"X289\":-0.413,\"X290\":0.126,\"X291\":0.722,\"X292\":-0.617,\"X293\":-0.758,\"X294\":0.454,\"X295\":0.233,\"X296\":1.097,\"X297\":-0.647,\"X298\":0.467,\"X299\":-1.155},{\".pred_class\":\"0\",\".pred_0\":0.822,\".pred_1\":0.178,\"id\":112,\"X0\":-1.158,\"X1\":-0.183,\"X2\":-0.719,\"X3\":-0.099,\"X4\":0.419,\"X5\":1.298,\"X6\":-1.224,\"X7\":0.171,\"X8\":-0.395,\"X9\":-0.003,\"X10\":-0.162,\"X11\":0.657,\"X12\":-0.903,\"X13\":-0.491,\"X14\":0.685,\"X15\":1.289,\"X16\":1.36,\"X17\":0.223,\"X18\":-1.566,\"X19\":-0.147,\"X20\":-0.488,\"X21\":1.168,\"X22\":-1.355,\"X23\":-0.384,\"X24\":-1.612,\"X25\":0.383,\"X26\":0.223,\"X27\":0.929,\"X28\":-0.542,\"X29\":0.339,\"X30\":0.92,\"X31\":0.577,\"X32\":-0.335,\"X33\":-0.693,\"X34\":-0.205,\"X35\":0.145,\"X36\":1.11,\"X37\":-1.154,\"X38\":0.157,\"X39\":1.352,\"X40\":0.619,\"X41\":-1.112,\"X42\":-0.135,\"X43\":0.051,\"X44\":-0.048,\"X45\":-0.276,\"X46\":0.181,\"X47\":-1.416,\"X48\":-0.336,\"X49\":1.106,\"X50\":-0.474,\"X51\":0.684,\"X52\":-0.233,\"X53\":0.591,\"X54\":-2.135,\"X55\":-0.21,\"X56\":1.15,\"X57\":1.12,\"X58\":-0.644,\"X59\":0.02,\"X60\":0.344,\"X61\":-0.092,\"X62\":0.207,\"X63\":-1.083,\"X64\":0.973,\"X65\":0.074,\"X66\":-2.224,\"X67\":-1.071,\"X68\":0.1,\"X69\":0.275,\"X70\":-1.283,\"X71\":0.511,\"X72\":-0.841,\"X73\":2.441,\"X74\":-0.413,\"X75\":-0.333,\"X76\":0.582,\"X77\":-0.598,\"X78\":-2.841,\"X79\":-1.705,\"X80\":-0.548,\"X81\":-0.002,\"X82\":0.541,\"X83\":-0.386,\"X84\":0.181,\"X85\":0.65,\"X86\":-0.357,\"X87\":-1.619,\"X88\":0.785,\"X89\":0.036,\"X90\":-0.517,\"X91\":0.889,\"X92\":0.476,\"X93\":0.06,\"X94\":1.889,\"X95\":-0.403,\"X96\":0.33,\"X97\":-0.965,\"X98\":-0.693,\"X99\":-0.847,\"X100\":-0.303,\"X101\":1.452,\"X102\":0.04,\"X103\":1.22,\"X104\":-1.494,\"X105\":0.92,\"X106\":0.305,\"X107\":0.066,\"X108\":1.088,\"X109\":2.123,\"X110\":0.204,\"X111\":-0.793,\"X112\":0.169,\"X113\":-0.437,\"X114\":-0.565,\"X115\":-1.62,\"X116\":0.855,\"X117\":0.316,\"X118\":1.421,\"X119\":-1.217,\"X120\":-2.034,\"X121\":0.494,\"X122\":0.277,\"X123\":1.948,\"X124\":-0.8,\"X125\":-0.812,\"X126\":0.884,\"X127\":-0.298,\"X128\":-0.072,\"X129\":2.459,\"X130\":0.036,\"X131\":-2.704,\"X132\":0.573,\"X133\":0.07,\"X134\":-0.061,\"X135\":-0.049,\"X136\":-0.126,\"X137\":-0.503,\"X138\":0.657,\"X139\":0.244,\"X140\":0.471,\"X141\":-1.8,\"X142\":-0.744,\"X143\":-1.445,\"X144\":-2.253,\"X145\":1.167,\"X146\":0.217,\"X147\":-0.896,\"X148\":0.596,\"X149\":0.364,\"X150\":-0.402,\"X151\":-0.447,\"X152\":-1.424,\"X153\":1.844,\"X154\":-0.501,\"X155\":-1.044,\"X156\":0.762,\"X157\":0.93,\"X158\":0.398,\"X159\":-1.02,\"X160\":-1.578,\"X161\":-0.045,\"X162\":2.261,\"X163\":-0.012,\"X164\":0.731,\"X165\":0.857,\"X166\":-1.499,\"X167\":-0.955,\"X168\":-1.609,\"X169\":-0.358,\"X170\":0.887,\"X171\":0.101,\"X172\":-0.647,\"X173\":-0.963,\"X174\":-0.976,\"X175\":0.084,\"X176\":0.313,\"X177\":-0.801,\"X178\":-1.322,\"X179\":0.988,\"X180\":-2.314,\"X181\":-0.201,\"X182\":-0.688,\"X183\":0.995,\"X184\":0.511,\"X185\":-0.094,\"X186\":1.303,\"X187\":0.483,\"X188\":-0.216,\"X189\":-0.679,\"X190\":0.671,\"X191\":-0.583,\"X192\":-0.901,\"X193\":-1.116,\"X194\":1.084,\"X195\":0.129,\"X196\":-0.516,\"X197\":-0.565,\"X198\":-0.825,\"X199\":-0.155,\"X200\":-1.842,\"X201\":-0.588,\"X202\":-2.215,\"X203\":-1.181,\"X204\":-0.521,\"X205\":-1.474,\"X206\":1.665,\"X207\":-0.731,\"X208\":0.895,\"X209\":0.532,\"X210\":0.157,\"X211\":-0.063,\"X212\":-0.906,\"X213\":2.487,\"X214\":-1.9,\"X215\":0.014,\"X216\":-2.328,\"X217\":-0.924,\"X218\":0.553,\"X219\":-0.055,\"X220\":-0.078,\"X221\":-0.173,\"X222\":-0.16,\"X223\":2.247,\"X224\":-0.762,\"X225\":0.891,\"X226\":0.848,\"X227\":0.496,\"X228\":-0.905,\"X229\":0.058,\"X230\":-0.284,\"X231\":1.227,\"X232\":0.322,\"X233\":0.991,\"X234\":0.503,\"X235\":-1.511,\"X236\":0.526,\"X237\":0.109,\"X238\":-0.248,\"X239\":0.914,\"X240\":-0.142,\"X241\":-0.208,\"X242\":-0.437,\"X243\":-0.428,\"X244\":-1.019,\"X245\":1.319,\"X246\":0.212,\"X247\":-0.656,\"X248\":-0.231,\"X249\":-1.089,\"X250\":1.063,\"X251\":-2.201,\"X252\":0.068,\"X253\":0.505,\"X254\":1.71,\"X255\":-0.985,\"X256\":-1.462,\"X257\":0.063,\"X258\":-0.179,\"X259\":0.747,\"X260\":-0.356,\"X261\":0.37,\"X262\":-0.027,\"X263\":0.217,\"X264\":0.355,\"X265\":-0.316,\"X266\":1.686,\"X267\":-0.37,\"X268\":-1.1,\"X269\":-0.878,\"X270\":0.361,\"X271\":0.447,\"X272\":0.9,\"X273\":0.331,\"X274\":-0.46,\"X275\":1.334,\"X276\":0.324,\"X277\":0.655,\"X278\":-1.535,\"X279\":0.179,\"X280\":2.001,\"X281\":2.003,\"X282\":-0.631,\"X283\":-0.719,\"X284\":-2.091,\"X285\":0.166,\"X286\":0.341,\"X287\":0.208,\"X288\":0.706,\"X289\":0.182,\"X290\":-0.414,\"X291\":-0.954,\"X292\":-0.362,\"X293\":4.28,\"X294\":0.158,\"X295\":-0.006,\"X296\":0.413,\"X297\":1.058,\"X298\":-0.296,\"X299\":-0.16},{\".pred_class\":\"0\",\".pred_0\":0.6422,\".pred_1\":0.3578,\"id\":181,\"X0\":0.2,\"X1\":-0.198,\"X2\":-0.884,\"X3\":-0.065,\"X4\":0.168,\"X5\":-0.908,\"X6\":-0.856,\"X7\":0.986,\"X8\":-0.563,\"X9\":2.602,\"X10\":-0.684,\"X11\":1.296,\"X12\":-0.177,\"X13\":0.473,\"X14\":0.521,\"X15\":-0.239,\"X16\":-0.525,\"X17\":-0.605,\"X18\":2.114,\"X19\":-1.064,\"X20\":-1.211,\"X21\":0.051,\"X22\":0.454,\"X23\":1.006,\"X24\":1.654,\"X25\":-1.19,\"X26\":-0.628,\"X27\":1.239,\"X28\":-0.348,\"X29\":-0.401,\"X30\":-0.566,\"X31\":0.615,\"X32\":0.17,\"X33\":1.402,\"X34\":0.076,\"X35\":-0.54,\"X36\":-1.896,\"X37\":-1.321,\"X38\":1.431,\"X39\":0.676,\"X40\":-1.385,\"X41\":0.976,\"X42\":1.286,\"X43\":-0.024,\"X44\":0.437,\"X45\":2.017,\"X46\":0.379,\"X47\":0.997,\"X48\":0.761,\"X49\":-1.315,\"X50\":1.06,\"X51\":0.775,\"X52\":-0.779,\"X53\":0.102,\"X54\":1.136,\"X55\":0.974,\"X56\":-0.357,\"X57\":1.622,\"X58\":-0.572,\"X59\":-0.839,\"X60\":-0.043,\"X61\":0.667,\"X62\":0.78,\"X63\":-0.747,\"X64\":0.726,\"X65\":1.001,\"X66\":-1.908,\"X67\":-0.388,\"X68\":1.161,\"X69\":0.236,\"X70\":0.359,\"X71\":-1.386,\"X72\":2.109,\"X73\":-0.027,\"X74\":-1.054,\"X75\":0.423,\"X76\":-0.733,\"X77\":-0.132,\"X78\":0.59,\"X79\":1.577,\"X80\":-2.12,\"X81\":0.761,\"X82\":-0.794,\"X83\":-0.162,\"X84\":-1.221,\"X85\":-0.141,\"X86\":-0.65,\"X87\":-0.385,\"X88\":-0.206,\"X89\":0.375,\"X90\":0.316,\"X91\":0.613,\"X92\":2.061,\"X93\":0.416,\"X94\":-1.059,\"X95\":-1.129,\"X96\":1.153,\"X97\":-1.923,\"X98\":-1.694,\"X99\":-1.491,\"X100\":-0.82,\"X101\":1.444,\"X102\":0.375,\"X103\":-1.374,\"X104\":-0.786,\"X105\":-0.015,\"X106\":-0.001,\"X107\":-1.405,\"X108\":0.416,\"X109\":-1.137,\"X110\":-0.859,\"X111\":-0.919,\"X112\":-0.431,\"X113\":0.579,\"X114\":-0.401,\"X115\":0.095,\"X116\":0.957,\"X117\":-0.794,\"X118\":0.479,\"X119\":-0.144,\"X120\":-0.974,\"X121\":-0.838,\"X122\":0.149,\"X123\":-1.291,\"X124\":0.308,\"X125\":0,\"X126\":0.55,\"X127\":-0.934,\"X128\":0.924,\"X129\":0.358,\"X130\":1.482,\"X131\":0.616,\"X132\":2.092,\"X133\":1.237,\"X134\":1.245,\"X135\":-0.975,\"X136\":0.951,\"X137\":1.326,\"X138\":-0.263,\"X139\":0.747,\"X140\":-1.329,\"X141\":-1.092,\"X142\":-1.009,\"X143\":2.399,\"X144\":0.093,\"X145\":-0.015,\"X146\":0.339,\"X147\":-0.19,\"X148\":-0.732,\"X149\":-0.176,\"X150\":-0.461,\"X151\":0.821,\"X152\":0.37,\"X153\":-0.818,\"X154\":-0.91,\"X155\":0.883,\"X156\":0.077,\"X157\":0.676,\"X158\":3.724,\"X159\":-1.067,\"X160\":0.426,\"X161\":-0.959,\"X162\":-0.188,\"X163\":0.707,\"X164\":-0.519,\"X165\":1.789,\"X166\":1.427,\"X167\":1.968,\"X168\":0.55,\"X169\":-1.083,\"X170\":0.121,\"X171\":-1.903,\"X172\":-0.507,\"X173\":0.527,\"X174\":-0.588,\"X175\":1.685,\"X176\":-0.141,\"X177\":0.566,\"X178\":0.711,\"X179\":0.999,\"X180\":-0.113,\"X181\":1.92,\"X182\":-0.484,\"X183\":-0.698,\"X184\":0.416,\"X185\":1.682,\"X186\":0.165,\"X187\":1.764,\"X188\":0.166,\"X189\":1.184,\"X190\":-0.471,\"X191\":-1.962,\"X192\":-2.337,\"X193\":1.835,\"X194\":-0.449,\"X195\":-0.416,\"X196\":-0.769,\"X197\":-0.687,\"X198\":-0.852,\"X199\":0.194,\"X200\":1.126,\"X201\":-0.685,\"X202\":2.113,\"X203\":-0.447,\"X204\":0.205,\"X205\":-1.133,\"X206\":0.089,\"X207\":0.1,\"X208\":0.818,\"X209\":1.458,\"X210\":-1.025,\"X211\":-0.074,\"X212\":2.166,\"X213\":-0.31,\"X214\":1.144,\"X215\":-0.172,\"X216\":-0.454,\"X217\":-0.263,\"X218\":1.02,\"X219\":-0.995,\"X220\":-1.203,\"X221\":-0.837,\"X222\":0.64,\"X223\":0.831,\"X224\":1.362,\"X225\":-0.123,\"X226\":0.172,\"X227\":-0.412,\"X228\":-0.015,\"X229\":-0.181,\"X230\":-0.298,\"X231\":0.507,\"X232\":0.852,\"X233\":0.587,\"X234\":-0.359,\"X235\":-1.842,\"X236\":-1.102,\"X237\":-0.465,\"X238\":1.009,\"X239\":0.257,\"X240\":-1.184,\"X241\":1.028,\"X242\":1.287,\"X243\":-0.298,\"X244\":0.248,\"X245\":2.495,\"X246\":0.267,\"X247\":-1.3,\"X248\":-0.347,\"X249\":-0.331,\"X250\":0.816,\"X251\":-0.006,\"X252\":0.023,\"X253\":1.05,\"X254\":-1.08,\"X255\":-0.515,\"X256\":-0.85,\"X257\":-1.525,\"X258\":0.319,\"X259\":1.14,\"X260\":-0.297,\"X261\":1.587,\"X262\":-1.149,\"X263\":-1.117,\"X264\":1.471,\"X265\":0.53,\"X266\":-0.381,\"X267\":1.138,\"X268\":1.998,\"X269\":-1.17,\"X270\":1.348,\"X271\":0.073,\"X272\":0.014,\"X273\":0.949,\"X274\":-0.469,\"X275\":-0.326,\"X276\":0.837,\"X277\":1.323,\"X278\":0.37,\"X279\":-2.357,\"X280\":2.439,\"X281\":0.48,\"X282\":-1.339,\"X283\":0.351,\"X284\":0.366,\"X285\":0.949,\"X286\":0.799,\"X287\":1.216,\"X288\":0.441,\"X289\":-1.834,\"X290\":-2.824,\"X291\":0.729,\"X292\":-0.247,\"X293\":-0.031,\"X294\":-1.241,\"X295\":1.522,\"X296\":-0.475,\"X297\":-1.053,\"X298\":-1.152,\"X299\":1.461}]",
          "events": [],
          "completed": "2025-08-20T16:27:41.802139-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.803514-05:00",
          "working_start": 79.4624,
          "event": "model",
          "model": "gpt-4.1",
          "input": [
            {
              "id": "r2Z0FN2D5F346CNWDgrNu4",
              "content": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?",
              "source": "input",
              "role": "user"
            },
            {
              "id": "SE3AMjUkqnSTFBWvxIYyta",
              "content": [
                {
                  "type": "text",
                  "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "fhr4H1WrUonu7tBOhh8bk0",
              "content": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "cPbJsRtDBB2gZKAbGioFVw",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  }
                }
              ]
            },
            {
              "id": "3lXVD16jGmmnUPiAttwlyl",
              "content": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n",
              "role": "tool",
              "tool_call_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
              "function": "run_r_code"
            },
            {
              "id": "hNyhc1TtQT3JwcXkLVi14A",
              "content": [
                {
                  "type": "text",
                  "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "f9vmQbID5GpJLqE3BYEehz",
              "content": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "04E3qdhqWvsz6zCejbbyXg",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                  }
                }
              ]
            },
            {
              "id": "QZcjfnusNzmtpSFfTcwHkA",
              "content": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]",
              "role": "tool",
              "tool_call_id": "call_QQIBObfU48S1e8BoXwQdM4jG",
              "function": "run_r_code"
            },
            {
              "id": "LPauEfOAjhYnNIBtLsQch6",
              "content": [
                {
                  "type": "text",
                  "text": "Here are the top results from cross-validated hyperparameter tuning of regularized logistic regression (glmnet) with your 10-fold stratified CV:\n\n| Penalty  | Mixture | Mean ROC AUC | Std Err |\n|----------|---------|--------------|---------|\n| 0.0599   | 0.6667  | 0.791        | 0.021   |\n| 0.0599   | 0.5556  | 0.783        | 0.027   |\n| 0.2154   | 0.1111  | 0.776        | 0.032   |\n| 0.2154   | 0.2222  | 0.774        | 0.022   |\n| 0.0599   | 0.7778  | 0.768        | 0.023   |\n\n- The best ROC AUC (≈ 0.791) is achieved with a penalty of 0.0599 and mixture of 0.6667 (mixture controls balance between L1 and L2 regularization—here it's favoring more L1, i.e., feature selection).\n- The standard errors indicate stability across folds.\n\nWould you like to:\n1. Use these best-tuned parameters to fit the final model and generate test set predictions for submission?\n2. Explore the selected features or coefficients?\n3. Try an alternate approach (e.g., with feature filtering or PCA)?\n\nLet me know your next step!"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            {
              "id": "eGQ04UetmBWt8uQlUy0rGE",
              "content": "Let's go with option 1 - use the best-tuned parameters to fit the final model and generate test set predictions for submission.",
              "source": "input",
              "role": "user"
            },
            {
              "id": "V0dfSL9qB5bktYoElEKnZa",
              "content": [
                {
                  "type": "text",
                  "text": ""
                }
              ],
              "source": "generate",
              "role": "assistant",
              "tool_calls": [
                {
                  "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
                  "function": "run_r_code",
                  "arguments": {
                    "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
                  }
                }
              ]
            },
            {
              "id": "W9lhy96J0U7QcoK8hhVndO",
              "content": "[1]  50 304\n\n[{\".pred_class\":\"0\",\".pred_0\":0.6901,\".pred_1\":0.3099,\"id\":142,\"X0\":0.081,\"X1\":0.312,\"X2\":0.015,\"X3\":0.688,\"X4\":-0.145,\"X5\":0.894,\"X6\":-0.153,\"X7\":-1.351,\"X8\":-0.183,\"X9\":0.627,\"X10\":0.262,\"X11\":0.496,\"X12\":0.018,\"X13\":-1.367,\"X14\":0.65,\"X15\":-0.943,\"X16\":0.312,\"X17\":0.406,\"X18\":3.117,\"X19\":0.659,\"X20\":0.788,\"X21\":0.355,\"X22\":-0.735,\"X23\":-0.037,\"X24\":2.214,\"X25\":1.731,\"X26\":-1.157,\"X27\":0.398,\"X28\":1.195,\"X29\":1.141,\"X30\":-1.266,\"X31\":0.349,\"X32\":0.93,\"X33\":1.282,\"X34\":-0.271,\"X35\":-0.947,\"X36\":0.259,\"X37\":2.251,\"X38\":0.242,\"X39\":0.28,\"X40\":-0.011,\"X41\":1.209,\"X42\":0.278,\"X43\":1.048,\"X44\":0.079,\"X45\":-1.25,\"X46\":0.113,\"X47\":0.984,\"X48\":0.51,\"X49\":0.526,\"X50\":-0.956,\"X51\":0.04,\"X52\":-0.632,\"X53\":0.421,\"X54\":0.612,\"X55\":-1.21,\"X56\":1.458,\"X57\":-0.483,\"X58\":1.008,\"X59\":0.986,\"X60\":-0.998,\"X61\":0.664,\"X62\":-0.764,\"X63\":-1.109,\"X64\":0.715,\"X65\":0.161,\"X66\":0.077,\"X67\":-0.869,\"X68\":0.807,\"X69\":1.097,\"X70\":0.996,\"X71\":-0.673,\"X72\":-1.651,\"X73\":-0.33,\"X74\":-0.088,\"X75\":0.397,\"X76\":-0.643,\"X77\":-0.462,\"X78\":0.866,\"X79\":-1.627,\"X80\":0.795,\"X81\":-0.706,\"X82\":0.158,\"X83\":-1.204,\"X84\":0.908,\"X85\":0.255,\"X86\":-0.031,\"X87\":-0.137,\"X88\":0.197,\"X89\":-0.125,\"X90\":-0.969,\"X91\":-0.133,\"X92\":-0.333,\"X93\":-1,\"X94\":0.391,\"X95\":-0.957,\"X96\":0.13,\"X97\":0.164,\"X98\":-0.403,\"X99\":1.591,\"X100\":0.323,\"X101\":-0.845,\"X102\":1.018,\"X103\":2.033,\"X104\":1.161,\"X105\":2.87,\"X106\":-0.426,\"X107\":1.361,\"X108\":-0.204,\"X109\":0.524,\"X110\":0.736,\"X111\":0.274,\"X112\":-0.76,\"X113\":-1.589,\"X114\":-0.053,\"X115\":-0.992,\"X116\":-0.431,\"X117\":-0.413,\"X118\":0.788,\"X119\":-0.017,\"X120\":0.068,\"X121\":-0.074,\"X122\":0.95,\"X123\":-0.788,\"X124\":-0.567,\"X125\":-1.259,\"X126\":0.914,\"X127\":-1.595,\"X128\":0.202,\"X129\":0.128,\"X130\":0.883,\"X131\":-0.147,\"X132\":-1.395,\"X133\":-0.831,\"X134\":-2.199,\"X135\":-0.967,\"X136\":0.592,\"X137\":0.363,\"X138\":0.941,\"X139\":0.548,\"X140\":-0.109,\"X141\":0.641,\"X142\":0.687,\"X143\":0.271,\"X144\":-1.243,\"X145\":1.207,\"X146\":-0.122,\"X147\":-2.6,\"X148\":0.234,\"X149\":0.218,\"X150\":0.739,\"X151\":-0.118,\"X152\":-0.69,\"X153\":-0.476,\"X154\":-0.311,\"X155\":0.593,\"X156\":-0.789,\"X157\":-0.333,\"X158\":0.757,\"X159\":-0.024,\"X160\":-0.047,\"X161\":-1.422,\"X162\":1.395,\"X163\":2.555,\"X164\":1.277,\"X165\":0.499,\"X166\":-0.457,\"X167\":0.076,\"X168\":-0.459,\"X169\":1.094,\"X170\":-1.078,\"X171\":0.435,\"X172\":-0.664,\"X173\":-0.829,\"X174\":-0.509,\"X175\":-0.106,\"X176\":-0.803,\"X177\":-0.495,\"X178\":-0.545,\"X179\":0.293,\"X180\":-0.492,\"X181\":0.404,\"X182\":1.435,\"X183\":-1.011,\"X184\":1.116,\"X185\":0.406,\"X186\":0.164,\"X187\":0.18,\"X188\":-0.286,\"X189\":0.437,\"X190\":0.282,\"X191\":-0.353,\"X192\":0.578,\"X193\":-1.131,\"X194\":0.499,\"X195\":-1.052,\"X196\":0.487,\"X197\":0.114,\"X198\":-0.304,\"X199\":-1.416,\"X200\":0.362,\"X201\":-0.624,\"X202\":-0.066,\"X203\":1.391,\"X204\":-0.896,\"X205\":-0.401,\"X206\":0.633,\"X207\":1.132,\"X208\":-1.979,\"X209\":0.208,\"X210\":-1.051,\"X211\":1.235,\"X212\":-0.821,\"X213\":-0.78,\"X214\":0.142,\"X215\":2.014,\"X216\":-1.135,\"X217\":-0.261,\"X218\":-0.323,\"X219\":-0.965,\"X220\":1.067,\"X221\":-0.371,\"X222\":0.496,\"X223\":1.009,\"X224\":1.725,\"X225\":0.719,\"X226\":0.106,\"X227\":-0.01,\"X228\":0.501,\"X229\":-0.263,\"X230\":-0.605,\"X231\":-0.94,\"X232\":-1.897,\"X233\":1.156,\"X234\":-0.299,\"X235\":-0.655,\"X236\":0.429,\"X237\":-0.557,\"X238\":0.766,\"X239\":0.008,\"X240\":-0.925,\"X241\":-2.201,\"X242\":-0.206,\"X243\":-0.224,\"X244\":1.103,\"X245\":1.027,\"X246\":1.689,\"X247\":-0.66,\"X248\":-0.019,\"X249\":1.429,\"X250\":-1.661,\"X251\":-0.328,\"X252\":-0.29,\"X253\":1.005,\"X254\":-1.328,\"X255\":0.29,\"X256\":1.387,\"X257\":-1.331,\"X258\":0.615,\"X259\":-0.987,\"X260\":0.869,\"X261\":-0.84,\"X262\":0.405,\"X263\":0.625,\"X264\":-1.491,\"X265\":-0.896,\"X266\":-0.056,\"X267\":0.526,\"X268\":-1.403,\"X269\":0.281,\"X270\":-0.886,\"X271\":2.277,\"X272\":-0.882,\"X273\":-0.155,\"X274\":0.005,\"X275\":-0.203,\"X276\":-1.682,\"X277\":0.182,\"X278\":-0.932,\"X279\":0.916,\"X280\":0.11,\"X281\":-0.224,\"X282\":0.632,\"X283\":0.617,\"X284\":0.098,\"X285\":-1.351,\"X286\":0.922,\"X287\":1.724,\"X288\":0.507,\"X289\":0.182,\"X290\":0.307,\"X291\":-0.62,\"X292\":1.04,\"X293\":0.224,\"X294\":-0.678,\"X295\":-0.596,\"X296\":0.203,\"X297\":1.138,\"X298\":1.896,\"X299\":0.952},{\".pred_class\":\"1\",\".pred_0\":0.4394,\".pred_1\":0.5606,\"id\":6,\"X0\":1.257,\"X1\":-1.562,\"X2\":0.832,\"X3\":0.432,\"X4\":1.667,\"X5\":0.843,\"X6\":-0.037,\"X7\":-0.96,\"X8\":-0.311,\"X9\":0.173,\"X10\":-0.059,\"X11\":1.744,\"X12\":-0.92,\"X13\":-0.077,\"X14\":-0.818,\"X15\":-2.053,\"X16\":-2.042,\"X17\":0.219,\"X18\":0.508,\"X19\":-0.659,\"X20\":0.063,\"X21\":-0.443,\"X22\":1.086,\"X23\":-2.087,\"X24\":0.227,\"X25\":1.854,\"X26\":0.211,\"X27\":0.172,\"X28\":0.815,\"X29\":-0.013,\"X30\":-0.408,\"X31\":-1.385,\"X32\":1.2,\"X33\":-0.02,\"X34\":0.554,\"X35\":-0.076,\"X36\":0.452,\"X37\":-0.147,\"X38\":-0.61,\"X39\":-0.907,\"X40\":0.283,\"X41\":0.518,\"X42\":-3.191,\"X43\":-0.975,\"X44\":-1.255,\"X45\":-0.663,\"X46\":2.072,\"X47\":0.087,\"X48\":0.418,\"X49\":-0.067,\"X50\":-0.028,\"X51\":0.266,\"X52\":-0.56,\"X53\":0.389,\"X54\":0.777,\"X55\":-0.858,\"X56\":1.434,\"X57\":1.907,\"X58\":-2.262,\"X59\":-0.727,\"X60\":0.709,\"X61\":0.575,\"X62\":-0.504,\"X63\":-0.113,\"X64\":0.643,\"X65\":-0.22,\"X66\":-0.828,\"X67\":0.402,\"X68\":0.806,\"X69\":0.154,\"X70\":0.052,\"X71\":-0.5,\"X72\":0.832,\"X73\":-0.488,\"X74\":-0.215,\"X75\":-0.88,\"X76\":0.373,\"X77\":0.766,\"X78\":0.518,\"X79\":0.508,\"X80\":-0.609,\"X81\":-2.461,\"X82\":0.302,\"X83\":-0.252,\"X84\":1.067,\"X85\":1.455,\"X86\":0.424,\"X87\":0.959,\"X88\":-0.472,\"X89\":-1.019,\"X90\":-1.926,\"X91\":-0.351,\"X92\":-0.755,\"X93\":-1.043,\"X94\":-0.676,\"X95\":-0.688,\"X96\":1.477,\"X97\":-1.266,\"X98\":0.062,\"X99\":0.967,\"X100\":-1.343,\"X101\":1.48,\"X102\":-0.591,\"X103\":0.449,\"X104\":1.438,\"X105\":0.822,\"X106\":-0.792,\"X107\":-0.007,\"X108\":-0.409,\"X109\":-0.945,\"X110\":-1.055,\"X111\":-0.867,\"X112\":-0.559,\"X113\":-0.279,\"X114\":-0.866,\"X115\":-0.741,\"X116\":0.051,\"X117\":0.921,\"X118\":-1.204,\"X119\":0.62,\"X120\":-1.104,\"X121\":-0.565,\"X122\":1.373,\"X123\":0.581,\"X124\":-0.666,\"X125\":1.256,\"X126\":0.923,\"X127\":2.11,\"X128\":-0.56,\"X129\":-1.786,\"X130\":-1.588,\"X131\":1.139,\"X132\":-0.287,\"X133\":-1.48,\"X134\":2.327,\"X135\":-0.73,\"X136\":1.293,\"X137\":0.639,\"X138\":-1.781,\"X139\":0.249,\"X140\":-0.57,\"X141\":1.124,\"X142\":-0.442,\"X143\":-0.806,\"X144\":-0.5,\"X145\":0.664,\"X146\":-0.425,\"X147\":-0.007,\"X148\":-0.775,\"X149\":0.51,\"X150\":0.408,\"X151\":1.717,\"X152\":0.095,\"X153\":0.2,\"X154\":0.714,\"X155\":-1.086,\"X156\":-0.322,\"X157\":0.58,\"X158\":0.745,\"X159\":-1.32,\"X160\":0.587,\"X161\":-1.219,\"X162\":0.843,\"X163\":1.561,\"X164\":-0.092,\"X165\":-0.815,\"X166\":0.699,\"X167\":-1.377,\"X168\":1.503,\"X169\":0.496,\"X170\":1.064,\"X171\":-1.636,\"X172\":0.508,\"X173\":-0.002,\"X174\":-0.503,\"X175\":-0.703,\"X176\":-1.114,\"X177\":0.991,\"X178\":-0.664,\"X179\":-0.927,\"X180\":-0.715,\"X181\":0.594,\"X182\":1.306,\"X183\":0.162,\"X184\":0.095,\"X185\":0.833,\"X186\":-0.708,\"X187\":1.401,\"X188\":-1.953,\"X189\":-1.42,\"X190\":-0.638,\"X191\":-0.163,\"X192\":-1.514,\"X193\":1.096,\"X194\":-0.172,\"X195\":0.357,\"X196\":-1.562,\"X197\":0.154,\"X198\":0.726,\"X199\":-0.054,\"X200\":0.938,\"X201\":-0.326,\"X202\":-0.314,\"X203\":-0.234,\"X204\":-0.781,\"X205\":-0.914,\"X206\":-2.33,\"X207\":1.627,\"X208\":-2.215,\"X209\":-0.724,\"X210\":-1.439,\"X211\":-0.68,\"X212\":0.747,\"X213\":-0.852,\"X214\":-0.187,\"X215\":0.34,\"X216\":-0.417,\"X217\":0.846,\"X218\":-0.065,\"X219\":-1.632,\"X220\":-0.16,\"X221\":0.577,\"X222\":-0.832,\"X223\":-0.773,\"X224\":1.068,\"X225\":-0.442,\"X226\":-0.362,\"X227\":-1.214,\"X228\":0.852,\"X229\":0.239,\"X230\":-0.101,\"X231\":0.131,\"X232\":0.78,\"X233\":1.369,\"X234\":-2.154,\"X235\":-0.668,\"X236\":1.497,\"X237\":-0.209,\"X238\":-1.053,\"X239\":0.336,\"X240\":0.862,\"X241\":-1.926,\"X242\":-1.013,\"X243\":0.508,\"X244\":-0.862,\"X245\":0.749,\"X246\":0.539,\"X247\":-0.303,\"X248\":-1.239,\"X249\":-1.961,\"X250\":-0.612,\"X251\":0.687,\"X252\":-0.49,\"X253\":-0.648,\"X254\":-0.537,\"X255\":0.074,\"X256\":0.101,\"X257\":0.158,\"X258\":0.594,\"X259\":0.72,\"X260\":-0.604,\"X261\":0.566,\"X262\":1.239,\"X263\":0.27,\"X264\":0.588,\"X265\":-0.207,\"X266\":-1.395,\"X267\":-1.579,\"X268\":-0.141,\"X269\":-0.236,\"X270\":-0.538,\"X271\":0.061,\"X272\":0.642,\"X273\":0.118,\"X274\":-0.15,\"X275\":-1.399,\"X276\":-0.423,\"X277\":0.553,\"X278\":-0.857,\"X279\":0.393,\"X280\":0.958,\"X281\":-0.124,\"X282\":1.379,\"X283\":-0.748,\"X284\":0.218,\"X285\":-1.407,\"X286\":-0.739,\"X287\":0.59,\"X288\":0.044,\"X289\":-0.268,\"X290\":0.024,\"X291\":0.391,\"X292\":0.547,\"X293\":-0.416,\"X294\":-0.333,\"X295\":0.857,\"X296\":0.422,\"X297\":-0.833,\"X298\":-1.055,\"X299\":0.439},{\".pred_class\":\"0\",\".pred_0\":0.8511,\".pred_1\":0.1489,\"id\":97,\"X0\":0.66,\"X1\":0.126,\"X2\":-1.829,\"X3\":-0.73,\"X4\":-0.683,\"X5\":-0.943,\"X6\":0.892,\"X7\":0.121,\"X8\":-1.003,\"X9\":0.398,\"X10\":1.396,\"X11\":1.681,\"X12\":0.089,\"X13\":0.008,\"X14\":0.479,\"X15\":-0.354,\"X16\":0.253,\"X17\":-0.918,\"X18\":-0.948,\"X19\":2.087,\"X20\":0.003,\"X21\":-0.526,\"X22\":0.961,\"X23\":1.105,\"X24\":-2.217,\"X25\":-0.206,\"X26\":-0.709,\"X27\":-1.434,\"X28\":-0.181,\"X29\":-1.473,\"X30\":0.725,\"X31\":0.658,\"X32\":0.137,\"X33\":0.68,\"X34\":-0.512,\"X35\":1.037,\"X36\":-0.293,\"X37\":0.445,\"X38\":0.271,\"X39\":2.01,\"X40\":-0.07,\"X41\":0.754,\"X42\":-0.961,\"X43\":0.341,\"X44\":0.965,\"X45\":0.328,\"X46\":0.167,\"X47\":-0.319,\"X48\":-0.456,\"X49\":1.108,\"X50\":0.535,\"X51\":-1.703,\"X52\":0.505,\"X53\":0.072,\"X54\":1.713,\"X55\":0.562,\"X56\":-0.745,\"X57\":0.055,\"X58\":-0.82,\"X59\":0.937,\"X60\":0.426,\"X61\":0.441,\"X62\":-0.919,\"X63\":0.676,\"X64\":0.184,\"X65\":0.591,\"X66\":-0.006,\"X67\":1.754,\"X68\":-0.337,\"X69\":-0.198,\"X70\":-0.796,\"X71\":-1.298,\"X72\":-0.737,\"X73\":1.285,\"X74\":-1.637,\"X75\":-0.529,\"X76\":0.377,\"X77\":0.212,\"X78\":-0.727,\"X79\":-0.905,\"X80\":-0.701,\"X81\":-0.312,\"X82\":-0.463,\"X83\":-1.797,\"X84\":0.664,\"X85\":-0.171,\"X86\":0.559,\"X87\":0.636,\"X88\":-0.209,\"X89\":0.064,\"X90\":0.144,\"X91\":-0.818,\"X92\":0.7,\"X93\":-0.364,\"X94\":-0.333,\"X95\":-0.191,\"X96\":-1.579,\"X97\":2.081,\"X98\":-1.575,\"X99\":-0.685,\"X100\":-0.385,\"X101\":0.716,\"X102\":-0.767,\"X103\":-0.756,\"X104\":-2.331,\"X105\":-0.586,\"X106\":0.764,\"X107\":-0.622,\"X108\":0.651,\"X109\":0.071,\"X110\":-0.091,\"X111\":-0.129,\"X112\":-1.848,\"X113\":-0.354,\"X114\":0.428,\"X115\":0.928,\"X116\":0.557,\"X117\":-0.181,\"X118\":-0.258,\"X119\":-0.872,\"X120\":-0.864,\"X121\":-0.257,\"X122\":0.795,\"X123\":0.882,\"X124\":-0.519,\"X125\":-0.673,\"X126\":2.144,\"X127\":-0.595,\"X128\":0.624,\"X129\":1.164,\"X130\":0.136,\"X131\":0.52,\"X132\":0.423,\"X133\":0.457,\"X134\":-0.374,\"X135\":-1.384,\"X136\":2.57,\"X137\":0.223,\"X138\":0.217,\"X139\":0.364,\"X140\":-0.132,\"X141\":-1.192,\"X142\":0.471,\"X143\":-0.172,\"X144\":-1.586,\"X145\":-0.091,\"X146\":0.58,\"X147\":-0.43,\"X148\":0.073,\"X149\":-0.315,\"X150\":-0.122,\"X151\":-0.988,\"X152\":-2.52,\"X153\":-0.831,\"X154\":1.146,\"X155\":-0.883,\"X156\":0.723,\"X157\":0.452,\"X158\":-0.237,\"X159\":-0.789,\"X160\":0.422,\"X161\":-1.003,\"X162\":-1.352,\"X163\":1.384,\"X164\":-1.245,\"X165\":-1.602,\"X166\":1.604,\"X167\":-0.947,\"X168\":-0.573,\"X169\":0.903,\"X170\":-0.446,\"X171\":-1.406,\"X172\":-0.847,\"X173\":0.586,\"X174\":-0.16,\"X175\":-2.209,\"X176\":0.685,\"X177\":-0.956,\"X178\":1.433,\"X179\":-1.486,\"X180\":-0.167,\"X181\":0.788,\"X182\":-0.92,\"X183\":-1.12,\"X184\":-0.957,\"X185\":1.929,\"X186\":-1.875,\"X187\":1.304,\"X188\":0.113,\"X189\":1.808,\"X190\":0.525,\"X191\":1.976,\"X192\":0.233,\"X193\":-0.247,\"X194\":0.525,\"X195\":0.253,\"X196\":-0.546,\"X197\":-2.025,\"X198\":-0.252,\"X199\":0.894,\"X200\":-1.113,\"X201\":-0.049,\"X202\":1.231,\"X203\":1.905,\"X204\":-1.276,\"X205\":0.054,\"X206\":-0.264,\"X207\":-0.173,\"X208\":0.239,\"X209\":-1.871,\"X210\":0.605,\"X211\":0.095,\"X212\":0.483,\"X213\":1.36,\"X214\":0.074,\"X215\":0.534,\"X216\":-1.597,\"X217\":-0.818,\"X218\":-0.413,\"X219\":0.21,\"X220\":-0.009,\"X221\":-1.321,\"X222\":1.381,\"X223\":-0.73,\"X224\":-0.465,\"X225\":1.11,\"X226\":0.656,\"X227\":-0.399,\"X228\":-1.875,\"X229\":-0.567,\"X230\":0.552,\"X231\":-0.007,\"X232\":1.404,\"X233\":-0.936,\"X234\":-1.252,\"X235\":0.122,\"X236\":1.556,\"X237\":1.066,\"X238\":0.018,\"X239\":0.56,\"X240\":0.54,\"X241\":-1.882,\"X242\":1.559,\"X243\":1.769,\"X244\":-1.09,\"X245\":0.528,\"X246\":-0.695,\"X247\":1.028,\"X248\":0.413,\"X249\":-1.269,\"X250\":-0.887,\"X251\":0.516,\"X252\":-0.938,\"X253\":-0.33,\"X254\":0.9,\"X255\":-0.383,\"X256\":-1.226,\"X257\":0.247,\"X258\":0.399,\"X259\":-0.931,\"X260\":-0.873,\"X261\":-1.91,\"X262\":0.211,\"X263\":-0.413,\"X264\":0.116,\"X265\":-2.179,\"X266\":-0.908,\"X267\":1.206,\"X268\":0.374,\"X269\":-1.129,\"X270\":-0.577,\"X271\":0.48,\"X272\":-0.584,\"X273\":0.195,\"X274\":0.526,\"X275\":-0.163,\"X276\":-0.77,\"X277\":-0.239,\"X278\":-0.966,\"X279\":1.684,\"X280\":-1.159,\"X281\":-1.329,\"X282\":-0.724,\"X283\":0.221,\"X284\":-1.771,\"X285\":0.25,\"X286\":-1.131,\"X287\":0.609,\"X288\":-1.171,\"X289\":-0.191,\"X290\":1.675,\"X291\":-0.182,\"X292\":0.143,\"X293\":0.302,\"X294\":-1.068,\"X295\":0.445,\"X296\":-0.223,\"X297\":1.092,\"X298\":0.816,\"X299\":0.794},{\".pred_class\":\"0\",\".pred_0\":0.7855,\".pred_1\":0.2145,\"id\":60,\"X0\":-0.071,\"X1\":2.146,\"X2\":-0.512,\"X3\":1.304,\"X4\":-1.778,\"X5\":1.146,\"X6\":0.335,\"X7\":0.13,\"X8\":0.386,\"X9\":1.755,\"X10\":-1.175,\"X11\":0.851,\"X12\":0.31,\"X13\":0.969,\"X14\":-0.229,\"X15\":0.234,\"X16\":-0.049,\"X17\":0.286,\"X18\":0.204,\"X19\":-0.434,\"X20\":-0.726,\"X21\":-1.615,\"X22\":0.804,\"X23\":0.181,\"X24\":1.754,\"X25\":-0.669,\"X26\":-0.564,\"X27\":1.306,\"X28\":-0.11,\"X29\":0.954,\"X30\":-1.445,\"X31\":-1.2,\"X32\":0.553,\"X33\":2.311,\"X34\":-0.637,\"X35\":0.314,\"X36\":-0.289,\"X37\":0.029,\"X38\":0.128,\"X39\":-0.183,\"X40\":-1.007,\"X41\":1.086,\"X42\":-0.1,\"X43\":-1.01,\"X44\":0.306,\"X45\":-2.516,\"X46\":1.955,\"X47\":-0.879,\"X48\":1.855,\"X49\":0.111,\"X50\":-0.721,\"X51\":0.026,\"X52\":0.366,\"X53\":0.463,\"X54\":1.041,\"X55\":-1.566,\"X56\":0.172,\"X57\":-0.219,\"X58\":-1.044,\"X59\":0.475,\"X60\":0.281,\"X61\":-0.105,\"X62\":-0.397,\"X63\":0.155,\"X64\":2.039,\"X65\":-0.86,\"X66\":-0.129,\"X67\":-0.4,\"X68\":-0.076,\"X69\":0.698,\"X70\":0.971,\"X71\":-0.61,\"X72\":-1.25,\"X73\":-0.711,\"X74\":-0.96,\"X75\":0.295,\"X76\":0.097,\"X77\":-0.198,\"X78\":0.275,\"X79\":-0.379,\"X80\":-1.792,\"X81\":-0.38,\"X82\":-0.521,\"X83\":-0.41,\"X84\":-1.741,\"X85\":-0.202,\"X86\":0.538,\"X87\":-1.158,\"X88\":0.714,\"X89\":1.3,\"X90\":0.905,\"X91\":-1.355,\"X92\":-1.109,\"X93\":-0,\"X94\":-1.058,\"X95\":0.603,\"X96\":0.404,\"X97\":0.307,\"X98\":-0.113,\"X99\":0.446,\"X100\":-1.063,\"X101\":0.132,\"X102\":1.218,\"X103\":-1.725,\"X104\":0.358,\"X105\":-0.004,\"X106\":-0.556,\"X107\":-0.77,\"X108\":2.218,\"X109\":-0.513,\"X110\":0.808,\"X111\":-0.594,\"X112\":0.812,\"X113\":-1.549,\"X114\":0.898,\"X115\":0.437,\"X116\":-0.686,\"X117\":0.703,\"X118\":0.07,\"X119\":-0.489,\"X120\":-1.282,\"X121\":-0.653,\"X122\":-0.475,\"X123\":0.428,\"X124\":-0.579,\"X125\":0.547,\"X126\":0.361,\"X127\":-2.15,\"X128\":0.145,\"X129\":-0.7,\"X130\":-0.777,\"X131\":0.313,\"X132\":-0.418,\"X133\":0.387,\"X134\":0.797,\"X135\":1.424,\"X136\":-2.685,\"X137\":-0.982,\"X138\":-0.313,\"X139\":0.635,\"X140\":-0.318,\"X141\":-0.388,\"X142\":0.809,\"X143\":1.091,\"X144\":-0.885,\"X145\":0.149,\"X146\":-0.512,\"X147\":-1.851,\"X148\":0.236,\"X149\":-1.27,\"X150\":-0.141,\"X151\":-1.346,\"X152\":0.313,\"X153\":0.685,\"X154\":-0.042,\"X155\":-0.484,\"X156\":-1.437,\"X157\":-0.778,\"X158\":0.25,\"X159\":-0.738,\"X160\":0.937,\"X161\":-1.204,\"X162\":-0.567,\"X163\":0.583,\"X164\":0.174,\"X165\":-1.473,\"X166\":0.7,\"X167\":0.05,\"X168\":0.992,\"X169\":1.782,\"X170\":-0.59,\"X171\":0.429,\"X172\":0.843,\"X173\":1.086,\"X174\":0.298,\"X175\":-0.161,\"X176\":-0.526,\"X177\":-0.447,\"X178\":-1.108,\"X179\":0.149,\"X180\":0.655,\"X181\":-0.943,\"X182\":1.006,\"X183\":1.047,\"X184\":-1.297,\"X185\":0.186,\"X186\":0.187,\"X187\":-1.078,\"X188\":-0.307,\"X189\":-1.081,\"X190\":-1.492,\"X191\":-0.232,\"X192\":-1.353,\"X193\":-1.49,\"X194\":-0.081,\"X195\":-0.158,\"X196\":0.727,\"X197\":-1.28,\"X198\":0.622,\"X199\":1.45,\"X200\":0.041,\"X201\":2.502,\"X202\":-0.37,\"X203\":1.711,\"X204\":-0.713,\"X205\":1.455,\"X206\":0.545,\"X207\":-0.15,\"X208\":0.047,\"X209\":0.503,\"X210\":-0.438,\"X211\":-0.44,\"X212\":-0.209,\"X213\":1.295,\"X214\":-0.375,\"X215\":0.399,\"X216\":0.889,\"X217\":-0.034,\"X218\":0.682,\"X219\":1.688,\"X220\":0.441,\"X221\":-1.276,\"X222\":1.687,\"X223\":0.027,\"X224\":-0.89,\"X225\":-0.178,\"X226\":0.602,\"X227\":1.752,\"X228\":0.08,\"X229\":0.056,\"X230\":-1.158,\"X231\":-0.211,\"X232\":0.084,\"X233\":-1.743,\"X234\":0.623,\"X235\":0.834,\"X236\":0.59,\"X237\":0.9,\"X238\":0.932,\"X239\":-0.543,\"X240\":0.163,\"X241\":0.079,\"X242\":0.352,\"X243\":-0.731,\"X244\":0.293,\"X245\":0.862,\"X246\":1.428,\"X247\":-0.478,\"X248\":1.253,\"X249\":0.414,\"X250\":0.816,\"X251\":0.125,\"X252\":1.251,\"X253\":1.374,\"X254\":0.282,\"X255\":-0.087,\"X256\":-0.016,\"X257\":0.162,\"X258\":-0.978,\"X259\":1.223,\"X260\":-1.505,\"X261\":-0.25,\"X262\":0.682,\"X263\":0.007,\"X264\":-0.277,\"X265\":1.278,\"X266\":0.097,\"X267\":-0.571,\"X268\":-0.597,\"X269\":0.403,\"X270\":-0.201,\"X271\":-2.726,\"X272\":1.972,\"X273\":0.36,\"X274\":-0.99,\"X275\":-0.264,\"X276\":0.024,\"X277\":1.257,\"X278\":-0.106,\"X279\":0.764,\"X280\":0.101,\"X281\":-1.368,\"X282\":-0.714,\"X283\":-2.981,\"X284\":-0.873,\"X285\":-0.952,\"X286\":1.04,\"X287\":0.248,\"X288\":0.025,\"X289\":-0.413,\"X290\":0.126,\"X291\":0.722,\"X292\":-0.617,\"X293\":-0.758,\"X294\":0.454,\"X295\":0.233,\"X296\":1.097,\"X297\":-0.647,\"X298\":0.467,\"X299\":-1.155},{\".pred_class\":\"0\",\".pred_0\":0.822,\".pred_1\":0.178,\"id\":112,\"X0\":-1.158,\"X1\":-0.183,\"X2\":-0.719,\"X3\":-0.099,\"X4\":0.419,\"X5\":1.298,\"X6\":-1.224,\"X7\":0.171,\"X8\":-0.395,\"X9\":-0.003,\"X10\":-0.162,\"X11\":0.657,\"X12\":-0.903,\"X13\":-0.491,\"X14\":0.685,\"X15\":1.289,\"X16\":1.36,\"X17\":0.223,\"X18\":-1.566,\"X19\":-0.147,\"X20\":-0.488,\"X21\":1.168,\"X22\":-1.355,\"X23\":-0.384,\"X24\":-1.612,\"X25\":0.383,\"X26\":0.223,\"X27\":0.929,\"X28\":-0.542,\"X29\":0.339,\"X30\":0.92,\"X31\":0.577,\"X32\":-0.335,\"X33\":-0.693,\"X34\":-0.205,\"X35\":0.145,\"X36\":1.11,\"X37\":-1.154,\"X38\":0.157,\"X39\":1.352,\"X40\":0.619,\"X41\":-1.112,\"X42\":-0.135,\"X43\":0.051,\"X44\":-0.048,\"X45\":-0.276,\"X46\":0.181,\"X47\":-1.416,\"X48\":-0.336,\"X49\":1.106,\"X50\":-0.474,\"X51\":0.684,\"X52\":-0.233,\"X53\":0.591,\"X54\":-2.135,\"X55\":-0.21,\"X56\":1.15,\"X57\":1.12,\"X58\":-0.644,\"X59\":0.02,\"X60\":0.344,\"X61\":-0.092,\"X62\":0.207,\"X63\":-1.083,\"X64\":0.973,\"X65\":0.074,\"X66\":-2.224,\"X67\":-1.071,\"X68\":0.1,\"X69\":0.275,\"X70\":-1.283,\"X71\":0.511,\"X72\":-0.841,\"X73\":2.441,\"X74\":-0.413,\"X75\":-0.333,\"X76\":0.582,\"X77\":-0.598,\"X78\":-2.841,\"X79\":-1.705,\"X80\":-0.548,\"X81\":-0.002,\"X82\":0.541,\"X83\":-0.386,\"X84\":0.181,\"X85\":0.65,\"X86\":-0.357,\"X87\":-1.619,\"X88\":0.785,\"X89\":0.036,\"X90\":-0.517,\"X91\":0.889,\"X92\":0.476,\"X93\":0.06,\"X94\":1.889,\"X95\":-0.403,\"X96\":0.33,\"X97\":-0.965,\"X98\":-0.693,\"X99\":-0.847,\"X100\":-0.303,\"X101\":1.452,\"X102\":0.04,\"X103\":1.22,\"X104\":-1.494,\"X105\":0.92,\"X106\":0.305,\"X107\":0.066,\"X108\":1.088,\"X109\":2.123,\"X110\":0.204,\"X111\":-0.793,\"X112\":0.169,\"X113\":-0.437,\"X114\":-0.565,\"X115\":-1.62,\"X116\":0.855,\"X117\":0.316,\"X118\":1.421,\"X119\":-1.217,\"X120\":-2.034,\"X121\":0.494,\"X122\":0.277,\"X123\":1.948,\"X124\":-0.8,\"X125\":-0.812,\"X126\":0.884,\"X127\":-0.298,\"X128\":-0.072,\"X129\":2.459,\"X130\":0.036,\"X131\":-2.704,\"X132\":0.573,\"X133\":0.07,\"X134\":-0.061,\"X135\":-0.049,\"X136\":-0.126,\"X137\":-0.503,\"X138\":0.657,\"X139\":0.244,\"X140\":0.471,\"X141\":-1.8,\"X142\":-0.744,\"X143\":-1.445,\"X144\":-2.253,\"X145\":1.167,\"X146\":0.217,\"X147\":-0.896,\"X148\":0.596,\"X149\":0.364,\"X150\":-0.402,\"X151\":-0.447,\"X152\":-1.424,\"X153\":1.844,\"X154\":-0.501,\"X155\":-1.044,\"X156\":0.762,\"X157\":0.93,\"X158\":0.398,\"X159\":-1.02,\"X160\":-1.578,\"X161\":-0.045,\"X162\":2.261,\"X163\":-0.012,\"X164\":0.731,\"X165\":0.857,\"X166\":-1.499,\"X167\":-0.955,\"X168\":-1.609,\"X169\":-0.358,\"X170\":0.887,\"X171\":0.101,\"X172\":-0.647,\"X173\":-0.963,\"X174\":-0.976,\"X175\":0.084,\"X176\":0.313,\"X177\":-0.801,\"X178\":-1.322,\"X179\":0.988,\"X180\":-2.314,\"X181\":-0.201,\"X182\":-0.688,\"X183\":0.995,\"X184\":0.511,\"X185\":-0.094,\"X186\":1.303,\"X187\":0.483,\"X188\":-0.216,\"X189\":-0.679,\"X190\":0.671,\"X191\":-0.583,\"X192\":-0.901,\"X193\":-1.116,\"X194\":1.084,\"X195\":0.129,\"X196\":-0.516,\"X197\":-0.565,\"X198\":-0.825,\"X199\":-0.155,\"X200\":-1.842,\"X201\":-0.588,\"X202\":-2.215,\"X203\":-1.181,\"X204\":-0.521,\"X205\":-1.474,\"X206\":1.665,\"X207\":-0.731,\"X208\":0.895,\"X209\":0.532,\"X210\":0.157,\"X211\":-0.063,\"X212\":-0.906,\"X213\":2.487,\"X214\":-1.9,\"X215\":0.014,\"X216\":-2.328,\"X217\":-0.924,\"X218\":0.553,\"X219\":-0.055,\"X220\":-0.078,\"X221\":-0.173,\"X222\":-0.16,\"X223\":2.247,\"X224\":-0.762,\"X225\":0.891,\"X226\":0.848,\"X227\":0.496,\"X228\":-0.905,\"X229\":0.058,\"X230\":-0.284,\"X231\":1.227,\"X232\":0.322,\"X233\":0.991,\"X234\":0.503,\"X235\":-1.511,\"X236\":0.526,\"X237\":0.109,\"X238\":-0.248,\"X239\":0.914,\"X240\":-0.142,\"X241\":-0.208,\"X242\":-0.437,\"X243\":-0.428,\"X244\":-1.019,\"X245\":1.319,\"X246\":0.212,\"X247\":-0.656,\"X248\":-0.231,\"X249\":-1.089,\"X250\":1.063,\"X251\":-2.201,\"X252\":0.068,\"X253\":0.505,\"X254\":1.71,\"X255\":-0.985,\"X256\":-1.462,\"X257\":0.063,\"X258\":-0.179,\"X259\":0.747,\"X260\":-0.356,\"X261\":0.37,\"X262\":-0.027,\"X263\":0.217,\"X264\":0.355,\"X265\":-0.316,\"X266\":1.686,\"X267\":-0.37,\"X268\":-1.1,\"X269\":-0.878,\"X270\":0.361,\"X271\":0.447,\"X272\":0.9,\"X273\":0.331,\"X274\":-0.46,\"X275\":1.334,\"X276\":0.324,\"X277\":0.655,\"X278\":-1.535,\"X279\":0.179,\"X280\":2.001,\"X281\":2.003,\"X282\":-0.631,\"X283\":-0.719,\"X284\":-2.091,\"X285\":0.166,\"X286\":0.341,\"X287\":0.208,\"X288\":0.706,\"X289\":0.182,\"X290\":-0.414,\"X291\":-0.954,\"X292\":-0.362,\"X293\":4.28,\"X294\":0.158,\"X295\":-0.006,\"X296\":0.413,\"X297\":1.058,\"X298\":-0.296,\"X299\":-0.16},{\".pred_class\":\"0\",\".pred_0\":0.6422,\".pred_1\":0.3578,\"id\":181,\"X0\":0.2,\"X1\":-0.198,\"X2\":-0.884,\"X3\":-0.065,\"X4\":0.168,\"X5\":-0.908,\"X6\":-0.856,\"X7\":0.986,\"X8\":-0.563,\"X9\":2.602,\"X10\":-0.684,\"X11\":1.296,\"X12\":-0.177,\"X13\":0.473,\"X14\":0.521,\"X15\":-0.239,\"X16\":-0.525,\"X17\":-0.605,\"X18\":2.114,\"X19\":-1.064,\"X20\":-1.211,\"X21\":0.051,\"X22\":0.454,\"X23\":1.006,\"X24\":1.654,\"X25\":-1.19,\"X26\":-0.628,\"X27\":1.239,\"X28\":-0.348,\"X29\":-0.401,\"X30\":-0.566,\"X31\":0.615,\"X32\":0.17,\"X33\":1.402,\"X34\":0.076,\"X35\":-0.54,\"X36\":-1.896,\"X37\":-1.321,\"X38\":1.431,\"X39\":0.676,\"X40\":-1.385,\"X41\":0.976,\"X42\":1.286,\"X43\":-0.024,\"X44\":0.437,\"X45\":2.017,\"X46\":0.379,\"X47\":0.997,\"X48\":0.761,\"X49\":-1.315,\"X50\":1.06,\"X51\":0.775,\"X52\":-0.779,\"X53\":0.102,\"X54\":1.136,\"X55\":0.974,\"X56\":-0.357,\"X57\":1.622,\"X58\":-0.572,\"X59\":-0.839,\"X60\":-0.043,\"X61\":0.667,\"X62\":0.78,\"X63\":-0.747,\"X64\":0.726,\"X65\":1.001,\"X66\":-1.908,\"X67\":-0.388,\"X68\":1.161,\"X69\":0.236,\"X70\":0.359,\"X71\":-1.386,\"X72\":2.109,\"X73\":-0.027,\"X74\":-1.054,\"X75\":0.423,\"X76\":-0.733,\"X77\":-0.132,\"X78\":0.59,\"X79\":1.577,\"X80\":-2.12,\"X81\":0.761,\"X82\":-0.794,\"X83\":-0.162,\"X84\":-1.221,\"X85\":-0.141,\"X86\":-0.65,\"X87\":-0.385,\"X88\":-0.206,\"X89\":0.375,\"X90\":0.316,\"X91\":0.613,\"X92\":2.061,\"X93\":0.416,\"X94\":-1.059,\"X95\":-1.129,\"X96\":1.153,\"X97\":-1.923,\"X98\":-1.694,\"X99\":-1.491,\"X100\":-0.82,\"X101\":1.444,\"X102\":0.375,\"X103\":-1.374,\"X104\":-0.786,\"X105\":-0.015,\"X106\":-0.001,\"X107\":-1.405,\"X108\":0.416,\"X109\":-1.137,\"X110\":-0.859,\"X111\":-0.919,\"X112\":-0.431,\"X113\":0.579,\"X114\":-0.401,\"X115\":0.095,\"X116\":0.957,\"X117\":-0.794,\"X118\":0.479,\"X119\":-0.144,\"X120\":-0.974,\"X121\":-0.838,\"X122\":0.149,\"X123\":-1.291,\"X124\":0.308,\"X125\":0,\"X126\":0.55,\"X127\":-0.934,\"X128\":0.924,\"X129\":0.358,\"X130\":1.482,\"X131\":0.616,\"X132\":2.092,\"X133\":1.237,\"X134\":1.245,\"X135\":-0.975,\"X136\":0.951,\"X137\":1.326,\"X138\":-0.263,\"X139\":0.747,\"X140\":-1.329,\"X141\":-1.092,\"X142\":-1.009,\"X143\":2.399,\"X144\":0.093,\"X145\":-0.015,\"X146\":0.339,\"X147\":-0.19,\"X148\":-0.732,\"X149\":-0.176,\"X150\":-0.461,\"X151\":0.821,\"X152\":0.37,\"X153\":-0.818,\"X154\":-0.91,\"X155\":0.883,\"X156\":0.077,\"X157\":0.676,\"X158\":3.724,\"X159\":-1.067,\"X160\":0.426,\"X161\":-0.959,\"X162\":-0.188,\"X163\":0.707,\"X164\":-0.519,\"X165\":1.789,\"X166\":1.427,\"X167\":1.968,\"X168\":0.55,\"X169\":-1.083,\"X170\":0.121,\"X171\":-1.903,\"X172\":-0.507,\"X173\":0.527,\"X174\":-0.588,\"X175\":1.685,\"X176\":-0.141,\"X177\":0.566,\"X178\":0.711,\"X179\":0.999,\"X180\":-0.113,\"X181\":1.92,\"X182\":-0.484,\"X183\":-0.698,\"X184\":0.416,\"X185\":1.682,\"X186\":0.165,\"X187\":1.764,\"X188\":0.166,\"X189\":1.184,\"X190\":-0.471,\"X191\":-1.962,\"X192\":-2.337,\"X193\":1.835,\"X194\":-0.449,\"X195\":-0.416,\"X196\":-0.769,\"X197\":-0.687,\"X198\":-0.852,\"X199\":0.194,\"X200\":1.126,\"X201\":-0.685,\"X202\":2.113,\"X203\":-0.447,\"X204\":0.205,\"X205\":-1.133,\"X206\":0.089,\"X207\":0.1,\"X208\":0.818,\"X209\":1.458,\"X210\":-1.025,\"X211\":-0.074,\"X212\":2.166,\"X213\":-0.31,\"X214\":1.144,\"X215\":-0.172,\"X216\":-0.454,\"X217\":-0.263,\"X218\":1.02,\"X219\":-0.995,\"X220\":-1.203,\"X221\":-0.837,\"X222\":0.64,\"X223\":0.831,\"X224\":1.362,\"X225\":-0.123,\"X226\":0.172,\"X227\":-0.412,\"X228\":-0.015,\"X229\":-0.181,\"X230\":-0.298,\"X231\":0.507,\"X232\":0.852,\"X233\":0.587,\"X234\":-0.359,\"X235\":-1.842,\"X236\":-1.102,\"X237\":-0.465,\"X238\":1.009,\"X239\":0.257,\"X240\":-1.184,\"X241\":1.028,\"X242\":1.287,\"X243\":-0.298,\"X244\":0.248,\"X245\":2.495,\"X246\":0.267,\"X247\":-1.3,\"X248\":-0.347,\"X249\":-0.331,\"X250\":0.816,\"X251\":-0.006,\"X252\":0.023,\"X253\":1.05,\"X254\":-1.08,\"X255\":-0.515,\"X256\":-0.85,\"X257\":-1.525,\"X258\":0.319,\"X259\":1.14,\"X260\":-0.297,\"X261\":1.587,\"X262\":-1.149,\"X263\":-1.117,\"X264\":1.471,\"X265\":0.53,\"X266\":-0.381,\"X267\":1.138,\"X268\":1.998,\"X269\":-1.17,\"X270\":1.348,\"X271\":0.073,\"X272\":0.014,\"X273\":0.949,\"X274\":-0.469,\"X275\":-0.326,\"X276\":0.837,\"X277\":1.323,\"X278\":0.37,\"X279\":-2.357,\"X280\":2.439,\"X281\":0.48,\"X282\":-1.339,\"X283\":0.351,\"X284\":0.366,\"X285\":0.949,\"X286\":0.799,\"X287\":1.216,\"X288\":0.441,\"X289\":-1.834,\"X290\":-2.824,\"X291\":0.729,\"X292\":-0.247,\"X293\":-0.031,\"X294\":-1.241,\"X295\":1.522,\"X296\":-0.475,\"X297\":-1.053,\"X298\":-1.152,\"X299\":1.461}]",
              "role": "tool",
              "tool_call_id": "call_OhWYEFBss59jAUNjhgjoJAa7",
              "function": "run_r_code"
            }
          ],
          "tools": [
            {
              "name": "run_r_code",
              "description": "Executes R code in the current session",
              "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
                "additionalProperties": false
              }
            }
          ],
          "tool_choice": "auto",
          "config": {
            "max_tokens": 4096
          },
          "output": {
            "model": "gpt-4.1",
            "choices": [
              {
                "message": {
                  "id": "qBxllt5c2GmpLIcDFKrG9W",
                  "content": [
                    {
                      "type": "text",
                      "text": "The final regularized logistic regression model has been trained on all your training data using the best-tuned parameters (penalty = 0.0599, mixture = 0.6667) and predictions for your test set have been generated and saved as submission.csv in your current working directory.\n\nYour test set predictions include both:\n- .pred_class: the predicted class (as a factor, matching your original target levels)\n- .pred_0 and .pred_1: the class probabilities for each class\n\n[SUBMITTED]\n\nIf you need a sample of the output file, further analysis, or model interpretation (such as feature coefficients), just let me know!"
                    }
                  ],
                  "source": "generate",
                  "role": "assistant",
                  "model": "gpt-4.1"
                },
                "stop_reason": "stop"
              }
            ],
            "usage": {
              "input_tokens": 13872,
              "input_tokens_cache_write": 0,
              "input_tokens_cache_read": 0,
              "output_tokens": 135
            },
            "time": 5.6759
          },
          "call": {
            "request": {
              "messages": [
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "I have a dataset called `train` that I need help modeling. It's a binary classification problem where I need to predict a target variable based on 300 continuous features. The challenge is that I only have 250 training samples but need to avoid overfitting. I want to evaluate performance using AUC ROC - can you help me approach this modeling problem?"
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Absolutely! With 250 samples and 300 features, your dataset is high dimensional relative to its size, which increases the risk of overfitting. Here are some best-practice approaches to tackle this scenario:\n\n## 1. Preprocessing\n\n- **Standardization:** Scale your features (e.g., with z-scores). This helps most models, especially regularized ones, perform better.\n- **Remove near-zero variance features:** Variables that do not vary much across samples likely contribute little and may add noise.\n\n## 2. Feature Selection/Dimensionality Reduction\n\n- **Univariate selection:** Use correlation with the target or univariate statistical tests to filter out irrelevant features.\n- **Regularization:** Prefer models with built-in feature selection, like Lasso logistic regression (L1 penalty) or Elastic Net.\n- **PCA:** Use Principal Component Analysis to reduce dimensions, if interpretability is less important.\n- **Recursive Feature Elimination:** Optionally, use wrappers like RFE, but these can be unstable in high dimensions and small n.\n\n## 3. Model Selection\n\nGiven the high risk of overfitting:\n- **Regularized logistic regression** (e.g., glmnet)\n- **Linear SVM with regularization**\n- **Tree-based methods only with aggressive pruning and regularization** (random forests and XGBoost can overfit unless tuned carefully)\n\n## 4. Cross-Validation Strategy\n\n- Use stratified **v-fold cross-validation** (e.g., 5- or 10-fold) to estimate AUC reliably. Since your sample is small, avoid validation sets that are too small, e.g., consider using leave-one-out (LOO-CV) or repeated CV.\n\n## 5. Avoiding Overfitting\n\n- Prefer simpler models. Avoid deep learning approaches, which are too complex for this data size.\n- Tune regularization parameters extensively via CV.\n- Use pipelines to avoid leakage: always perform feature selection/scaling within CV folds.\n\n## 6. ROC AUC Evaluation\n\n- Use the average AUC from your v-fold cross-validation to assess performance.\n\n---\n\n**Shall I show you how to set up such a pipeline in R with tidymodels and run cross-validated AUC for a regularized logistic regression using glmnet? Or do you want to see feature selection (e.g., univariate filter or PCA) as well?**"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "That approach makes sense. I'd like to see the pipeline in R with tidymodels. Let's try regularized logistic regression with glmnet first since it has built-in feature selection."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "name": "run_r_code",
                      "input": {
                        "code": "library(tidymodels)\n\n# Read data (assuming train.csv exists with proper formatting)\ndf <- read.csv('train.csv')\n\n# Identify outcome and predictors\noutcome <- 'target'  # change if target variable uses a different name\npredictors <- setdiff(names(df), outcome)\n\n# Setup stratified v-fold CV (let's use 10-fold for small n)\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = outcome)\n\n# Create a recipe: center/scale, remove NZV\ntune_rec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nspec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(tune_rec) %>%\n  add_model(spec)\n\n# Grid for penalty - explore log scale\npenalty_grid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tuning\nset.seed(2024)\ntune_res <- tune_grid(\n  wf, \n  resamples = folds, \n  grid = penalty_grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_Vc1jCSmXrX5rLgK96BWSCueY",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(outcome)\n\n  # Now:\n  data %>% select(all_of(outcome))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\r         \r\n\n→ A | error:   For a classification model, the outcome should be a <factor>, not a double\n                                                                                                vector.\n\n\n\u001b[36mℹ\u001b[39m Solving\r\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n\nError: All models failed. Run `show_notes(.Last.tune.result)` for more information.\n"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "It looks like the modeling step failed because the target variable is not a factor—glmnet in tidymodels expects a factor outcome for classification. This is a common issue when reading in CSV data with binary targets encoded as 0/1 or similar.\n\nHere’s how to modify the pipeline:\n\n- Ensure the outcome variable is a factor with two levels before running the workflow.\n\nWould you like me to re-run the pipeline, explicitly converting the target variable to a factor (e.g., with as.factor()), or do you use a different target column name?"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Yes, please re-run the pipeline and convert the target variable to a factor. The target column is simply called \"target\" in my dataset."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                      "name": "run_r_code",
                      "input": {
                        "code": "library(tidymodels)\n\n# Read data\ndf <- read.csv('train.csv')\ndf$target <- as.factor(df$target)\n\n# Setup stratified 10-fold CV\nset.seed(2024)\nfolds <- vfold_cv(df, v = 10, strata = target)\n\n# Recipe: remove NZV, normalize\nrec <- recipe(target ~ ., data = df) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_predictors())\n\n# Model spec: Regularized logistic regression\nglmnet_spec <- logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine('glmnet')\n\n# Workflow\nwf <- workflow() %>%\n  add_recipe(rec) %>%\n  add_model(glmnet_spec)\n\n# Penalty grid (log scale)\ngrid <- grid_regular(penalty(range = c(-4, 1)), mixture(), levels = 10)\n\n# Tune\ntune_res <- tune_grid(\n  wf,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc)\n)\n\n# Show best results\nshow_best(tune_res, metric = 'roc_auc')"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_QQIBObfU48S1e8BoXwQdM4jG",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "[{\"penalty\":0.0599,\"mixture\":0.6667,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7906,\"n\":10,\"std_err\":0.0213,\".config\":\"pre0_mod057_post0\"},{\"penalty\":0.0599,\"mixture\":0.5556,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7828,\"n\":10,\"std_err\":0.0267,\".config\":\"pre0_mod056_post0\"},{\"penalty\":0.2154,\"mixture\":0.1111,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7763,\"n\":10,\"std_err\":0.0321,\".config\":\"pre0_mod062_post0\"},{\"penalty\":0.2154,\"mixture\":0.2222,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.774,\"n\":10,\"std_err\":0.0222,\".config\":\"pre0_mod063_post0\"},{\"penalty\":0.0599,\"mixture\":0.7778,\".metric\":\"roc_auc\",\".estimator\":\"binary\",\"mean\":0.7684,\"n\":10,\"std_err\":0.0233,\".config\":\"pre0_mod058_post0\"}]"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": "Here are the top results from cross-validated hyperparameter tuning of regularized logistic regression (glmnet) with your 10-fold stratified CV:\n\n| Penalty  | Mixture | Mean ROC AUC | Std Err |\n|----------|---------|--------------|---------|\n| 0.0599   | 0.6667  | 0.791        | 0.021   |\n| 0.0599   | 0.5556  | 0.783        | 0.027   |\n| 0.2154   | 0.1111  | 0.776        | 0.032   |\n| 0.2154   | 0.2222  | 0.774        | 0.022   |\n| 0.0599   | 0.7778  | 0.768        | 0.023   |\n\n- The best ROC AUC (≈ 0.791) is achieved with a penalty of 0.0599 and mixture of 0.6667 (mixture controls balance between L1 and L2 regularization—here it's favoring more L1, i.e., feature selection).\n- The standard errors indicate stability across folds.\n\nWould you like to:\n1. Use these best-tuned parameters to fit the final model and generate test set predictions for submission?\n2. Explore the selected features or coefficients?\n3. Try an alternate approach (e.g., with feature filtering or PCA)?\n\nLet me know your next step!"
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "Let's go with option 1 - use the best-tuned parameters to fit the final model and generate test set predictions for submission."
                    }
                  ]
                },
                {
                  "role": "assistant",
                  "content": [
                    {
                      "type": "text",
                      "text": ""
                    },
                    {
                      "type": "tool_use",
                      "id": "call_OhWYEFBss59jAUNjhgjoJAa7",
                      "name": "run_r_code",
                      "input": {
                        "code": "# Read test set\ntest <- read.csv('test.csv')\n\n# Re-train with best parameters on all training data\nfinal_wf <- wf %>%\n  finalize_workflow(list(\n    penalty = 0.0599,\n    mixture = 0.6667\n  ))\nfinal_fit <- fit(final_wf, data = df)\n\n# Predict on test set\npreds <- augment(final_fit, new_data = test)\n\n# Write as submission.csv\nwrite.csv(preds, file = 'submission.csv', row.names = FALSE)\n\n# Show the first few rows of the predictions\ndim(preds)\nhead(preds)"
                      }
                    }
                  ]
                },
                {
                  "role": "user",
                  "content": [
                    {
                      "tool_use_id": "call_OhWYEFBss59jAUNjhgjoJAa7",
                      "type": "tool_result",
                      "content": [
                        {
                          "type": "text",
                          "text": "[1]  50 304\n\n[{\".pred_class\":\"0\",\".pred_0\":0.6901,\".pred_1\":0.3099,\"id\":142,\"X0\":0.081,\"X1\":0.312,\"X2\":0.015,\"X3\":0.688,\"X4\":-0.145,\"X5\":0.894,\"X6\":-0.153,\"X7\":-1.351,\"X8\":-0.183,\"X9\":0.627,\"X10\":0.262,\"X11\":0.496,\"X12\":0.018,\"X13\":-1.367,\"X14\":0.65,\"X15\":-0.943,\"X16\":0.312,\"X17\":0.406,\"X18\":3.117,\"X19\":0.659,\"X20\":0.788,\"X21\":0.355,\"X22\":-0.735,\"X23\":-0.037,\"X24\":2.214,\"X25\":1.731,\"X26\":-1.157,\"X27\":0.398,\"X28\":1.195,\"X29\":1.141,\"X30\":-1.266,\"X31\":0.349,\"X32\":0.93,\"X33\":1.282,\"X34\":-0.271,\"X35\":-0.947,\"X36\":0.259,\"X37\":2.251,\"X38\":0.242,\"X39\":0.28,\"X40\":-0.011,\"X41\":1.209,\"X42\":0.278,\"X43\":1.048,\"X44\":0.079,\"X45\":-1.25,\"X46\":0.113,\"X47\":0.984,\"X48\":0.51,\"X49\":0.526,\"X50\":-0.956,\"X51\":0.04,\"X52\":-0.632,\"X53\":0.421,\"X54\":0.612,\"X55\":-1.21,\"X56\":1.458,\"X57\":-0.483,\"X58\":1.008,\"X59\":0.986,\"X60\":-0.998,\"X61\":0.664,\"X62\":-0.764,\"X63\":-1.109,\"X64\":0.715,\"X65\":0.161,\"X66\":0.077,\"X67\":-0.869,\"X68\":0.807,\"X69\":1.097,\"X70\":0.996,\"X71\":-0.673,\"X72\":-1.651,\"X73\":-0.33,\"X74\":-0.088,\"X75\":0.397,\"X76\":-0.643,\"X77\":-0.462,\"X78\":0.866,\"X79\":-1.627,\"X80\":0.795,\"X81\":-0.706,\"X82\":0.158,\"X83\":-1.204,\"X84\":0.908,\"X85\":0.255,\"X86\":-0.031,\"X87\":-0.137,\"X88\":0.197,\"X89\":-0.125,\"X90\":-0.969,\"X91\":-0.133,\"X92\":-0.333,\"X93\":-1,\"X94\":0.391,\"X95\":-0.957,\"X96\":0.13,\"X97\":0.164,\"X98\":-0.403,\"X99\":1.591,\"X100\":0.323,\"X101\":-0.845,\"X102\":1.018,\"X103\":2.033,\"X104\":1.161,\"X105\":2.87,\"X106\":-0.426,\"X107\":1.361,\"X108\":-0.204,\"X109\":0.524,\"X110\":0.736,\"X111\":0.274,\"X112\":-0.76,\"X113\":-1.589,\"X114\":-0.053,\"X115\":-0.992,\"X116\":-0.431,\"X117\":-0.413,\"X118\":0.788,\"X119\":-0.017,\"X120\":0.068,\"X121\":-0.074,\"X122\":0.95,\"X123\":-0.788,\"X124\":-0.567,\"X125\":-1.259,\"X126\":0.914,\"X127\":-1.595,\"X128\":0.202,\"X129\":0.128,\"X130\":0.883,\"X131\":-0.147,\"X132\":-1.395,\"X133\":-0.831,\"X134\":-2.199,\"X135\":-0.967,\"X136\":0.592,\"X137\":0.363,\"X138\":0.941,\"X139\":0.548,\"X140\":-0.109,\"X141\":0.641,\"X142\":0.687,\"X143\":0.271,\"X144\":-1.243,\"X145\":1.207,\"X146\":-0.122,\"X147\":-2.6,\"X148\":0.234,\"X149\":0.218,\"X150\":0.739,\"X151\":-0.118,\"X152\":-0.69,\"X153\":-0.476,\"X154\":-0.311,\"X155\":0.593,\"X156\":-0.789,\"X157\":-0.333,\"X158\":0.757,\"X159\":-0.024,\"X160\":-0.047,\"X161\":-1.422,\"X162\":1.395,\"X163\":2.555,\"X164\":1.277,\"X165\":0.499,\"X166\":-0.457,\"X167\":0.076,\"X168\":-0.459,\"X169\":1.094,\"X170\":-1.078,\"X171\":0.435,\"X172\":-0.664,\"X173\":-0.829,\"X174\":-0.509,\"X175\":-0.106,\"X176\":-0.803,\"X177\":-0.495,\"X178\":-0.545,\"X179\":0.293,\"X180\":-0.492,\"X181\":0.404,\"X182\":1.435,\"X183\":-1.011,\"X184\":1.116,\"X185\":0.406,\"X186\":0.164,\"X187\":0.18,\"X188\":-0.286,\"X189\":0.437,\"X190\":0.282,\"X191\":-0.353,\"X192\":0.578,\"X193\":-1.131,\"X194\":0.499,\"X195\":-1.052,\"X196\":0.487,\"X197\":0.114,\"X198\":-0.304,\"X199\":-1.416,\"X200\":0.362,\"X201\":-0.624,\"X202\":-0.066,\"X203\":1.391,\"X204\":-0.896,\"X205\":-0.401,\"X206\":0.633,\"X207\":1.132,\"X208\":-1.979,\"X209\":0.208,\"X210\":-1.051,\"X211\":1.235,\"X212\":-0.821,\"X213\":-0.78,\"X214\":0.142,\"X215\":2.014,\"X216\":-1.135,\"X217\":-0.261,\"X218\":-0.323,\"X219\":-0.965,\"X220\":1.067,\"X221\":-0.371,\"X222\":0.496,\"X223\":1.009,\"X224\":1.725,\"X225\":0.719,\"X226\":0.106,\"X227\":-0.01,\"X228\":0.501,\"X229\":-0.263,\"X230\":-0.605,\"X231\":-0.94,\"X232\":-1.897,\"X233\":1.156,\"X234\":-0.299,\"X235\":-0.655,\"X236\":0.429,\"X237\":-0.557,\"X238\":0.766,\"X239\":0.008,\"X240\":-0.925,\"X241\":-2.201,\"X242\":-0.206,\"X243\":-0.224,\"X244\":1.103,\"X245\":1.027,\"X246\":1.689,\"X247\":-0.66,\"X248\":-0.019,\"X249\":1.429,\"X250\":-1.661,\"X251\":-0.328,\"X252\":-0.29,\"X253\":1.005,\"X254\":-1.328,\"X255\":0.29,\"X256\":1.387,\"X257\":-1.331,\"X258\":0.615,\"X259\":-0.987,\"X260\":0.869,\"X261\":-0.84,\"X262\":0.405,\"X263\":0.625,\"X264\":-1.491,\"X265\":-0.896,\"X266\":-0.056,\"X267\":0.526,\"X268\":-1.403,\"X269\":0.281,\"X270\":-0.886,\"X271\":2.277,\"X272\":-0.882,\"X273\":-0.155,\"X274\":0.005,\"X275\":-0.203,\"X276\":-1.682,\"X277\":0.182,\"X278\":-0.932,\"X279\":0.916,\"X280\":0.11,\"X281\":-0.224,\"X282\":0.632,\"X283\":0.617,\"X284\":0.098,\"X285\":-1.351,\"X286\":0.922,\"X287\":1.724,\"X288\":0.507,\"X289\":0.182,\"X290\":0.307,\"X291\":-0.62,\"X292\":1.04,\"X293\":0.224,\"X294\":-0.678,\"X295\":-0.596,\"X296\":0.203,\"X297\":1.138,\"X298\":1.896,\"X299\":0.952},{\".pred_class\":\"1\",\".pred_0\":0.4394,\".pred_1\":0.5606,\"id\":6,\"X0\":1.257,\"X1\":-1.562,\"X2\":0.832,\"X3\":0.432,\"X4\":1.667,\"X5\":0.843,\"X6\":-0.037,\"X7\":-0.96,\"X8\":-0.311,\"X9\":0.173,\"X10\":-0.059,\"X11\":1.744,\"X12\":-0.92,\"X13\":-0.077,\"X14\":-0.818,\"X15\":-2.053,\"X16\":-2.042,\"X17\":0.219,\"X18\":0.508,\"X19\":-0.659,\"X20\":0.063,\"X21\":-0.443,\"X22\":1.086,\"X23\":-2.087,\"X24\":0.227,\"X25\":1.854,\"X26\":0.211,\"X27\":0.172,\"X28\":0.815,\"X29\":-0.013,\"X30\":-0.408,\"X31\":-1.385,\"X32\":1.2,\"X33\":-0.02,\"X34\":0.554,\"X35\":-0.076,\"X36\":0.452,\"X37\":-0.147,\"X38\":-0.61,\"X39\":-0.907,\"X40\":0.283,\"X41\":0.518,\"X42\":-3.191,\"X43\":-0.975,\"X44\":-1.255,\"X45\":-0.663,\"X46\":2.072,\"X47\":0.087,\"X48\":0.418,\"X49\":-0.067,\"X50\":-0.028,\"X51\":0.266,\"X52\":-0.56,\"X53\":0.389,\"X54\":0.777,\"X55\":-0.858,\"X56\":1.434,\"X57\":1.907,\"X58\":-2.262,\"X59\":-0.727,\"X60\":0.709,\"X61\":0.575,\"X62\":-0.504,\"X63\":-0.113,\"X64\":0.643,\"X65\":-0.22,\"X66\":-0.828,\"X67\":0.402,\"X68\":0.806,\"X69\":0.154,\"X70\":0.052,\"X71\":-0.5,\"X72\":0.832,\"X73\":-0.488,\"X74\":-0.215,\"X75\":-0.88,\"X76\":0.373,\"X77\":0.766,\"X78\":0.518,\"X79\":0.508,\"X80\":-0.609,\"X81\":-2.461,\"X82\":0.302,\"X83\":-0.252,\"X84\":1.067,\"X85\":1.455,\"X86\":0.424,\"X87\":0.959,\"X88\":-0.472,\"X89\":-1.019,\"X90\":-1.926,\"X91\":-0.351,\"X92\":-0.755,\"X93\":-1.043,\"X94\":-0.676,\"X95\":-0.688,\"X96\":1.477,\"X97\":-1.266,\"X98\":0.062,\"X99\":0.967,\"X100\":-1.343,\"X101\":1.48,\"X102\":-0.591,\"X103\":0.449,\"X104\":1.438,\"X105\":0.822,\"X106\":-0.792,\"X107\":-0.007,\"X108\":-0.409,\"X109\":-0.945,\"X110\":-1.055,\"X111\":-0.867,\"X112\":-0.559,\"X113\":-0.279,\"X114\":-0.866,\"X115\":-0.741,\"X116\":0.051,\"X117\":0.921,\"X118\":-1.204,\"X119\":0.62,\"X120\":-1.104,\"X121\":-0.565,\"X122\":1.373,\"X123\":0.581,\"X124\":-0.666,\"X125\":1.256,\"X126\":0.923,\"X127\":2.11,\"X128\":-0.56,\"X129\":-1.786,\"X130\":-1.588,\"X131\":1.139,\"X132\":-0.287,\"X133\":-1.48,\"X134\":2.327,\"X135\":-0.73,\"X136\":1.293,\"X137\":0.639,\"X138\":-1.781,\"X139\":0.249,\"X140\":-0.57,\"X141\":1.124,\"X142\":-0.442,\"X143\":-0.806,\"X144\":-0.5,\"X145\":0.664,\"X146\":-0.425,\"X147\":-0.007,\"X148\":-0.775,\"X149\":0.51,\"X150\":0.408,\"X151\":1.717,\"X152\":0.095,\"X153\":0.2,\"X154\":0.714,\"X155\":-1.086,\"X156\":-0.322,\"X157\":0.58,\"X158\":0.745,\"X159\":-1.32,\"X160\":0.587,\"X161\":-1.219,\"X162\":0.843,\"X163\":1.561,\"X164\":-0.092,\"X165\":-0.815,\"X166\":0.699,\"X167\":-1.377,\"X168\":1.503,\"X169\":0.496,\"X170\":1.064,\"X171\":-1.636,\"X172\":0.508,\"X173\":-0.002,\"X174\":-0.503,\"X175\":-0.703,\"X176\":-1.114,\"X177\":0.991,\"X178\":-0.664,\"X179\":-0.927,\"X180\":-0.715,\"X181\":0.594,\"X182\":1.306,\"X183\":0.162,\"X184\":0.095,\"X185\":0.833,\"X186\":-0.708,\"X187\":1.401,\"X188\":-1.953,\"X189\":-1.42,\"X190\":-0.638,\"X191\":-0.163,\"X192\":-1.514,\"X193\":1.096,\"X194\":-0.172,\"X195\":0.357,\"X196\":-1.562,\"X197\":0.154,\"X198\":0.726,\"X199\":-0.054,\"X200\":0.938,\"X201\":-0.326,\"X202\":-0.314,\"X203\":-0.234,\"X204\":-0.781,\"X205\":-0.914,\"X206\":-2.33,\"X207\":1.627,\"X208\":-2.215,\"X209\":-0.724,\"X210\":-1.439,\"X211\":-0.68,\"X212\":0.747,\"X213\":-0.852,\"X214\":-0.187,\"X215\":0.34,\"X216\":-0.417,\"X217\":0.846,\"X218\":-0.065,\"X219\":-1.632,\"X220\":-0.16,\"X221\":0.577,\"X222\":-0.832,\"X223\":-0.773,\"X224\":1.068,\"X225\":-0.442,\"X226\":-0.362,\"X227\":-1.214,\"X228\":0.852,\"X229\":0.239,\"X230\":-0.101,\"X231\":0.131,\"X232\":0.78,\"X233\":1.369,\"X234\":-2.154,\"X235\":-0.668,\"X236\":1.497,\"X237\":-0.209,\"X238\":-1.053,\"X239\":0.336,\"X240\":0.862,\"X241\":-1.926,\"X242\":-1.013,\"X243\":0.508,\"X244\":-0.862,\"X245\":0.749,\"X246\":0.539,\"X247\":-0.303,\"X248\":-1.239,\"X249\":-1.961,\"X250\":-0.612,\"X251\":0.687,\"X252\":-0.49,\"X253\":-0.648,\"X254\":-0.537,\"X255\":0.074,\"X256\":0.101,\"X257\":0.158,\"X258\":0.594,\"X259\":0.72,\"X260\":-0.604,\"X261\":0.566,\"X262\":1.239,\"X263\":0.27,\"X264\":0.588,\"X265\":-0.207,\"X266\":-1.395,\"X267\":-1.579,\"X268\":-0.141,\"X269\":-0.236,\"X270\":-0.538,\"X271\":0.061,\"X272\":0.642,\"X273\":0.118,\"X274\":-0.15,\"X275\":-1.399,\"X276\":-0.423,\"X277\":0.553,\"X278\":-0.857,\"X279\":0.393,\"X280\":0.958,\"X281\":-0.124,\"X282\":1.379,\"X283\":-0.748,\"X284\":0.218,\"X285\":-1.407,\"X286\":-0.739,\"X287\":0.59,\"X288\":0.044,\"X289\":-0.268,\"X290\":0.024,\"X291\":0.391,\"X292\":0.547,\"X293\":-0.416,\"X294\":-0.333,\"X295\":0.857,\"X296\":0.422,\"X297\":-0.833,\"X298\":-1.055,\"X299\":0.439},{\".pred_class\":\"0\",\".pred_0\":0.8511,\".pred_1\":0.1489,\"id\":97,\"X0\":0.66,\"X1\":0.126,\"X2\":-1.829,\"X3\":-0.73,\"X4\":-0.683,\"X5\":-0.943,\"X6\":0.892,\"X7\":0.121,\"X8\":-1.003,\"X9\":0.398,\"X10\":1.396,\"X11\":1.681,\"X12\":0.089,\"X13\":0.008,\"X14\":0.479,\"X15\":-0.354,\"X16\":0.253,\"X17\":-0.918,\"X18\":-0.948,\"X19\":2.087,\"X20\":0.003,\"X21\":-0.526,\"X22\":0.961,\"X23\":1.105,\"X24\":-2.217,\"X25\":-0.206,\"X26\":-0.709,\"X27\":-1.434,\"X28\":-0.181,\"X29\":-1.473,\"X30\":0.725,\"X31\":0.658,\"X32\":0.137,\"X33\":0.68,\"X34\":-0.512,\"X35\":1.037,\"X36\":-0.293,\"X37\":0.445,\"X38\":0.271,\"X39\":2.01,\"X40\":-0.07,\"X41\":0.754,\"X42\":-0.961,\"X43\":0.341,\"X44\":0.965,\"X45\":0.328,\"X46\":0.167,\"X47\":-0.319,\"X48\":-0.456,\"X49\":1.108,\"X50\":0.535,\"X51\":-1.703,\"X52\":0.505,\"X53\":0.072,\"X54\":1.713,\"X55\":0.562,\"X56\":-0.745,\"X57\":0.055,\"X58\":-0.82,\"X59\":0.937,\"X60\":0.426,\"X61\":0.441,\"X62\":-0.919,\"X63\":0.676,\"X64\":0.184,\"X65\":0.591,\"X66\":-0.006,\"X67\":1.754,\"X68\":-0.337,\"X69\":-0.198,\"X70\":-0.796,\"X71\":-1.298,\"X72\":-0.737,\"X73\":1.285,\"X74\":-1.637,\"X75\":-0.529,\"X76\":0.377,\"X77\":0.212,\"X78\":-0.727,\"X79\":-0.905,\"X80\":-0.701,\"X81\":-0.312,\"X82\":-0.463,\"X83\":-1.797,\"X84\":0.664,\"X85\":-0.171,\"X86\":0.559,\"X87\":0.636,\"X88\":-0.209,\"X89\":0.064,\"X90\":0.144,\"X91\":-0.818,\"X92\":0.7,\"X93\":-0.364,\"X94\":-0.333,\"X95\":-0.191,\"X96\":-1.579,\"X97\":2.081,\"X98\":-1.575,\"X99\":-0.685,\"X100\":-0.385,\"X101\":0.716,\"X102\":-0.767,\"X103\":-0.756,\"X104\":-2.331,\"X105\":-0.586,\"X106\":0.764,\"X107\":-0.622,\"X108\":0.651,\"X109\":0.071,\"X110\":-0.091,\"X111\":-0.129,\"X112\":-1.848,\"X113\":-0.354,\"X114\":0.428,\"X115\":0.928,\"X116\":0.557,\"X117\":-0.181,\"X118\":-0.258,\"X119\":-0.872,\"X120\":-0.864,\"X121\":-0.257,\"X122\":0.795,\"X123\":0.882,\"X124\":-0.519,\"X125\":-0.673,\"X126\":2.144,\"X127\":-0.595,\"X128\":0.624,\"X129\":1.164,\"X130\":0.136,\"X131\":0.52,\"X132\":0.423,\"X133\":0.457,\"X134\":-0.374,\"X135\":-1.384,\"X136\":2.57,\"X137\":0.223,\"X138\":0.217,\"X139\":0.364,\"X140\":-0.132,\"X141\":-1.192,\"X142\":0.471,\"X143\":-0.172,\"X144\":-1.586,\"X145\":-0.091,\"X146\":0.58,\"X147\":-0.43,\"X148\":0.073,\"X149\":-0.315,\"X150\":-0.122,\"X151\":-0.988,\"X152\":-2.52,\"X153\":-0.831,\"X154\":1.146,\"X155\":-0.883,\"X156\":0.723,\"X157\":0.452,\"X158\":-0.237,\"X159\":-0.789,\"X160\":0.422,\"X161\":-1.003,\"X162\":-1.352,\"X163\":1.384,\"X164\":-1.245,\"X165\":-1.602,\"X166\":1.604,\"X167\":-0.947,\"X168\":-0.573,\"X169\":0.903,\"X170\":-0.446,\"X171\":-1.406,\"X172\":-0.847,\"X173\":0.586,\"X174\":-0.16,\"X175\":-2.209,\"X176\":0.685,\"X177\":-0.956,\"X178\":1.433,\"X179\":-1.486,\"X180\":-0.167,\"X181\":0.788,\"X182\":-0.92,\"X183\":-1.12,\"X184\":-0.957,\"X185\":1.929,\"X186\":-1.875,\"X187\":1.304,\"X188\":0.113,\"X189\":1.808,\"X190\":0.525,\"X191\":1.976,\"X192\":0.233,\"X193\":-0.247,\"X194\":0.525,\"X195\":0.253,\"X196\":-0.546,\"X197\":-2.025,\"X198\":-0.252,\"X199\":0.894,\"X200\":-1.113,\"X201\":-0.049,\"X202\":1.231,\"X203\":1.905,\"X204\":-1.276,\"X205\":0.054,\"X206\":-0.264,\"X207\":-0.173,\"X208\":0.239,\"X209\":-1.871,\"X210\":0.605,\"X211\":0.095,\"X212\":0.483,\"X213\":1.36,\"X214\":0.074,\"X215\":0.534,\"X216\":-1.597,\"X217\":-0.818,\"X218\":-0.413,\"X219\":0.21,\"X220\":-0.009,\"X221\":-1.321,\"X222\":1.381,\"X223\":-0.73,\"X224\":-0.465,\"X225\":1.11,\"X226\":0.656,\"X227\":-0.399,\"X228\":-1.875,\"X229\":-0.567,\"X230\":0.552,\"X231\":-0.007,\"X232\":1.404,\"X233\":-0.936,\"X234\":-1.252,\"X235\":0.122,\"X236\":1.556,\"X237\":1.066,\"X238\":0.018,\"X239\":0.56,\"X240\":0.54,\"X241\":-1.882,\"X242\":1.559,\"X243\":1.769,\"X244\":-1.09,\"X245\":0.528,\"X246\":-0.695,\"X247\":1.028,\"X248\":0.413,\"X249\":-1.269,\"X250\":-0.887,\"X251\":0.516,\"X252\":-0.938,\"X253\":-0.33,\"X254\":0.9,\"X255\":-0.383,\"X256\":-1.226,\"X257\":0.247,\"X258\":0.399,\"X259\":-0.931,\"X260\":-0.873,\"X261\":-1.91,\"X262\":0.211,\"X263\":-0.413,\"X264\":0.116,\"X265\":-2.179,\"X266\":-0.908,\"X267\":1.206,\"X268\":0.374,\"X269\":-1.129,\"X270\":-0.577,\"X271\":0.48,\"X272\":-0.584,\"X273\":0.195,\"X274\":0.526,\"X275\":-0.163,\"X276\":-0.77,\"X277\":-0.239,\"X278\":-0.966,\"X279\":1.684,\"X280\":-1.159,\"X281\":-1.329,\"X282\":-0.724,\"X283\":0.221,\"X284\":-1.771,\"X285\":0.25,\"X286\":-1.131,\"X287\":0.609,\"X288\":-1.171,\"X289\":-0.191,\"X290\":1.675,\"X291\":-0.182,\"X292\":0.143,\"X293\":0.302,\"X294\":-1.068,\"X295\":0.445,\"X296\":-0.223,\"X297\":1.092,\"X298\":0.816,\"X299\":0.794},{\".pred_class\":\"0\",\".pred_0\":0.7855,\".pred_1\":0.2145,\"id\":60,\"X0\":-0.071,\"X1\":2.146,\"X2\":-0.512,\"X3\":1.304,\"X4\":-1.778,\"X5\":1.146,\"X6\":0.335,\"X7\":0.13,\"X8\":0.386,\"X9\":1.755,\"X10\":-1.175,\"X11\":0.851,\"X12\":0.31,\"X13\":0.969,\"X14\":-0.229,\"X15\":0.234,\"X16\":-0.049,\"X17\":0.286,\"X18\":0.204,\"X19\":-0.434,\"X20\":-0.726,\"X21\":-1.615,\"X22\":0.804,\"X23\":0.181,\"X24\":1.754,\"X25\":-0.669,\"X26\":-0.564,\"X27\":1.306,\"X28\":-0.11,\"X29\":0.954,\"X30\":-1.445,\"X31\":-1.2,\"X32\":0.553,\"X33\":2.311,\"X34\":-0.637,\"X35\":0.314,\"X36\":-0.289,\"X37\":0.029,\"X38\":0.128,\"X39\":-0.183,\"X40\":-1.007,\"X41\":1.086,\"X42\":-0.1,\"X43\":-1.01,\"X44\":0.306,\"X45\":-2.516,\"X46\":1.955,\"X47\":-0.879,\"X48\":1.855,\"X49\":0.111,\"X50\":-0.721,\"X51\":0.026,\"X52\":0.366,\"X53\":0.463,\"X54\":1.041,\"X55\":-1.566,\"X56\":0.172,\"X57\":-0.219,\"X58\":-1.044,\"X59\":0.475,\"X60\":0.281,\"X61\":-0.105,\"X62\":-0.397,\"X63\":0.155,\"X64\":2.039,\"X65\":-0.86,\"X66\":-0.129,\"X67\":-0.4,\"X68\":-0.076,\"X69\":0.698,\"X70\":0.971,\"X71\":-0.61,\"X72\":-1.25,\"X73\":-0.711,\"X74\":-0.96,\"X75\":0.295,\"X76\":0.097,\"X77\":-0.198,\"X78\":0.275,\"X79\":-0.379,\"X80\":-1.792,\"X81\":-0.38,\"X82\":-0.521,\"X83\":-0.41,\"X84\":-1.741,\"X85\":-0.202,\"X86\":0.538,\"X87\":-1.158,\"X88\":0.714,\"X89\":1.3,\"X90\":0.905,\"X91\":-1.355,\"X92\":-1.109,\"X93\":-0,\"X94\":-1.058,\"X95\":0.603,\"X96\":0.404,\"X97\":0.307,\"X98\":-0.113,\"X99\":0.446,\"X100\":-1.063,\"X101\":0.132,\"X102\":1.218,\"X103\":-1.725,\"X104\":0.358,\"X105\":-0.004,\"X106\":-0.556,\"X107\":-0.77,\"X108\":2.218,\"X109\":-0.513,\"X110\":0.808,\"X111\":-0.594,\"X112\":0.812,\"X113\":-1.549,\"X114\":0.898,\"X115\":0.437,\"X116\":-0.686,\"X117\":0.703,\"X118\":0.07,\"X119\":-0.489,\"X120\":-1.282,\"X121\":-0.653,\"X122\":-0.475,\"X123\":0.428,\"X124\":-0.579,\"X125\":0.547,\"X126\":0.361,\"X127\":-2.15,\"X128\":0.145,\"X129\":-0.7,\"X130\":-0.777,\"X131\":0.313,\"X132\":-0.418,\"X133\":0.387,\"X134\":0.797,\"X135\":1.424,\"X136\":-2.685,\"X137\":-0.982,\"X138\":-0.313,\"X139\":0.635,\"X140\":-0.318,\"X141\":-0.388,\"X142\":0.809,\"X143\":1.091,\"X144\":-0.885,\"X145\":0.149,\"X146\":-0.512,\"X147\":-1.851,\"X148\":0.236,\"X149\":-1.27,\"X150\":-0.141,\"X151\":-1.346,\"X152\":0.313,\"X153\":0.685,\"X154\":-0.042,\"X155\":-0.484,\"X156\":-1.437,\"X157\":-0.778,\"X158\":0.25,\"X159\":-0.738,\"X160\":0.937,\"X161\":-1.204,\"X162\":-0.567,\"X163\":0.583,\"X164\":0.174,\"X165\":-1.473,\"X166\":0.7,\"X167\":0.05,\"X168\":0.992,\"X169\":1.782,\"X170\":-0.59,\"X171\":0.429,\"X172\":0.843,\"X173\":1.086,\"X174\":0.298,\"X175\":-0.161,\"X176\":-0.526,\"X177\":-0.447,\"X178\":-1.108,\"X179\":0.149,\"X180\":0.655,\"X181\":-0.943,\"X182\":1.006,\"X183\":1.047,\"X184\":-1.297,\"X185\":0.186,\"X186\":0.187,\"X187\":-1.078,\"X188\":-0.307,\"X189\":-1.081,\"X190\":-1.492,\"X191\":-0.232,\"X192\":-1.353,\"X193\":-1.49,\"X194\":-0.081,\"X195\":-0.158,\"X196\":0.727,\"X197\":-1.28,\"X198\":0.622,\"X199\":1.45,\"X200\":0.041,\"X201\":2.502,\"X202\":-0.37,\"X203\":1.711,\"X204\":-0.713,\"X205\":1.455,\"X206\":0.545,\"X207\":-0.15,\"X208\":0.047,\"X209\":0.503,\"X210\":-0.438,\"X211\":-0.44,\"X212\":-0.209,\"X213\":1.295,\"X214\":-0.375,\"X215\":0.399,\"X216\":0.889,\"X217\":-0.034,\"X218\":0.682,\"X219\":1.688,\"X220\":0.441,\"X221\":-1.276,\"X222\":1.687,\"X223\":0.027,\"X224\":-0.89,\"X225\":-0.178,\"X226\":0.602,\"X227\":1.752,\"X228\":0.08,\"X229\":0.056,\"X230\":-1.158,\"X231\":-0.211,\"X232\":0.084,\"X233\":-1.743,\"X234\":0.623,\"X235\":0.834,\"X236\":0.59,\"X237\":0.9,\"X238\":0.932,\"X239\":-0.543,\"X240\":0.163,\"X241\":0.079,\"X242\":0.352,\"X243\":-0.731,\"X244\":0.293,\"X245\":0.862,\"X246\":1.428,\"X247\":-0.478,\"X248\":1.253,\"X249\":0.414,\"X250\":0.816,\"X251\":0.125,\"X252\":1.251,\"X253\":1.374,\"X254\":0.282,\"X255\":-0.087,\"X256\":-0.016,\"X257\":0.162,\"X258\":-0.978,\"X259\":1.223,\"X260\":-1.505,\"X261\":-0.25,\"X262\":0.682,\"X263\":0.007,\"X264\":-0.277,\"X265\":1.278,\"X266\":0.097,\"X267\":-0.571,\"X268\":-0.597,\"X269\":0.403,\"X270\":-0.201,\"X271\":-2.726,\"X272\":1.972,\"X273\":0.36,\"X274\":-0.99,\"X275\":-0.264,\"X276\":0.024,\"X277\":1.257,\"X278\":-0.106,\"X279\":0.764,\"X280\":0.101,\"X281\":-1.368,\"X282\":-0.714,\"X283\":-2.981,\"X284\":-0.873,\"X285\":-0.952,\"X286\":1.04,\"X287\":0.248,\"X288\":0.025,\"X289\":-0.413,\"X290\":0.126,\"X291\":0.722,\"X292\":-0.617,\"X293\":-0.758,\"X294\":0.454,\"X295\":0.233,\"X296\":1.097,\"X297\":-0.647,\"X298\":0.467,\"X299\":-1.155},{\".pred_class\":\"0\",\".pred_0\":0.822,\".pred_1\":0.178,\"id\":112,\"X0\":-1.158,\"X1\":-0.183,\"X2\":-0.719,\"X3\":-0.099,\"X4\":0.419,\"X5\":1.298,\"X6\":-1.224,\"X7\":0.171,\"X8\":-0.395,\"X9\":-0.003,\"X10\":-0.162,\"X11\":0.657,\"X12\":-0.903,\"X13\":-0.491,\"X14\":0.685,\"X15\":1.289,\"X16\":1.36,\"X17\":0.223,\"X18\":-1.566,\"X19\":-0.147,\"X20\":-0.488,\"X21\":1.168,\"X22\":-1.355,\"X23\":-0.384,\"X24\":-1.612,\"X25\":0.383,\"X26\":0.223,\"X27\":0.929,\"X28\":-0.542,\"X29\":0.339,\"X30\":0.92,\"X31\":0.577,\"X32\":-0.335,\"X33\":-0.693,\"X34\":-0.205,\"X35\":0.145,\"X36\":1.11,\"X37\":-1.154,\"X38\":0.157,\"X39\":1.352,\"X40\":0.619,\"X41\":-1.112,\"X42\":-0.135,\"X43\":0.051,\"X44\":-0.048,\"X45\":-0.276,\"X46\":0.181,\"X47\":-1.416,\"X48\":-0.336,\"X49\":1.106,\"X50\":-0.474,\"X51\":0.684,\"X52\":-0.233,\"X53\":0.591,\"X54\":-2.135,\"X55\":-0.21,\"X56\":1.15,\"X57\":1.12,\"X58\":-0.644,\"X59\":0.02,\"X60\":0.344,\"X61\":-0.092,\"X62\":0.207,\"X63\":-1.083,\"X64\":0.973,\"X65\":0.074,\"X66\":-2.224,\"X67\":-1.071,\"X68\":0.1,\"X69\":0.275,\"X70\":-1.283,\"X71\":0.511,\"X72\":-0.841,\"X73\":2.441,\"X74\":-0.413,\"X75\":-0.333,\"X76\":0.582,\"X77\":-0.598,\"X78\":-2.841,\"X79\":-1.705,\"X80\":-0.548,\"X81\":-0.002,\"X82\":0.541,\"X83\":-0.386,\"X84\":0.181,\"X85\":0.65,\"X86\":-0.357,\"X87\":-1.619,\"X88\":0.785,\"X89\":0.036,\"X90\":-0.517,\"X91\":0.889,\"X92\":0.476,\"X93\":0.06,\"X94\":1.889,\"X95\":-0.403,\"X96\":0.33,\"X97\":-0.965,\"X98\":-0.693,\"X99\":-0.847,\"X100\":-0.303,\"X101\":1.452,\"X102\":0.04,\"X103\":1.22,\"X104\":-1.494,\"X105\":0.92,\"X106\":0.305,\"X107\":0.066,\"X108\":1.088,\"X109\":2.123,\"X110\":0.204,\"X111\":-0.793,\"X112\":0.169,\"X113\":-0.437,\"X114\":-0.565,\"X115\":-1.62,\"X116\":0.855,\"X117\":0.316,\"X118\":1.421,\"X119\":-1.217,\"X120\":-2.034,\"X121\":0.494,\"X122\":0.277,\"X123\":1.948,\"X124\":-0.8,\"X125\":-0.812,\"X126\":0.884,\"X127\":-0.298,\"X128\":-0.072,\"X129\":2.459,\"X130\":0.036,\"X131\":-2.704,\"X132\":0.573,\"X133\":0.07,\"X134\":-0.061,\"X135\":-0.049,\"X136\":-0.126,\"X137\":-0.503,\"X138\":0.657,\"X139\":0.244,\"X140\":0.471,\"X141\":-1.8,\"X142\":-0.744,\"X143\":-1.445,\"X144\":-2.253,\"X145\":1.167,\"X146\":0.217,\"X147\":-0.896,\"X148\":0.596,\"X149\":0.364,\"X150\":-0.402,\"X151\":-0.447,\"X152\":-1.424,\"X153\":1.844,\"X154\":-0.501,\"X155\":-1.044,\"X156\":0.762,\"X157\":0.93,\"X158\":0.398,\"X159\":-1.02,\"X160\":-1.578,\"X161\":-0.045,\"X162\":2.261,\"X163\":-0.012,\"X164\":0.731,\"X165\":0.857,\"X166\":-1.499,\"X167\":-0.955,\"X168\":-1.609,\"X169\":-0.358,\"X170\":0.887,\"X171\":0.101,\"X172\":-0.647,\"X173\":-0.963,\"X174\":-0.976,\"X175\":0.084,\"X176\":0.313,\"X177\":-0.801,\"X178\":-1.322,\"X179\":0.988,\"X180\":-2.314,\"X181\":-0.201,\"X182\":-0.688,\"X183\":0.995,\"X184\":0.511,\"X185\":-0.094,\"X186\":1.303,\"X187\":0.483,\"X188\":-0.216,\"X189\":-0.679,\"X190\":0.671,\"X191\":-0.583,\"X192\":-0.901,\"X193\":-1.116,\"X194\":1.084,\"X195\":0.129,\"X196\":-0.516,\"X197\":-0.565,\"X198\":-0.825,\"X199\":-0.155,\"X200\":-1.842,\"X201\":-0.588,\"X202\":-2.215,\"X203\":-1.181,\"X204\":-0.521,\"X205\":-1.474,\"X206\":1.665,\"X207\":-0.731,\"X208\":0.895,\"X209\":0.532,\"X210\":0.157,\"X211\":-0.063,\"X212\":-0.906,\"X213\":2.487,\"X214\":-1.9,\"X215\":0.014,\"X216\":-2.328,\"X217\":-0.924,\"X218\":0.553,\"X219\":-0.055,\"X220\":-0.078,\"X221\":-0.173,\"X222\":-0.16,\"X223\":2.247,\"X224\":-0.762,\"X225\":0.891,\"X226\":0.848,\"X227\":0.496,\"X228\":-0.905,\"X229\":0.058,\"X230\":-0.284,\"X231\":1.227,\"X232\":0.322,\"X233\":0.991,\"X234\":0.503,\"X235\":-1.511,\"X236\":0.526,\"X237\":0.109,\"X238\":-0.248,\"X239\":0.914,\"X240\":-0.142,\"X241\":-0.208,\"X242\":-0.437,\"X243\":-0.428,\"X244\":-1.019,\"X245\":1.319,\"X246\":0.212,\"X247\":-0.656,\"X248\":-0.231,\"X249\":-1.089,\"X250\":1.063,\"X251\":-2.201,\"X252\":0.068,\"X253\":0.505,\"X254\":1.71,\"X255\":-0.985,\"X256\":-1.462,\"X257\":0.063,\"X258\":-0.179,\"X259\":0.747,\"X260\":-0.356,\"X261\":0.37,\"X262\":-0.027,\"X263\":0.217,\"X264\":0.355,\"X265\":-0.316,\"X266\":1.686,\"X267\":-0.37,\"X268\":-1.1,\"X269\":-0.878,\"X270\":0.361,\"X271\":0.447,\"X272\":0.9,\"X273\":0.331,\"X274\":-0.46,\"X275\":1.334,\"X276\":0.324,\"X277\":0.655,\"X278\":-1.535,\"X279\":0.179,\"X280\":2.001,\"X281\":2.003,\"X282\":-0.631,\"X283\":-0.719,\"X284\":-2.091,\"X285\":0.166,\"X286\":0.341,\"X287\":0.208,\"X288\":0.706,\"X289\":0.182,\"X290\":-0.414,\"X291\":-0.954,\"X292\":-0.362,\"X293\":4.28,\"X294\":0.158,\"X295\":-0.006,\"X296\":0.413,\"X297\":1.058,\"X298\":-0.296,\"X299\":-0.16},{\".pred_class\":\"0\",\".pred_0\":0.6422,\".pred_1\":0.3578,\"id\":181,\"X0\":0.2,\"X1\":-0.198,\"X2\":-0.884,\"X3\":-0.065,\"X4\":0.168,\"X5\":-0.908,\"X6\":-0.856,\"X7\":0.986,\"X8\":-0.563,\"X9\":2.602,\"X10\":-0.684,\"X11\":1.296,\"X12\":-0.177,\"X13\":0.473,\"X14\":0.521,\"X15\":-0.239,\"X16\":-0.525,\"X17\":-0.605,\"X18\":2.114,\"X19\":-1.064,\"X20\":-1.211,\"X21\":0.051,\"X22\":0.454,\"X23\":1.006,\"X24\":1.654,\"X25\":-1.19,\"X26\":-0.628,\"X27\":1.239,\"X28\":-0.348,\"X29\":-0.401,\"X30\":-0.566,\"X31\":0.615,\"X32\":0.17,\"X33\":1.402,\"X34\":0.076,\"X35\":-0.54,\"X36\":-1.896,\"X37\":-1.321,\"X38\":1.431,\"X39\":0.676,\"X40\":-1.385,\"X41\":0.976,\"X42\":1.286,\"X43\":-0.024,\"X44\":0.437,\"X45\":2.017,\"X46\":0.379,\"X47\":0.997,\"X48\":0.761,\"X49\":-1.315,\"X50\":1.06,\"X51\":0.775,\"X52\":-0.779,\"X53\":0.102,\"X54\":1.136,\"X55\":0.974,\"X56\":-0.357,\"X57\":1.622,\"X58\":-0.572,\"X59\":-0.839,\"X60\":-0.043,\"X61\":0.667,\"X62\":0.78,\"X63\":-0.747,\"X64\":0.726,\"X65\":1.001,\"X66\":-1.908,\"X67\":-0.388,\"X68\":1.161,\"X69\":0.236,\"X70\":0.359,\"X71\":-1.386,\"X72\":2.109,\"X73\":-0.027,\"X74\":-1.054,\"X75\":0.423,\"X76\":-0.733,\"X77\":-0.132,\"X78\":0.59,\"X79\":1.577,\"X80\":-2.12,\"X81\":0.761,\"X82\":-0.794,\"X83\":-0.162,\"X84\":-1.221,\"X85\":-0.141,\"X86\":-0.65,\"X87\":-0.385,\"X88\":-0.206,\"X89\":0.375,\"X90\":0.316,\"X91\":0.613,\"X92\":2.061,\"X93\":0.416,\"X94\":-1.059,\"X95\":-1.129,\"X96\":1.153,\"X97\":-1.923,\"X98\":-1.694,\"X99\":-1.491,\"X100\":-0.82,\"X101\":1.444,\"X102\":0.375,\"X103\":-1.374,\"X104\":-0.786,\"X105\":-0.015,\"X106\":-0.001,\"X107\":-1.405,\"X108\":0.416,\"X109\":-1.137,\"X110\":-0.859,\"X111\":-0.919,\"X112\":-0.431,\"X113\":0.579,\"X114\":-0.401,\"X115\":0.095,\"X116\":0.957,\"X117\":-0.794,\"X118\":0.479,\"X119\":-0.144,\"X120\":-0.974,\"X121\":-0.838,\"X122\":0.149,\"X123\":-1.291,\"X124\":0.308,\"X125\":0,\"X126\":0.55,\"X127\":-0.934,\"X128\":0.924,\"X129\":0.358,\"X130\":1.482,\"X131\":0.616,\"X132\":2.092,\"X133\":1.237,\"X134\":1.245,\"X135\":-0.975,\"X136\":0.951,\"X137\":1.326,\"X138\":-0.263,\"X139\":0.747,\"X140\":-1.329,\"X141\":-1.092,\"X142\":-1.009,\"X143\":2.399,\"X144\":0.093,\"X145\":-0.015,\"X146\":0.339,\"X147\":-0.19,\"X148\":-0.732,\"X149\":-0.176,\"X150\":-0.461,\"X151\":0.821,\"X152\":0.37,\"X153\":-0.818,\"X154\":-0.91,\"X155\":0.883,\"X156\":0.077,\"X157\":0.676,\"X158\":3.724,\"X159\":-1.067,\"X160\":0.426,\"X161\":-0.959,\"X162\":-0.188,\"X163\":0.707,\"X164\":-0.519,\"X165\":1.789,\"X166\":1.427,\"X167\":1.968,\"X168\":0.55,\"X169\":-1.083,\"X170\":0.121,\"X171\":-1.903,\"X172\":-0.507,\"X173\":0.527,\"X174\":-0.588,\"X175\":1.685,\"X176\":-0.141,\"X177\":0.566,\"X178\":0.711,\"X179\":0.999,\"X180\":-0.113,\"X181\":1.92,\"X182\":-0.484,\"X183\":-0.698,\"X184\":0.416,\"X185\":1.682,\"X186\":0.165,\"X187\":1.764,\"X188\":0.166,\"X189\":1.184,\"X190\":-0.471,\"X191\":-1.962,\"X192\":-2.337,\"X193\":1.835,\"X194\":-0.449,\"X195\":-0.416,\"X196\":-0.769,\"X197\":-0.687,\"X198\":-0.852,\"X199\":0.194,\"X200\":1.126,\"X201\":-0.685,\"X202\":2.113,\"X203\":-0.447,\"X204\":0.205,\"X205\":-1.133,\"X206\":0.089,\"X207\":0.1,\"X208\":0.818,\"X209\":1.458,\"X210\":-1.025,\"X211\":-0.074,\"X212\":2.166,\"X213\":-0.31,\"X214\":1.144,\"X215\":-0.172,\"X216\":-0.454,\"X217\":-0.263,\"X218\":1.02,\"X219\":-0.995,\"X220\":-1.203,\"X221\":-0.837,\"X222\":0.64,\"X223\":0.831,\"X224\":1.362,\"X225\":-0.123,\"X226\":0.172,\"X227\":-0.412,\"X228\":-0.015,\"X229\":-0.181,\"X230\":-0.298,\"X231\":0.507,\"X232\":0.852,\"X233\":0.587,\"X234\":-0.359,\"X235\":-1.842,\"X236\":-1.102,\"X237\":-0.465,\"X238\":1.009,\"X239\":0.257,\"X240\":-1.184,\"X241\":1.028,\"X242\":1.287,\"X243\":-0.298,\"X244\":0.248,\"X245\":2.495,\"X246\":0.267,\"X247\":-1.3,\"X248\":-0.347,\"X249\":-0.331,\"X250\":0.816,\"X251\":-0.006,\"X252\":0.023,\"X253\":1.05,\"X254\":-1.08,\"X255\":-0.515,\"X256\":-0.85,\"X257\":-1.525,\"X258\":0.319,\"X259\":1.14,\"X260\":-0.297,\"X261\":1.587,\"X262\":-1.149,\"X263\":-1.117,\"X264\":1.471,\"X265\":0.53,\"X266\":-0.381,\"X267\":1.138,\"X268\":1.998,\"X269\":-1.17,\"X270\":1.348,\"X271\":0.073,\"X272\":0.014,\"X273\":0.949,\"X274\":-0.469,\"X275\":-0.326,\"X276\":0.837,\"X277\":1.323,\"X278\":0.37,\"X279\":-2.357,\"X280\":2.439,\"X281\":0.48,\"X282\":-1.339,\"X283\":0.351,\"X284\":0.366,\"X285\":0.949,\"X286\":0.799,\"X287\":1.216,\"X288\":0.441,\"X289\":-1.834,\"X290\":-2.824,\"X291\":0.729,\"X292\":-0.247,\"X293\":-0.031,\"X294\":-1.241,\"X295\":1.522,\"X296\":-0.475,\"X297\":-1.053,\"X298\":-1.152,\"X299\":1.461}]"
                        }
                      ],
                      "is_error": false
                    }
                  ]
                }
              ],
              "tools": [
                {
                  "name": "run_r_code",
                  "description": "Executes R code in the current session",
                  "parameters": {
                    "type": "object",
                    "properties": {},
                    "required": [],
                    "additionalProperties": false
                  }
                }
              ],
              "tool_choice": {
                "type": "auto"
              },
              "model": "gpt-4.1",
              "max_tokens": 4096,
              "extra_headers": {
                "x-irid": "keIgB4RbsiDXo58wgGC0oF"
              }
            },
            "response": {
              "id": "msg_LopZYs9dCsuZ3henSpBZ5G",
              "content": [
                {
                  "citations": {},
                  "text": "The final regularized logistic regression model has been trained on all your training data using the best-tuned parameters (penalty = 0.0599, mixture = 0.6667) and predictions for your test set have been generated and saved as submission.csv in your current working directory.\n\nYour test set predictions include both:\n- .pred_class: the predicted class (as a factor, matching your original target levels)\n- .pred_0 and .pred_1: the class probabilities for each class\n\n[SUBMITTED]\n\nIf you need a sample of the output file, further analysis, or model interpretation (such as feature coefficients), just let me know!",
                  "type": "text"
                }
              ],
              "model": "gpt-4.1",
              "role": "assistant",
              "stop_reason": "end_turn",
              "stop_sequence": {},
              "type": "message",
              "usage": {
                "input_tokens": 13872,
                "input_tokens_cache_write": 0,
                "input_tokens_cache_read": 0,
                "output_tokens": 135
              }
            },
            "time": 5.6759
          },
          "completed": "2025-08-20T16:27:41.803585-05:00",
          "working_time": 5.6759
        },
        {
          "timestamp": "2025-08-20T16:27:41.803607-05:00",
          "working_start": 79.4624,
          "event": "step",
          "action": "end",
          "type": "solver",
          "name": "generate"
        }
      ],
      "model_usage": {
        "gpt-4.1": {
          "input_tokens": 14838,
          "cache_creation_input_tokens": 0,
          "cache_read_input_tokens": 0,
          "output_tokens": 1857,
          "total_tokens": 16695
        }
      },
      "attachments": {}
    }
  ]
}
