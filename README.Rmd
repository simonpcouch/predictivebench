---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# predictivebench: A data science LLM evaluation

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/DSBench)](https://CRAN.R-project.org/package=DSBench)
<!-- badges: end -->

This repository implements an adaptation of [DSBench](https://arxiv.org/abs/2409.07703), a data science LLM evaluation, in R using [vitals](https://vitals.tidyverse.org/).

This implementation of the benchmark diverges from the original in a few ways:

* predictivebench implements only the data modeling section of DSBench.
* In DSBench, the .xslx, .csv, and other source files are inlined into the user prompt. In predictivebench, models are situated in a working directory where those files exist and must call tools to read them.
* File names and references are renamed to be less indicative of competition data science / model evaluations. This decreases the risk of sandbagging, where models will realize theyâ€™re being evaluated and their behavior might change as a result. 
* Rather than requiring the agent to complete the task in one fell swoop, the agent is allowed to ask a "user" questions (and _must_ do so to perform very well on the eval). This allows for a number of advantages:
  * In many real-world agent use cases, the agent interacts with the user regularly. It's possible that agents may behave differently when acting completely independently versus in tandem with a user.
  * Seldomly in real-world data science would a model be provided with all of the necessary information to complete a task in its initial prompt. Instead, rather than providing the full description of a data science competition and all of the necessary context needed to complete it in the first prompt, the agent is provided with a much more sparse (3-4 sentence) description of the problem and must elicit information from the user.
  * The rest of the needed context lives with a "user" that is actually an LLM itself. This mock user has access to a knowledge bank for the specific question with some relevant information for the question, and is instructed to provide information from the bank if asked for it. Otherwise, it will passively nudge the model along when asked for input.

> IMPORTANT
>
> This evaluation is highly experimental and much of its documentation is aspirational.

# Example

```{r}
#| include: false
cat <- function(x, width = 0.9 * getOption("width")) {
  lines <- unlist(strsplit(x, "\n"))
  wrapped <- unlist(lapply(lines, strwrap, width = width))
  base::cat(wrapped, sep = "\n")
}
```

```{r}
library(tibble)
library(predictivebench)

modeling_dataset
```

Here's an example question:

```{r}
cat(modeling_dataset$input[[1]]$question)
```

The mock user, in this question, would have access to the following information that it can provide to the agent as requested:

```{r}
cat(modeling_dataset$input[[1]]$knowledge)
```

Run the task like so:

```{r}
#| eval: false
tsk <- modeling_task()
tsk

tsk$eval(solver_chat = predictive:::predictive_client())
```
